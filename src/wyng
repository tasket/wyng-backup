#!/usr/bin/env python3
# editor width: 100   -----------------------------------------------------------------------------


###  Wyng â€“ The Logical Volume Backup Tool
###  Copyright Christopher Laprise 2018-2025 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.
###  Permission to redistribute under similar naming "Wyng", "wyng backup", etc.
###  is granted only for un-modified code (GPLv3 S.7-c).


# Return codes:
# 1 - Fatal error
# 2 - Non-fatal error (i.e. specific volumes)
# 3 - Authentication error
# 4 - Timeout
# 5 - Destination not ready
# 6 - Archive not found
# 7 - Local storage not ready
# 8 - Signal
# 9 - User Input, Option or Selection error


import sys, signal, os, stat, shutil, subprocess as SPr, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, copy, hashlib, hmac, functools, uuid
import getpass, base64, platform, resource, itertools, string, struct
import xml.etree.ElementTree, json, ctypes, ctypes.util, atexit
from array import array   ; from urllib.parse import urlparse   ; from math import ceil

try:
    import zstd, warnings
    # Required for crippled zstd library
    warnings.filterwarnings("ignore", category=DeprecationWarning)
except:
    zstd = None

try:
    from Cryptodome.Cipher import ChaCha20 as Cipher_ChaCha20
    from Cryptodome.Cipher import ChaCha20_Poly1305 as Cipher_ChaCha20_Poly1305
    from Cryptodome.Random import get_random_bytes
    import Cryptodome, Cryptodome.Protocol.KDF, Cryptodome.Hash.SHA512
except:
    Cryptodome = Cipher_ChaCha20 = Cipher_ChaCha20_Poly1305 = None


class InputError(ValueError):   pass

class OptionError(ValueError):   pass

class SelectionError(ValueError):   pass

class AuthenticationError(ValueError):   pass

class DecodeError(ValueError):   pass

class StorageError(ValueError):   pass


# ArchiveSet manages archive metadata incl. volumes and sessions

class ArchiveSet:

    confname      = "archive.ini"   ; saltname    = "archive.salt"
    max_volumes   = 1024            ; vid_len     = 10
    min_chunksize = 0x10000         ; mhash_sz    = 44            ; comment_len = 100
    max_conf_sz   = (vid_len + comment_len + mhash_sz + 25) * max_volumes + 1000

    attr_ints   = {"format_ver","chunksize","mci_count","dataci_count"}
    attr_str    = {"compression","compr_level","hashtype","uuid","updated_at","ci_mode"}

    def __init__(self, top, dest, opts, ext="", allvols=False, children=2, show=False,
                 pass_agent=0, passphrase=None, prior_auth=None, reload=False):
        self.time_start      = time_start
        self.monotonic_start = monotonic_start
        self.dest        = dest
        self.opts        = copy.deepcopy(opts)
        self.path        = top
        self.confpath    = pjoin(self.path, self.confname)
        self.saltpath    = pjoin(self.path, self.saltname)
        self.confprefix  = b"[WYNG%02d]\n" % format_version
        self.modeprefix  = b"ci = "
        self.mcrypto     = mcrypto = None
        self.datacrypto  = datacrypto = None
        self.vols        = {}
        self.dedupindex  = {}
        self.dedupsessions = []
        self.raw_hashval = None
        self.big_tmpdir  = self.path+"/.tmp"
        self.subdir      = None
        self.gethash     = hash_funcs["blake2b"]
        self.autoreduce  = children > 1
        self.apdays      = None
        self._autoprune_s= set()

        # persisted:
        self.format_ver  = 0
        self.chunksize   = self.min_chunksize * 2
        self.compression = "zstd" if zstd else "zlib"
        self.compr_level = str(compressors[self.compression][1])
        self.hashtype    = "hmac-sha256"
        self.uuid        = None
        self.updated_at  = None
        self.ci_mode     = None
        self.mci_count   = self.dataci_count  =  0
        self.in_process  = []

        shutil.rmtree(self.big_tmpdir, ignore_errors=True)
        os.makedirs(self.big_tmpdir, exist_ok=True)

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"], cp["volumes"], cp["in_process"]  =  {},{},{}

        # halt configuration if this is a new or temp config
        os.makedirs(self.path, exist_ok=True)
        if not exists(self.confpath+ext):
            if debug:   print("New archive conf:")
            self.uuid = str(uuid.uuid4())    ; return

        # decode archive.ini file
        # Format "magic" is "[WYNGvv]\n" where vv = format version: 9 bytes
        # Format mode is next as "ci = m0\n" where m = cipher mode: 8 bytes
        with open(self.confpath+ext, "rb") as f:
            self.header = (header0 := f.read(9)) + (header1 := f.read(8))
            buf = f.read(self.max_conf_sz)
        self.raw_hashval = hashlib.sha256(header0 + header1 + buf).hexdigest()
        if not (header0 == self.confprefix and header1.startswith(self.modeprefix)):
            if header0.startswith(b"[var]"):
                self.format_ver = 2
                return
            else:
                raise ValueError("Not a Wyng format.")

        # use existing auth if specified
        if isinstance(prior_auth, ArchiveSet):
            ci = prior_auth.ci_mode
            self.mcrypto    = mcrypto = prior_auth.mcrypto
            self.datacrypto = datacrypto = prior_auth.datacrypto
        else:
            ci = header1[-3:-1].decode("ASCII")

        # parse metadata crypto mode and instantiate it
        if ci != "00" and not mcrypto:
            ci_types = DataCryptography.crypto_codes[ci]
            if debug:   print(f"\n{self.confname} metadata cipher = {ci_types[1]}")

            # access a key agent, if available for this UUID and archive URL
            agent_helper_write(tmpdir)
            agentkeys  = agent_get(dest.agent_name, pass_agent)

            # get passphrase if we got no agentkeys
            if not passphrase and not agentkeys:   passphrase = get_passphrase(cmd=opts.passcmd)
            if passphrase:
                if passphrase == bytes(len(passphrase)):
                    raise AuthenticationError("No authentication input.")
            elif not agentkeys:
                raise AuthenticationError("No authentication input.")
            passphrase_s1 = passphrase[:] if passphrase else None

            # create both crypto instances and load metadata key
            self.datacrypto = datacrypto = DataCryptography()
            self.mcrypto    = mcrypto    = DataCryptography()
            mcrypto.load(ci_types[1], self.saltpath, slot=1,
                         passphrase=passphrase_s1, agentkeys=agentkeys)

        # initial decryption + auth of archive.ini (root body)
        try:
            if mcrypto:   buf = mcrypto.decrypt(buf)
        except ValueError as e:
            if "MAC check failed" in e.args:
                if debug:   err_out(repr(e))
                raise AuthenticationError("Could not decrypt/authenticate archive.")
            raise e

        # read attributes from archive.ini
        if show:   print(gzip.decompress(buf).decode("UTF-8"))
        cp.read_string(gzip.decompress(buf).decode("UTF-8"))
        for key in self.attr_ints & set(cp["var"]):   setattr(self, key, int(cp["var"][key]))
        for key in self.attr_str & set(cp["var"]):    setattr(self, key, cp["var"][key])
        self.in_process  = [ ln for ln in cp["in_process"].values() ]

        # consistency checks:
        if self.ci_mode and self.ci_mode != ci:
            raise ValueError("Header ci_mode mismatch: %s != %s" % (ci, self.ci_mode))
        else:
            self.ci_mode = ci
        if self.format_ver > format_version or not self.format_ver:
            raise ValueError("Archive format ver = "+repr(self.format_ver)+
                                ". Expected = "+repr(format_version))
        if float(self.updated_at) < 1514782800:
            raise ValueError("Bad update timestamp: "+repr(self.updated_at))
        if not self.uuid or len(list(map(bytes.fromhex, self.uuid.split("-")))) < 2:
            raise ValueError("Malformed UUID: "+repr(self.uuid))

        # set final arch_init properties
        # use blake2b for metadata if hmac-sha256 is used for data:
        self.gethash     = hash_funcs["blake2b"] if self.hashtype == "hmac-sha256" \
                           else hash_funcs[self.hashtype]
        if self.compression == "zstd" and "zstd" not in compressors:
            raise SystemError("This archive requires the 'python3-zstd' module, "
                              "which is not installed.")
        self.compress    = compressors[self.compression][2]
        self.decompress  = compressors[self.compression][0].decompress
        self.subdir      = "/a_" + ArchiveSet.get_combo_id(os.getuid(), self.uuid, dest.spec)

        # init data crypto
        if mcrypto and datacrypto.key is None:
            if not float(self.updated_at) < self.time_start:
                raise ValueError("Current time is less than archive timestamp!")

            data_ci = ci_types[0]
            cadence = 20 + (data_ci.endswith(("-dgr","-msr")) * 100)
            datacrypto.load(data_ci, mcrypto.keyfile, slot=0, cadence=cadence,
                            passphrase=passphrase, agentkeys=agentkeys)

            # persist keys to allow future invocations w/o passphrase input
            if pass_agent > 0 and not agentkeys:
                agent_make(dest.agent_name, pass_agent, [datacrypto.key, mcrypto.key])

        if mcrypto:
            # sync counters and set working salt file
            self.mci_count    = mcrypto.set_counter(self.mci_count, saltfile=self.saltpath)
            self.dataci_count = datacrypto.set_counter(self.dataci_count, saltfile=mcrypto.keyfile)

        # Enh: make 'hashtype' selectable for cipher modes that support hmac
        if datacrypto and datacrypto.mhashkey:
            self.getdatahash = datacrypto.getmhash_hmac
        else:
            self.getdatahash = hash_funcs[self.hashtype]

        # Ensure fully encrypted or unencrypted
        if int(bool(self.mcrypto)) + int(bool(self.datacrypto)) == 1:
            raise RuntimeError("Invalid encryption.")

        # load volume metadata objects
        if children:
            self.load_volumes(2 if reload else children, show=show, reload=reload)


    def load_volumes(self, children, vids=[], reload=False, handle=False, show=False):

        errors = []    ; vids = vids or list(self.conf["volumes"].keys())
        recv_list = [(x+"/volinfo", ArchiveVolume.max_volinfosz)
                       for x in self.conf["volumes"].keys() if x in vids]
        fetch_file_blobs(recv_list, self.path, self.dest, skip_exists=not reload)
        do_exec([[CP.chattr, "+c"] + list(self.conf["volumes"].keys())],
                cwd=self.path, check=False)

        for key, value in self.conf["volumes"].items():
            # conditions when a volume must be loaded:
            # - allvols flag is set
            # - in_process flag from an unfinished action
            # - volume specified on command line
            # - no volumes specified (hence all)
            # - deduplication is in effect
            #if children: ## and (allvols or self.in_process or options.from_arch or \
            ##(len(options.volumes)==0 or key in options.volumes or options.dedup)):
                # instantiate:

            if vids and key not in vids:   continue
            vid = key; hashval = value; vname = err = None

            #loadses = allvols or self.in_process or options.from_arch or options.dedup or
            try:
                vol  = ArchiveVolume(self, vid, hashval, pjoin(self.path,vid), name=vname,
                                     children=children, reload=reload, show=show)
            except Exception as e:
                err = e    ; errors.append((vid, e))

            if not err:
                self.vols[vol.name] = vol
            elif not handle and reload:
                raise err

        if errors and not reload:
            if debug:   print(repr(errors[0][1]), "\nRELOAD:", [x[0] for x in errors])
            return self.load_volumes(2, vids=[x[0] for x in errors], reload=True,
                                     handle=handle, show=show)

        return errors


    def save_ini(self, ext="", init=False):
        c = self.conf['var']    ; mcrypto = self.mcrypto
        if not self.in_process:
            c['updated_at']  = self.updated_at = \
                                str(self.time_start - self.monotonic_start + time.monotonic())
        if init:
            c['uuid']        = self.uuid
            c['format_ver']  = str(format_version)
            c['chunksize']   = str(self.chunksize)
            c['compression'] = self.compression
            c['compr_level'] = self.compr_level
            c['hashtype']    = self.hashtype
            assert bool(self.ci_mode)
            c['ci_mode']     = self.ci_mode

        if mcrypto:
            if self.datacrypto.counter > self.datacrypto.ctstart:
                self.dataci_count = self.datacrypto.counter
            self.mci_count = mcrypto.counter
            c['dataci_count']= str(self.dataci_count)
            c['mci_count']   = str(self.mci_count)

        self.conf['in_process'] = { x: ln if type(ln) is str else ":|".join(ln)
                                          for x,ln in enumerate(self.in_process) }

        os.makedirs(self.path, exist_ok=True)    ; etag = b''
        with io.StringIO() as fs:
            self.conf.write(fs)    ; fs.flush()
            buf = gzip.compress(fs.getvalue().encode("UTF-8"), 4)
        if mcrypto:   etag, buf = mcrypto.encrypt(buf)
        with open(self.confpath+ext, "wb") as f:
            f.write(b''.join((self.confprefix, self.modeprefix,
                              self.ci_mode.encode("ASCII"), b"\n")))
            f.write(etag)    ; f.write(buf)

        return [self.confname+ext]

    def rename_saved(self, ext=".tmp"):
        if exists(self.confpath+ext):
            os.replace(self.confpath+ext, self.confpath)

    # Set or clear state for the archive as 'in_process' in case of interruption during write.
    # Format is list containing strings or list of strings. For latter, ':|' is the delimiter.
    def set_in_process(self, outer_list, tmp=False, save=True, todest=True):
        fssync(aset.path)
        self.in_process = [] if outer_list is None else outer_list
        if not save:   return
        self.save_ini(".tmp" if tmp else "")

        #if not tmp:   os.replace(self.confpath+".tmp", self.confpath)
        if todest:      update_dest(self, pathlist=[self.confname], ext=".tmp" if tmp else "")

    def stop(self):
        self.autoreduce = False

    def add_volume_meta(self, datavol, desc="", ext=""):
        errs = []
        if len(self.conf["volumes"]) >= self.max_volumes:
            raise ValueError("Too many volumes")
        if datavol in self.vols:
            print(datavol+" is already configured.")    ; return None

        namecheck = ArchiveVolume.volname_check(datavol)
        if namecheck:
            errs.append(namecheck+"\n")
        if len(desc.encode("UTF-8")) > self.comment_len:
            errs.append("Error: Max "+self.comment_len+" size for volume desc.\n")
        if not desc.isprintable():
            errs.append("Error: [^control] not allowed in volume desc.\n")
        if errs:
            sys.stderr.write("".join(errs))    ; error_cache.append(datavol)    ; return None

        while (vid := "Vol_"+os.urandom(3).hex()) in self.conf["volumes"]:   pass

        self.vols[datavol] = vol = ArchiveVolume(self, vid, None, pjoin(self.path,vid),
                                               name=datavol, children=0)
        vol.save_volinfo(ext)
        return vol

    def delete_volume_meta(self, vname=None, vid=None):
        # Enh: add delete-by-vid
        vid = self.vols[vname].vid if vname else vid
        assert vid.startswith("Vol_")
        vpath = self.path+"/"+vid
        del(self.conf["volumes"][vid])
        if vname in self.vols: del(self.vols[datavol])
        self.save_ini()

        if exists(vpath):    shutil.rmtree(vpath)
        return vid

    def rename_volume_meta(self, datavol, newname, desc="", ext=""):
        vol = self.vols[datavol]
        if newname in self.vols or ArchiveVolume.volname_check(newname):   return False
        if len(desc.encode("UTF-8")) > self.comment_len or not desc.isprintable():
            err_out("Description format error.")    ; return False

        vol.name = newname          ; vol.desc = desc
        self.vols[newname] = vol    ; del(self.vols[datavol])
        vol.save_volinfo(ext)
        return True

    def b64hash(self, buf):
        return base64.urlsafe_b64encode(self.gethash(buf)).decode("ascii")

    def encode_file(self, fname, fdest=None, get_digest=True, compress=True):
        # Enh: optimize memory use
        mcrypto = self.mcrypto    ; digest = None    ; etag = b''
        destname= fdest or fname+(".z" if compress else "")

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            inbuf = bytes(inmap) if self.compression == "zstd" else inmap
            mbuf  = self.compress(inbuf, int(self.compr_level)) if compress else inbuf
            if get_digest:   digest = self.b64hash(mbuf)
            if mcrypto:      etag, mbuf = mcrypto.encrypt(mbuf)
            with open(destname,"wb") as f:   f.write(etag); f.write(mbuf)

        return digest

    def decode_file(self, fname, digest, max_sz, try_ext=".tmp"):
        # Enh: optimize memory use
        mcrypto = self.mcrypto         ; buf_start = mcrypto.buf_start if mcrypto else 0

        for ext in ["", try_ext] if try_ext and exists(fname+try_ext) else [""]:
            error = None
            with  open(fname+ext,"rb") as inf:
                if not (buf_start < os.stat(inf.fileno()).st_size <= max_sz + buf_start):
                    error = f"Bad buffer size {os.stat(inf.fileno()).st_size}: {fname+ext}"
                else:
                    mbuf = mcrypto.decrypt(inf.read()) if mcrypto else bytes(inf.read())
                    if not hmac.compare_digest(digest, self.b64hash(mbuf)):
                        error = "MD file %s hash %s, expected %s" \
                                % (fname+ext, self.b64hash(mbuf), digest)
            if not error:
                if ext:   os.replace(fname+ext, fname)
                return self.decompress(mbuf)
            if ext:   os.remove(fname+ext)

        raise DecodeError(error)

    # compile global sessions for synced autoprune calendar
    def get_global_ap_ses(self):
        if self._autoprune_s:   return self._autoprune_s.copy()
        self._autoprune_s = {s for v in self.vols.values() for s in v.sesnames} \
                             - {v.last for v in self.vols.values()}
        return self._autoprune_s.copy()

    def get_combo_id(userid, uuid, url):
        assert uuid or url
        return hashlib.blake2b(
                           bytes(str(userid) + uuid + url, encoding="UTF-8"),
                           digest_size=20).hexdigest()


class ArchiveVolume:

    vid_chk       = re.compile(r"^Vol_[0-9a-fA-F]+$")
    mdvol_chk     = re.compile(r"^wyng-.*metadata$")
    max_sessions  = 16384    ; volname_len = 4000    ; sesname_sz = 17
    max_volinfosz = (ArchiveSet.mhash_sz + sesname_sz + 10) * max_sessions \
                  + volname_len * 4 + ArchiveSet.comment_len * 4 + 10

    __slots__ = ("vid","name","hashval","archive","path","alias","aliastype","sessions","sesnames",
                 "_seslist","last","meta_checked","tags","desc","changed_bytes")

    def __init__(self, archive, vid, hashval, path, name=None,
                 children=2, reload=False, show=False):
        self.vid       = vid                       ; self.tags    = {}
        self.archive   = archive                   ; self.path    = path
        self.hashval   = hashval                   ; self.changed_bytes = 0
        self.last      = "None"                    ; self.meta_checked  = False
        self.alias     = name                      ; self.aliastype     = None
        # persisted here:
        self.name      = name                      ; self.desc      = ""
        self.sessions  = {}                        ; self.sesnames= []

        Ses = ArchiveSession
        if show:  print("\n", vid, "-", name)
        if path and not exists(path):   os.makedirs(path)
        if path and hashval:
            buf = archive.decode_file(pjoin(path,"volinfo"), hashval, self.max_volinfosz)
            if show:   print(buf.decode("UTF-8"))

            with io.StringIO(buf.decode("UTF-8")) as f:
                for ln in f:
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if vname.startswith("S_"):
                        self.sessions[vname] = Ses(self, vname, value, "")
                    elif vname not in {"name","desc"}:
                        if debug:   err_out("Bad vol attr name: "+vname)
                    else:
                        setattr(self, vname, value)

            if (res := ArchiveVolume.volname_check(self.name)):   raise ValueError(res)
            if not self.vid_chk.match(vid):   raise ValueError("Bad vid: "+repr(vid))
            if self.desc and not all(map(str.isprintable, self.desc)) \
            or len(self.desc) > ArchiveSet.comment_len:
                raise ValueError("Bad desc: "+repr(self.desc))
            if len(self.sessions) > self.max_sessions:
                raise ValueError(f"Too many sessions: {len(self.sessions)}")

        self.alias = self.name

        if children > 1:
            self.load_sessions()

        if exists(pjoin(path,"volchanged")):
            self.changed_bytes = int(open(pjoin(path,"volchanged"),"r").readlines()[0].strip())

    def load_sessions(self, sesids=[], handle=False, mfdecode=False, reload=False, show=False):

        sessions, path, vid, vname = self.sessions, self.path, self.vid, self.name
        Ses = ArchiveSession    ; sesids = sesids or list(sessions.keys())    ; errors = []

        # fetch session metadata 'info' from remote
        ses_list =[(vid+"/"+x, x, vname) for x in sesids]
        fetch_file_blobs([(x+"/info", ArchiveSession.max_infosz) for x,_,_ in ses_list],
                         self.archive.path, self.archive.dest, skip_exists=not reload, skip0=True)

        for sname in sesids or sessions:
            if not (s := sessions[sname]).loaded:
                try:
                    s = sessions[sname] = Ses(self, sname, s.hashval, path+"/"+sname, show=show)
                except Exception as e:
                    #err_out(f"{self.vid} - {sname}\n{repr(e)}")
                    errors.append((sname, e))
                    if not handle and reload:   raise e

        # session name list sorted by sequence field
        if not errors:
            self._seslist = list(self.sessions.values())
            self._seslist.sort(key=lambda x: x.sequence)
            self.sesnames = [y.name for y in self._seslist]
        elif not reload:
            if debug:   print(repr(errors[0][1]), "\nRELOAD:", [x[0] for x in errors])
            return self.load_sessions(sesids=[x[0] for x in errors], handle=handle, reload=True)
        elif not handle:
            raise errors[0][1]

        if self.sesnames:   self.last = self.sesnames[-1]

        if mfdecode and not errors:
            errors.extend(self.decode_manifests(sesids, force=True, handle=handle))
            if errors and not handle:   raise errors[0][1]

        return errors

    def save_volinfo(self, ext=""):
        os.makedirs(self.path, exist_ok=True)
        arch = self.archive    ; vid = self.vid   ; fname = "volinfo"   ; etag = b''
        with io.StringIO() as f:
            print("name =", self.name, file=f)
            print("desc =", self.desc, file=f)
            for ses in self.sessions.values():
                if ses.saved:   print(ses.name, "=", ses.hashval, file=f)

            buf = arch.compress(f.getvalue().encode("UTF-8"), int(arch.compr_level))
            assert len(buf) <= self.max_volinfosz
            arch.conf["volumes"][vid] = arch.b64hash(buf)

        if arch.mcrypto:
            etag, buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
        with open(pjoin(self.path,"sessions"+ext), "w") as f:
            json.dump(self.sesnames, f)
        return [pjoin(vid,"sessions"+ext), pjoin(vid,fname+ext)] + arch.save_ini(ext)

    def rename_saved(self, ext=".tmp"):
        assert ext and exists(pjoin(self.path,"volinfo")+ext)
        for rpath in (pjoin(self.path,"volinfo"), pjoin(self.path,"sessions")):
            if exists(rpath+ext):   os.replace(rpath+ext, rpath)
        self.archive.rename_saved(ext)

    def volname_check(vname): # Fix: move to LocalStorage class
        if not 0 < len(vname) <= ArchiveVolume.volname_len:
            return f"Volume path/name length must be 1 - {ArchiveVolume.volname_len}."
        if not vname.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in vname.strip().split("/")]) or vname.strip()[0] == "/":
            return "Bad volume name."
        return ""

    def volsize(self):
        return self.sessions[self.last].volsize if self.sessions else 0

    def last_chunk_addr(self, vsize=None):
        if vsize is None:  vsize = self.volsize()
        chdigits    = max_address.bit_length() // 4
        lchunk_addr = max(0, (vsize-1) - ((vsize-1) % self.archive.chunksize))
        return lchunk_addr, ("x%0"+str(chdigits)+"x") % lchunk_addr

    def mapfile(self, pos=None):
        if not self.sessions:   return None
        s = self.sessions[self.sesnames[pos] if pos else self.last]
        return f"{self.path}/{s.name}_{s.sequence}.deltamap"

    # Based on last session size unless volume_size is specified.
    def mapsize(self, volume_size=None):
        vs = volume_size or self.volsize()
        return (vs // self.archive.chunksize // 8) + 1

    def map_used(self, ext=""):
        return os.stat(self.mapfile()+ext).st_blocks if exists(self.mapfile()+ext) else 0

    def changed_bytes_add(self, amount, reset=False, save=False):
        if reset:
            if exists(self.path+"/volchanged"):   os.remove(self.path+"/volchanged")
            self.changed_bytes = 0  ; return

        self.changed_bytes += amount
        if save:
            with open(self.path+"/volchanged", "w") as f:
                print(self.changed_bytes, file=f)
                f.flush()    ; os.fsync(f.fileno())

    def init_deltamap(self, timestamp=None):
        self.changed_bytes_add(0, reset=True)    ; bmfile = self.mapfile()
        if exists(bmfile):
            os.remove(bmfile)
        with open(bmfile, "wb") as bmapf:
            bmapf.truncate(self.mapsize())    ; bmapf.flush()
        if timestamp:   os.utime(bmfile, ns=(timestamp,)*2)

    def is_mdvol(self):
        return bool(self.mdvol_chk.match(self.name))

    def new_session(self, sname, addtags=[]):
        ns = ArchiveSession(self, sname, None, addtags=addtags)
        ns.path = pjoin(self.path, sname)
        ns.sequence = self.sessions[self.last].sequence + 1 if self.sessions else 0
        ns.previous = self.last

        self.last = sname
        self.sesnames.append(sname)
        self.sessions[sname] = ns
        if self.archive.dedupindex:    self.archive.dedupsessions.append(ns)
        return ns

    def delete_session(self, sname, remove=True, force=False):
        ses     = self.sessions[sname]
        index   = self.sesnames.index(sname)    ; affected = None
        if sname == self.last and ses.saved and not force:
            raise ValueError("Cannot delete last session")

        for tag in list(ses.tags.keys()):   self.sessions[sname].tag_del(tag)
        del(self.sesnames[index], self.sessions[sname])

        if self.archive.dedupsessions:
            indexdd = self.archive.dedupsessions.index(ses)
            self.archive.dedupsessions[indexdd] = None

        # Following condition means:
        #   * sesnames cannot be empty
        #   * ses wasn't deleted from end of list

        if len(self.sesnames) > index:
            affected = self.sesnames[index]
            self.sessions[affected].previous = ses.previous

        self.last  = self.sesnames[-1] if len(self.sesnames) else "None"

        if remove and exists(ses.path):   shutil.rmtree(ses.path)
        return affected

    # Decodes a 'manifest.z' file
    def decode_one_manifest(self, ses, force=False):
        if not ses.path:   return
        path = ses.path+"/manifest"
        if not exists(path) or (force and not ses.just_fetched):
            os.makedirs(ses.path, exist_ok=True)
            do_exec([[CP.chattr, "+c", ses.path]], check=False)
            if ses.manifesthash == "0":   open(path,"wb").close()   ; return

            buf = self.archive.decode_file(path+".z", ses.manifesthash, ses.manifest_max())
            with open(path+".dtmp", "wb") as f:   f.write(buf)
            os.replace(path+".dtmp", path)
            ses.just_fetched = True
        else:
            os.utime(path)

        if exists(path+".z"):   os.utime(path+".z")

    # Fetches files and passes them to decode_one_manifest()
    def decode_manifests(self, sesnames, force=False, handle=False):
        apath    = self.archive.path    ; dest = self.archive.dest    ; errors = []
        ses_list = [self.sessions[ses] for ses in sesnames]

        ff = fetch_file_blobs([(pjoin(self.vid, ses.name, "manifest.z"), ses.manifest_max())
                                 for ses in ses_list if ses.path and not ses.path.endswith("-tmp")
                                 and ses.manifesthash != "0"],
                              apath, dest, skip_exists=not force)

        for ses in ses_list:
            try:
                self.decode_one_manifest(ses, force=force)
            except Exception as e:
                errors.append((ses.name, e))
                if not handle:   raise e

        return errors


class ArchiveSession:

    attr_str  = ("localtime","previous","permissions","manifesthash")
    attr_int  = ("volsize","sequence")
    __slots__ = attr_str + attr_int \
              + ("tags","volume","archive","name","path","saved","loaded","toggle",
                 "uuid","hashval","meta_checked","just_fetched")
    name_chk  = re.compile(r"^S_[0-9]{8}-[0-9]{6}$")
    sesname_sz= ArchiveVolume.sesname_sz    ; tag_len = 25    ; max_tags = 5    ; max_field = 64
    max_permsz= 32 * 4 * 2 + 5  # user & group names in utf-8 plus mode
    max_infosz= (tag_len * 4 + ArchiveSet.comment_len * 4 + 10) * max_tags + max_permsz \
              + sum(map(len, attr_str+attr_int)) + len(attr_str+attr_int)*(max_field+10)

    def __init__(self, volume, name, hashval, path="", addtags=[], show=False):
        self.volume   = volume;    self.archive = arch = volume.archive
        self.name     = name
        self.path     = path
        self.saved    = self.loaded = False
        self.toggle   = True
        self.hashval  = hashval
        self.meta_checked = False
        self.just_fetched = False
        # persisted:
        self.localtime= None
        self.volsize  = None
        self.sequence = None
        self.previous = "None"
        self.tags     = {}
        self.permissions  = ""
        self.manifesthash = None

        if path and hashval:
            buf = arch.decode_file(pjoin(path,"info"), hashval, self.max_infosz)
            if show:   print(buf.decode("UTF-8"))
            with io.StringIO(buf.decode("UTF-8")) as sf:
                attrlist = self.attr_str + self.attr_int
                for ln in sf:
                    vname, value = map(str.strip, ln.split("=", maxsplit=1))
                    if vname == "tag":
                        self.tag_add(ArchiveSession.tag_parse(value))
                        continue
                    elif vname not in attrlist:
                        if debug:   err_out("Bad ses attr name: "+vname)
                    else:
                        setattr(self, vname,
                                int(value) if vname in self.attr_int else value)

            if not 0 <= self.volsize <= max_address:
                raise ValueError(f"Bad volume size: {self.volsize}")
            if self.sequence < 0:
                raise ValueError(f"Bad sequence: {self.sequence}")
            if not (self.localtime.isdigit() or self.name_chk.match("S_" + self.localtime)):
                raise ValueError("Bad localtime.")
            if not self.name_chk.match(self.name):
                raise ValueError("Bad name field.")
            if not (x := self.permissions) in {"r","w"} and not (len((y := x.split(":"))) == 3 \
            and len(x) <= self.max_permsz and y[0].isdigit() and "".join(y[1:]).isalnum()):
                raise ValueError(f"Bad permissions field: {x}")

            self.saved = self.loaded = True

        for tag in addtags:   self.tag_add(tag)


    def manifest_max(self):
        return (self.volsize // self.archive.chunksize + 1) \
               * (self.archive.mhash_sz + len(hex(max_address)) + 10)

    def tag_parse(tag, delim=" "):
        errs = []      ; parts   = tag.strip().split(delim, maxsplit=1)
        tag_id = parts[0].strip().lower()[:ArchiveSession.tag_len]
        comment = parts[1].strip() if len(parts)>1 else ""

        if re.match(r".*[,=\^]", tag_id) or any(map(str.isspace, tag_id)) \
        or not tag_id.isprintable():
            errs.append("Error: [^control], [space], and ',^=' not allowed in tag ID.")
        if not comment.isprintable():
            errs.append("Error: [^control] not allowed in tag comment.")
        if len(comment) > ArchiveSet.comment_len:
            errs.append("Error: Oversize tag comment.")
        if tag_id == "all":   errs.append("Error: tag 'all' is reserved.")
        if errs:   err_out("\n".join(errs))

        return tuple() if errs else (tag_id, comment)

    def tag_add(self, tag):
        if len(self.tags) >= ArchiveSession.max_tags:
            err_out(f"{ArchiveSession.max_tags} maximum tags.")   ; return False
        self.saved = False    ; voltags = self.volume.tags    ; tid = tag[0]
        if tid not in self.tags:   self.tags[tid] = tag[1]
        if tid in voltags:
            voltags[tid].add(self.name)
        else:
            voltags[tid] = {self.name}
        return True

    def tag_del(self, tag):
        del(self.tags[tag])   ; voltags = self.volume.tags
        if tag in voltags:
            if self.name in voltags[tag]:   voltags[tag].remove(self.name)
            if len(voltags[tag]) == 0:    del(voltags[tag])

    def gettime(self):
        if is_num(self.localtime):
            return int(self.localtime)
        else:
            return int(time.mktime(time.strptime(self.localtime, "%Y%m%d-%H%M%S"))*1000000000)

    def is_changed(self):
        sn   = self.volume.sesnames    ; ii = sn.index(self.name)
        prev = self.volume.sessions[sn[ii-1]] if ii else None
        return self.manifesthash != "0" or ii == 0 or self.volsize != prev.volsize \
               or self.permissions != prev.permissions

    def save_info(self, ext=""):
        assert self.path   ; saved_files = []   ; arch = self.volume.archive
        etag = b''   ; fname = "info"   ; relpath = os.path.relpath(self.path, arch.path)

        if os.path.getsize(self.path+"/manifest"+ext) == 0:
            self.manifesthash = "0"
        else:
            saved_files.append(pjoin(relpath, mfz := "manifest.z"+ext))
            self.manifesthash = arch.encode_file(self.path+"/manifest"+ext, fdest=self.path+"/"+mfz)
        with io.StringIO() as f:
            for attr in self.attr_str+self.attr_int:
                print(attr, "=", field := getattr(self, attr), file=f)
                assert len(str(field)) <= self.max_field
            for tkey, tdesc in self.tags.items():
                print("tag =",   tkey, tdesc, file=f)
            buf = arch.compress(f.getvalue().encode("UTF-8"),
                                int(arch.compr_level))
            self.hashval = arch.b64hash(buf)
            assert len(buf) <= self.max_infosz

        if arch.mcrypto:   etag, buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
        self.saved = self.loaded = True
        saved_files.extend([pjoin(relpath,fname+ext)] + self.volume.save_volinfo(ext))
        return saved_files

    def rename_saved(self, ext=".tmp"):
        assert ext and exists(pjoin(self.path,"info")+ext)
        for rpath in (pjoin(self.path,x) for x in ("info", "manifest", "manifest.z")):
            if exists(rpath+ext):   os.replace(rpath+ext, rpath)
        self.volume.rename_saved(ext)

# END class ArchiveSet, ArchiveVolume, ArchiveSession


class OldArchiveSet_V2:
    confname = "archive.ini"  ; format_ver = 2   ; a_ints = {"chunksize"}

    def __init__(self, top, dest):
        self.path  = top      ; self.dest = dest          ; self.vols  = {}
        if exists(self.path+"/in_process"):
            raise ValueError("in_process must be resolved before upgrade.")

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()    ; cp.optionxform = lambda option: option
        cp["var"], cp["volumes"] = {}, {}                 ; cp.read(pjoin(top, self.confname))
        for name in cp["var"].keys():
            setattr(self, name, int(cp["var"][name]) \
                                if name in self.a_ints else cp["var"][name])

        # load volume metadata objects
        for key in cp["volumes"]:
            fetch_file_blobs([(pjoin(key,"volinfo"), 100000000)], top, dest)
            self.vols[key] = self.Volume(self, key, pjoin(top,key), self.vgname)

    class Volume:
        def __init__(self, archive, name, path, vgname):
            self.archive = archive    ; self.name = name    ; self.path = path
            self.format_ver = "0"     ; self.uuid = None    ; self.desc = ""
            self.sessions, self.sesnames = {}, []
            self.first = self.last = "None"

            # load volume info
            with open(pjoin(path,"volinfo"), "r") as f:
                for ln in f:
                    vname, value = ln.strip().split("=", maxsplit=1)
                    setattr(self, vname.strip(), value.strip())

            if self.format_ver != "2": raise ValueError("Format ver = "+self.format_ver)

            # load sessions as reverse-linked list, starting with the last
            if debug:  print("\nV2 VOLUME", self.name, self.first, self.last)
            sprev = self.last
            while sprev != "None":
                if debug:  print(sprev, end="  ")
                fetch_file_blobs([(f"{name}/{sprev}/info", 100000000),
                                  (f"{name}/{sprev}/manifest", 100000000)], archive.path, archive.dest)
                s = self.sessions[sprev] = self.Ses(self, sprev, path+"/"+sprev)
                sprev = s.previous    ; self.sesnames.insert(0, s.name)
                if sprev == "None" and self.first != s.name:
                    raise ValueError("PREVIOUS MISMATCH: %s/%s, EXPECTED %s"
                                     % (self.name, s.name, self.first))

        class Ses:
            def __init__(self, volume, name, path="", addtags={}):
                self.name     = name     ; self.path = path    ; self.volume = volume
                self.previous = "None"   ; self.tags = {}
                self.localtime= self.volsize  = self.format = self.sequence = None

                with open(pjoin(path,"info"), "r") as sf:
                    for ln in sf:
                        if ln.strip() == "uuid =":  continue
                        vname, value = ln.split("=", maxsplit=1)
                        vname = vname.strip()    ; value = value.strip()
                        if value == "none":   value = "None"
                        if vname == "tag":
                            self.tag_add(value.split(maxsplit=1))
                            continue

                        setattr(self, vname,
                                int(value) if vname in ("volsize","sequence") else value)

                    if not exists(pjoin(path,"manifest")):
                        raise FileNotFoundError("ERROR: Manifest does not exist for "+name)

                for tag in addtags:   self.tag_add(tag)

            def tag_add(self, tag):
                tid = tag[0]
                if tid not in self.tags:   self.tags[tid] = tag[1]

# END OldArchiveSet_V2


def upgrade_from_v2(aset):
    print("Creating metadata backup file 'wyng_metadata_bak.tbz'.")
    do_exec([[CP.tar, "-cjf", "wyng_metadata_bak.tbz", aset.path]])
    print("Upgrading archive format...", end="")   ; dest = aset.dest   ; sessions, moves = [],[]

    os.mkdir(tmpdir+"/:upgrade:")
    new_aset = ArchiveSet(tmpdir+"/:upgrade:", aset.dest, aset.opts)
    for attr in ("uuid","chunksize","compression","compr_level","hashtype"):
        setattr(new_aset, attr, getattr(aset, attr))
    new_aset.ci_mode = "00"    ; new_aset.save_ini(init=True)
    new_aset = ArchiveSet(new_aset.path, aset.dest, aset.opts)

    for vname, vol in aset.vols.items():
        newvol = new_aset.add_volume_meta(vname, desc=vol.desc)
        assert newvol.vid not in aset.vols    ; moves.append(f"mv -T {vname} {newvol.vid}\n")
        for sname in vol.sesnames:
            ses = vol.sessions[sname]    ; newses = newvol.new_session(sname)
            for attr in ("tags","volsize","localtime"):
                setattr(newses, attr, getattr(ses, attr))
            os.makedirs(newses.path, exist_ok=True)
            with open(ses.path+"/manifest", "r") as mreadf, \
                    open(newses.path+"/manifest", "w") as mwf:
                for hexhash, addr in (ln.split() for ln in mreadf):
                    print("0" if hexhash == "0" else
                            base64.urlsafe_b64encode(bytes.fromhex(hexhash)).decode("ASCII"),
                            addr, file=mwf)
            newses.permissions = "w"    ; newses.save_info()
            sessions.append(newses)     ; os.remove(ses.path+"/manifest")
    catch_signals()
    dest.run(["".join(moves)], destcd=dest.path, trap=True)
    update_dest(new_aset, pathlist=[new_aset.confname],
                volumes=new_aset.vols.values(), sessions=sessions)
    catch_signals(**signormal)    ; print("Done.", end="")


def agent_helper_write(path):
    agent_program = r'''#  Copyright Christopher Laprise 2018-2025
#  Licensed under GNU General Public License v3. See wyng-backup/LICENSE file.
import os, sys, signal, time

def gettimes():    return time.clock_gettime(time.CLOCK_BOOTTIME), time.monotonic()

def sighandler(s,f):
    if s == ALRM:   raise TimeoutError("SIGALRM")

def do_mkpipe():
    if exists("/tmp/"+agent_name):   os.remove("/tmp/"+agent_name)
    os.mkfifo("/tmp/"+agent_name, mode=0o600)

class KeyHandler:
    def __init__(self):
        self.keys = []

    def __del__(self):
        for key in self.keys:
            for ii in range(len(key)):   key[ii] = 0
        os.remove("/tmp/"+agent_name)

## MAIN ##
cmd    = sys.argv[1]      ; agent_name = sys.argv[2]     ; inread = sys.stdin.buffer.read
KH     = KeyHandler()     ; magic  = b"\xff\x11\x15"     ; exists = os.path.exists
SIGINT = signal.SIGINT    ; USR1   = signal.SIGUSR1      ; ALRM   = signal.SIGALRM
for s in (SIGINT,USR1,ALRM):   signal.signal(s, sighandler)   ; signal.siginterrupt(s, True)

signal.alarm(10)    ; do_mkpipe()    ; duration = min(inread(1)[0], 60) * 60
bstart, mstart = gettimes()
for slot in range(2):
    KH.keys.append(bytearray(inread(inread(1)[0])))
    if inread(3) != magic:   raise ValueError("Bad magic.")

while True:
    try:
        signal.alarm(0)   ; res = signal.sigtimedwait({USR1}, 60)   ; signal.alarm(10)
        btime, mtime = gettimes()
        if (res is None and btime - bstart >= duration) \
        or abs(btime - mtime) > abs(bstart - mstart) + 5:
            break
        elif res is None:
            continue
        with open("/tmp/"+agent_name,"wb") as npipe:
            for key in KH.keys:
                npipe.write(magic + len(key).to_bytes(1,"big"))   ; npipe.write(key)
        bstart, mstart = gettimes()
    except TimeoutError:
        print("timeout")    ; do_mkpipe()    ; continue
'''
    if not exists(pfile := f"{path}/{prog_name}_agent.py"):
        with open(pfile, "wb") as progf:
            progf.write(bytes(agent_program, encoding="UTF-8"))


def agent_get(agname, duration):
    ps = SPr.check_output([CP.ps, "-u"+str(os.getuid()), "-o", "pid,command"], text=True)
    findp  = [x for x in ps.splitlines() if (agname in x and prog_name+"_agent.py store" in x)]
    if not findp:   return None
    pid    = int(findp[0].split()[0])   ;  os_kill(pid, signal.SIGUSR1)
    result = []   ; catch_signals(["ALRM"], iflag=True) ;   signal.alarm(10)
    try:
        with open("/tmp/"+agname, "rb") as npipe:
            for slot in range(2):
                if m := npipe.read(3) != Destination.magic:
                    raise ValueError("*Magic not found, got "+repr(m))
                result.append(key := bytearray(npipe.read(ksz := npipe.read(1)[0])))
                if len(key) != ksz:   raise ValueError("Key length")
    except (TimeoutError, ValueError) as e:
        for k in result:   clear_array(k)
        result = None    ; os_kill(pid)
    finally:
        if duration == -1:   os_kill(pid)
        signal.alarm(0)  ; catch_signals(**signormal)
        return result


def agent_make(agname, duration, keys):
    p = SPr.Popen([CP.python, f"{tmpdir}/{prog_name}_agent.py","store",agname], text=False,
                  shell=False, stdin=SPr.PIPE, stdout=SPr.DEVNULL, stderr=SPr.STDOUT,
                  process_group=0)
    pwrite = p.stdin.write   ; pwrite(min(duration,60).to_bytes(1,"big"))
    for k in keys:   pwrite(len(k).to_bytes(1,"big"))   ; pwrite(k)   ; pwrite(Destination.magic)
    p.poll(); p.stdin.flush(); p.stdin.close()
    time.sleep(0.2)
    return p


# DataCryptography(): Handle crypto functions and state for volume data

class DataCryptography:

    crypto_key_bits = 256      ; max_ct_bits = 128        ; salt_sz   = 64
    nonce_sz        = 24       ; tag_sz      = 16         ; saltchksz = salt_sz+nonce_sz+tag_sz
    max_keyfile_sz  = (salt_sz + (max_ct_bits // 8)) * 4 + saltchksz # max 4 slots
    time_headroom   = int(60*60*24*365.25*50)             ; timesz    = 32 // 8

    # Matrix of recommended mode pairs = 'formatcode: (data, metadata, selectable)'
    # User selects a data cipher which is automatically paired w a metadata authentication cipher.
    crypto_codes    = {"00":  ("off",                "off", 1),
                       "10":  ("n/a",                "n/a", 0),
                       "20":  ("n/a",                "n/a", 0),
                       "30":  ("xchacha20",          "xchacha20-poly1305",     0),
                       "31":  ("n/a",                "n/a", 0),
                       "32":  ("xchacha20-ct",       "xchacha20-poly1305-ct",  1),
                       "33":  ("n/a",                "n/a", 0),
                       "34":  ("xchacha20-msr",      "xchacha20-poly1305-msr", 1),
                       "35":  ("xchacha20-dgr",      "xchacha20-poly1305-msr", 1),
                       "40":  ("n/a",                "n/a", 0)}

    __slots__ = ("key","keyfile","ci_type","counter","ctstart","ctcadence","countsz","max_count",
                 "slot","slot_offset","key_sz","randomsz","buf_start","mode",
                 "encrypt","decrypt","auth","ChaCha20_new","ChaCha20_Poly1305_new",
                 "time_start","monotonic_start","get_rnd","noncekey","mhashkey")

    def __init__(self):
        self.key = self.noncekey = self.keyfile = self.counter = self.ctstart = self.countsz \
                 = self.mhashkey = self.slot_offset = None

    def load(self, ci_type, keyfile, slot, passphrase, agentkeys=None, cadence=1, init=False):

        if tuple(time.gmtime(0))[:6] != (1970, 1, 1, 0, 0, 0):
            raise SystemError("System time epoch is not 1970-01-01.")
        assert passphrase is None or type(passphrase) == bytearray
        assert type(cadence) is int and cadence > 0

        if not Cryptodome \
        or (ci_type.startswith("xchacha20") and Cryptodome.version_info[0:2] < (3,9)):
            raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")

        self.time_start = time_start       ; self.monotonic_start = monotonic_start
        self.slot       = slot             ; self.slot_offset = self.get_slot_offset(slot)
        self.keyfile    = keyfile          ; self.ci_type     = ci_type
        self.ctcadence  = cadence          ; mknoncekey       = mkmhashkey = False
        self.get_rnd    = get_random_bytes ; self.auth        = False

        # xchacha20 common
        self.key_sz  = self.crypto_key_bits//8 ; self.max_count = 2**80-64
        self.countsz = 10                      ; self.buf_start = self.nonce_sz
        self.randomsz=self.nonce_sz - self.countsz - self.timesz
        self.ChaCha20_new = Cipher_ChaCha20.new
        self.ChaCha20_Poly1305_new = Cipher_ChaCha20_Poly1305.new
        self.decrypt = self._dec_chacha20

        if ci_type == "xchacha20":
            self.encrypt = self._enc_chacha20_ct

        elif ci_type in ("xchacha20-poly1305", "xchacha20-poly1305-ct"):
            self.max_count = 2**61-64
            self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305_ct

        elif ci_type == "xchacha20-ct":
            self.encrypt = self._enc_chacha20_ct
            mkmhashkey   = True

        elif ci_type == "xchacha20-msr":
            self.encrypt = self._enc_chacha20_msr
            mknoncekey   = mkmhashkey = True

        elif ci_type == "xchacha20-poly1305-msr":
            self.max_count = 2**63-64
            self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305_msr
            mknoncekey   = True

        elif ci_type == "xchacha20-dgr":
            self.encrypt = self._enc_chacha20_dgr
            mknoncekey   = mkmhashkey = True

        else:
            raise ValueError("Invalid cipher spec "+ci_type)


        # Load counter and salt from salt file
        if not issubclass(type(keyfile), io.IOBase):
            if init:
                # initialize new salt file
                if not (slot == 1 and not exists(keyfile)):
                    raise ValueError("Bad slot or keyfile.")
                open(keyfile, "wb").close()    ; sf = self.keyfile = self.open_saltfile(keyfile)
                for ii in range(4):
                    sf.seek(self.get_slot_offset(ii))
                    sf.write(bytes(self.countsz) + self.get_rnd(self.salt_sz))
            else:
                sf = self.keyfile = self.open_saltfile(keyfile)

        ct, salt     = self.load_slot(self.slot, self.keyfile)
        # Compatibility kludge for original counter mode salt
        if ci_type in ("xchacha20","xchacha20-poly1305"):   salt = salt[:32]

        # Advance counter with a safe margin 'cadence X2' if cadence is quick (<101)
        self.counter = self.ctstart = ct + (cadence * 2 * int(cadence < 101))

        # Import or derive encryption key
        if agentkeys:
            self.key = agentkeys[slot]
        else:
            self.key = self.derive_key(salt, passphrase, self.key_sz)
        if not (len(self.key) == self.key_sz and type(self.key) is bytearray):
            raise ValueError("Bad key.")

        if mknoncekey:
            # Derive subkey for generating nonces
            _na, nk_salt  = self.load_slot(2, self.keyfile)
            self.noncekey = self.derive_subkey(self.key, 64, 0, nk_salt,
                                               b"Wyng_Nonces" + str(self.slot).encode())

        if mkmhashkey:
            # Derive subkey for data hashing
            _na, mhk_salt = self.load_slot(3, self.keyfile)
            self.mhashkey = self.derive_subkey(self.key, 64, 1, mhk_salt,
                                               b"Wyng-Manifest-Hash" + str(self.slot).encode())

        if init and slot == 1:
            sbuf = b"".join(self.encrypt(self.hash_salts()[0]))
            sf.seek(self.get_slot_offset(4))    ; sf.write(sbuf)    ; sf.flush()
            # Write a copy of salt file, inverted so it isn't deduped.
            with open(os.path.dirname(sf.name)+"/salt.bak","wb") as outf:
                sf.seek(0)    ; outf.write(bytes(x ^ 0xff for x in sf.read()))

        return self.counter


    def __del__(self):
        if self.counter and self.counter > self.ctstart:   self.save_counter()
        if self.key:        clear_array(self.key)
        if self.noncekey:   clear_array(self.noncekey)
        if self.mhashkey:   clear_array(self.mhashkey)


    def open_saltfile(self, fpath, verify=False):
        if self.slot != 1:   raise ValueError("Bad slot.")
        sf = open(fpath, "r+b", buffering=0)

        if verify:
            h, enc_h  = self.hash_salts(sf)
            if len(enc_h) < self.saltchksz:   raise ValueError("Wrong salt hash size "+str(len(enc_h)))
            if not hmac.compare_digest(h, self.decrypt(enc_h)):   raise ValueError("Bad salt hash.")

        return sf

    def get_slot_offset(self, slot):
        return (self.salt_sz + (self.max_ct_bits//8)) * slot

    def load_slot(self, slot, saltf=None):
        saltf = saltf or self.keyfile
        saltf.seek(self.get_slot_offset(slot))
        counter = int.from_bytes(saltf.read(self.countsz), "big")
        salt    = saltf.read(self.salt_sz)
        saltf.seek(self.get_slot_offset(slot+1))
        return counter, salt

    def hash_salts(self, saltf=None):
        saltf = saltf or self.keyfile    ; slots = (0,1,2,3)
        h = hashlib.blake2b(b"".join((self.load_slot(x, saltf)[1] for x in slots)),
                            digest_size=self.salt_sz).digest()
        enc_h = saltf.read()
        return h, enc_h

    def derive_key(self, salt, passphrase, size):
        key = bytearray(hashlib.scrypt(passphrase, salt=salt, n=2**19, r=8, p=1,
                                        maxmem=640*1024*1024, dklen=size))
        clear_array(passphrase)
        return key

    def derive_subkey(self, key, size, subslot, salt, context):
        skeys = Cryptodome.Protocol.KDF.HKDF(key, size, salt, Cryptodome.Hash.SHA512,
                                             max(subslot+1, 2), context)
        return bytearray(skeys[subslot])

    # Provide keyed hash func for manifests (send/receive)
    def getmhash_hmac(self, buf):
        return hmac.digest(self.mhashkey, buf, "sha256")

    # Update key counter on disk; call directly at end of transaction if cadence > 1
    def save_counter(self):
        self.keyfile.seek(self.slot_offset)
        self.keyfile.write(self.counter.to_bytes(self.countsz, "big"))
        self.keyfile.flush()

    # Update counter with a new value, if greater.  Update salt file location.
    def set_counter(self, ct, saltfile=None):
        if issubclass(type(saltfile), io.IOBase):
            self.keyfile = saltfile
        elif saltfile and saltfile != self.keyfile.name:
            self.keyfile = self.open_saltfile(saltfile, verify=True)

        self.counter = max(ct, self.load_slot(self.slot)[0])    ; self.save_counter()
        return self.counter

    # Decrypt [X]ChaCha20:
    def _dec_chacha20(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        return cipher.decrypt(untrusted_buf[self.nonce_sz:])

    # Decrypt [X]ChaCha20-Poly1305:
    def _dec_chacha20_poly1305(self, buf):
        untrusted_buf = buf
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)

    # Encrypt [X]ChaCha20 (protected counter nonce):
    def _enc_chacha20_ct(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.time_start - self.monotonic_start + time.monotonic())
                               - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        buf    = cipher.encrypt(buf)
        return  nonce, buf

    # Encrypt [X]ChaCha20-Poly1305 (protected counter nonce):
    def _enc_chacha20_poly1305_ct(self, buf):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.time_start - self.monotonic_start + time.monotonic())
                               - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (HMAC nonce: message & random inputs)
    def _enc_chacha20_msr(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || buf
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(buf)
        nonce   = nonce_h.digest()[:24]

        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)

    # Encrypt [X]ChaCha20-Poly1305 (HMAC nonce: message & random inputs)
    def _enc_chacha20_poly1305_msr(self, buf):
        self.counter += ceil(len(buf) // 16384)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || buf
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(buf)
        nonce   = nonce_h.digest()[:24]

        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (HMAC nonce: digest & random inputs)
    def _enc_chacha20_dgr(self, buf, mhash):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || Hm (manifest hash)
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(mhash)
        nonce   = nonce_h.digest()[:24]

        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)


# Define absolute paths of commands

class CP:
    awk    = "/usr/bin/gawk"    ; sed   = "/usr/bin/sed"     ; sort     = "/usr/bin/sort"
    cat    = "/usr/bin/cat"     ; mkdir = "/usr/bin/mkdir"   ; python   = "/usr/bin/python3"
    mv     = "/usr/bin/mv"      ; grep  = "/usr/bin/grep"    ; ssh      = "/usr/bin/ssh"
    sh     = "/usr/bin/sh"      ; tar   = "/usr/bin/tar"     ; tail     = "/usr/bin/tail"
    tee    = "/usr/bin/tee"     ; sync  = "/usr/bin/sync"    ; cp       = "/usr/bin/cp"
    chattr = "/usr/bin/chattr"  ; lsattr= "/usr/bin/lsattr"  ; sha256sum= "/usr/bin/sha256sum"
    cmp    = "/usr/bin/cmp"     ; gzip  = "/usr/bin/gzip"    ; env      = "/usr/bin/env"
    ps     = "/usr/bin/ps"      ; uniq  = "/usr/bin/uniq"    ; ionice   = "/usr/bin/ionice"
    qvm_run= "/usr/bin/qvm-run" ; rm    = "/usr/bin/rm"      ; find     = "/usr/bin/find"
    thin_delta = "/usr/sbin/thin_delta" ; filefrag = "/usr/sbin/filefrag"  ; lvm = "/usr/sbin/lvm"
    blkdiscard = "/sbin/blkdiscard"     ; dmsetup  = "/sbin/dmsetup"   ; xargs = "/usr/bin/xargs"
    btrfs      = "/usr/sbin/btrfs" if os.path.exists("/usr/sbin/btrfs") else "/bin/btrfs"


# Manage local (source) data volumes, thin lvm and reflink image files and snapshots.
# Volume names are mapped into the 'lvols' cache based on whether they are found
# under the current --local path or if a volume name exists in the archive.
# lvols enties may point to local volumes that are non-existant, so use v.exists().

class LocalStorage:

    BLKDISCARD               = 0x1277           ; BLKDISCARDZEROES        = 0x127c
    FALLOC_FL_KEEP_SIZE      = 0x01             ; FALLOC_FL_PUNCH_HOLE    = 0x02
    FALLOC_FL_COLLAPSE_RANGE = 0x08             ; FALLOC_FL_ZERO_RANGE    = 0x10
    FALLOC_FL_INSERT_RANGE   = 0x20             ; FALLOC_FL_UNSHARE_RANGE = 0x40
    FALLOC_FL_PUNCH_FULL     = FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE

    try:
        fallocate = ctypes.CDLL(ctypes.util.find_library("c")).fallocate
        fallocate.restype = ctypes.c_int
        fallocate.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int64, ctypes.c_int64]
    except:
        fallocate = None    ; print("fallocate() function not available.")

    def __init__(self, localpath, opts, auuid=None, arch_vols={}, clean=False, sync=False,
                 require_online=False):

        self.stypes  = { "tlvm":  LvmVolume, "rlnk": ReflinkVolume, "file": FileVolume,
                         None: FileVolume }
        self.rltypes = { "btrfs", "xfs" }
        self.opts    = copy.deepcopy(opts)

        assert len(auuid) > 8
        self.gc_procs  = []          ; self.locked  = False      ; self.auuid     = auuid
        self.clean     = clean       ; self.sync    = sync       ; self.arch_vols = arch_vols
        self.lvols, self.vgs_all = {}, {}
        self.users, self.groups  = {}, {}
        self.path = self.pooltype = self.fstype = self.lvpool = self.block_size = None
        self.online = self.can_snapshot = False
        self.prep_snapshots = prepare_snapshots_reflink

        loctype, locvol, locpool, pathxvg = LocalStorage.parse_local_path(localpath)
        ol_reason = ""

        if loctype is None:
            self.path = self.pooltype = None

        elif loctype in ("lvm volgroup", "tlvm pool"):
            self.pooltype       = "tlvm"          ; self.path    = "/dev/"+pathxvg+"/"
            self.block_size     = 512
            self.vgname         = pathxvg         ; self.lvpool  = locpool
            self.online  = self.can_snapshot = exists(self.path)

            self.acquire_deltas = get_lvm_deltas
            self.process_deltas = update_delta_digest_lvm
            self.prep_snapshots = prepare_snapshots_lvm

        elif loctype == "file" and exists(pathxvg):
            self.path       = pathxvg.rstrip("/")+"/"
            self.online, self.can_snapshot = True, False
            self.fstype     = fs = LocalStorage.get_fs_type(self.path)
            self.pooltype   = "rlnk" if fs in self.rltypes else "file"

            if fs == "btrfs":
                if os.stat(self.path).st_ino == 256:
                    self.snappath = self.path+"wyng_snapshot_tmp/"   ; self.can_snapshot = True
            elif fs == "xfs":
                self.snappath = ""   ; self.can_snapshot = True

            if self.can_snapshot:
                self.acquire_deltas = get_reflink_deltas
            self.process_deltas = update_delta_digest_reflink
            self.prep_snapshots = prepare_snapshots_reflink

        self.users  = {x[0]: int(x[2]) for x in
                        (ln.split(":") for ln in open("/etc/passwd","r") if ln.strip())}
        self.groups = {x[0]: int(x[2]) for x in
                        (ln.split(":") for ln in open("/etc/group","r") if ln.strip())}

        if require_online and not self.online:
            err_out("Local storage is offline: "+repr(localpath)+ol_reason)

        self.LVolClass = self.stypes[self.pooltype]
        if self.online:
            self.metadata_unlock()
            self.update_vol_list(arch_vols)

        if debug:
            print("**fstype is", self.fstype)
            print("**pooltype", self.pooltype, "not" if not self.online else "", "online")
            print(self.path, self.lvpool)

    def __del__(self):
        if self.clean:
            for p in self.gc_procs:   p.wait()


    # Note: file_punch_hole() and block_discard_chunk() have the same arg signature...
    def file_punch_hole(self, fn, start, length):
        if self.fallocate(fn, self.FALLOC_FL_PUNCH_FULL, start, length) == 0:
            return True
        else:
            return False

    def block_discard_chunk(self, fn, start, length):
        try:
            return fcntl.ioctl(fn, self.BLKDISCARD, struct.pack("LL", start, length))
        except OSError:
            return False
        else:
            return True

    def exists(self, volname):
        return exists(self.path + volname)

    def setperms(self, path, perms):
        if not perms:   return
        p_t = perms.split(":")    ; user = group = None

        if len(p_t) == 1:
            # set lvm type perm
            assert p_t[0] in ("r","w")
            pbits = 0o600 if p_t[0]=="w" else 0o400
        else:
            pbits, user, group = p_t    ; pbits = int(pbits)

        if os.path.isfile(path):
            os.chmod(path, stat.S_IMODE(pbits))
            if admin_permission and user and user in self.users:
                shutil.chown(path, user, group if group in self.groups else None)
        elif self.pooltype == "tlvm" and path.startswith(self.path):
            p  = "w" if pbits & stat.S_IWUSR else ""
            old= SPr.check_output([CP.lvm, "lvs", "--noheadings", "-o","lv_attr", path], text=True)
            if p != old.strip()[1]:
                do_exec([[CP.lvm, "lvchange", path, "-p", "r"+p]])
        else:
            raise ValueError("Bad non-file path: "+path)

    def getperms(self, path, vstat=None):
        vstat = vstat or os.stat(os.path.realpath(path))    ; uid, gid = vstat.st_uid, vstat.st_gid
        # use names in place of uid/gid numbers:
        user  = [x for x, num in self.users.items()  if num == uid][0]
        group = [x for x, num in self.groups.items() if num == gid][0]
        return f"{stat.S_IMODE(vstat.st_mode)}:{user}:{group}"

    def settime(self, path, t):
        if t > 0 and exists(path):
            os.utime(path, ns=(t,t))
            return True
        else:
            return False

    def getsize(self, path):
        with open(path, "rb") as f:
            return f.seek(0,2)

    def metadata_lock(self, lvpool=None):
        mark = 0    ; spath = self.path
        if self.pooltype == "tlvm":
            self._lvm_meta_snapshot("reserve", pool=lvpool)
        elif self.pooltype == "rlnk":
            if not self.fstype or self.fstype not in ("btrfs","xfs"):
                raise ValueError("Bad fstype "+repr(self.fstype))
            if self.fstype == "btrfs":
                mark, spath = self._btrfs_subvol_snapshot()
            elif self.fstype == "xfs":
                pass # possibly file-lock and chmod -r rlnk snapshots
        else:
            raise ValueError(f"'{self.pooltype}' pooltype, no metadata.")

        self.locked = True
        return mark, spath

    def metadata_unlock(self, lvpool=None):
        mark_t = (0, self.path)
        if self.pooltype == "tlvm":
            for pl in {lvpool} if lvpool else {getattr(x, "pool_lv", self.lvpool)
                                               for x in self.lvols.values()}:
                if pl:   self._lvm_meta_snapshot("release", pool=pl)

        elif self.pooltype == "rlnk":
            if self.fstype == "btrfs":
                mark_t = self._btrfs_subvol_snapshot(delete=True)
        else:
            if debug:   print(f"'{self.pooltype}' pooltype, no metadata.")

        self.locked = False
        return mark_t

    def check_support(self):
        if self.pooltype == "tlvm":
            for prg in (CP.lvm, CP.dmsetup, CP.thin_delta ):
                if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)
            try:
                p = SPr.check_output([CP.thin_delta, "-V"])
            except:
                p = b""
            ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
            if p and ver < target_ver:
                raise RuntimeError("Thin provisioning tools version >= "+target_ver+" required.")
        elif self.pooltype == "rlnk" and self.fstype == "btrfs":
            if not shutil.which(CP.btrfs):  raise RuntimeError("Required command not found: btrfs")
            # maybe also check kernel version and xfs reflink support...

    def _btrfs_subvol_snapshot(self, delete=False):
        if delete and not self.can_snapshot:   return None, None

        svpath = self.path    ; dest = self.snappath    ; gen = 0    ; sv_start = time.monotonic()

        # remove snapshot at wrong path
        if exists(badpath := self.path.rstrip("/")+os.path.basename(dest.rstrip("/"))):
            do_exec([[CP.env, "-u", "LC_ALL",
                      CP.btrfs, "subvolume", "delete", "-c", badpath]], check=False)

        if debug:   print(f"\nSubvol snapshot: '{dest}' {exists(dest)} del={delete}", flush=True)
        if exists(dest):
            if delete:   gen = self._get_btrfs_generation(dest)
            do_exec([[CP.env, "-u", "LC_ALL",
                      CP.btrfs, "subvolume", "delete", "-c", dest]])
        if not delete:
            do_exec([[CP.env, "-u", "LC_ALL",
                      CP.btrfs, "subvolume", "snapshot", "-r", svpath, dest]])
            gen = self._get_btrfs_generation(dest)
            # possibly check /sys/fs/btrfs/uuid#/exclusive_operation

            if debug:   print("Created subvol (sec):", time.monotonic() - sv_start, flush=True)
        return gen, dest

    def _get_btrfs_generation(self, path):
        base = "/"+os.path.basename(path.rstrip("/"))
        res = SPr.check_output([CP.env, "-u", "LC_ALL", CP.btrfs, "subvolume", "list", path], text=True)
        res = [x for x in res.splitlines() if x.strip().endswith(base)]
        if debug:   print("*generation", res)
        rln = res[0].split()    ; assert len(res) == 1 and rln[2] == "gen"
        return int(rln[3])

    # Reserve or release lvm thinpool metadata snapshot.
    # action must be "reserve" or "release".
    def _lvm_meta_snapshot(self, action, pool=None):
        vgname   = self.vgname.replace("-","--")
        poolname = (pool or self.lvpool).replace("-","--")

        if debug:   print(f"TLVM meta snapshot: {vgname}/{poolname} {action}", flush=True)
        do_exec([[CP.dmsetup,"message", vgname+"-"+poolname+"-tpool",
                "0", action+"_metadata_snap"]], check= action=="reserve")

    def new_vol_entry(self, newname, vid, replace=False):
        if newname not in (lvols := self.lvols) or replace:
            lvols[newname] = vol = self.LVolClass(self, newname, vid=vid)
        else:
            vol = lvols[newname]    ; vol.vid = vid
        self.arch_vols[newname] = vid

        for sv in (vol.snap1, vol.snap2) if vol.snap1 else []:
            if sv and sv not in lvols:   lvols[sv] = self.LVolClass(self, sv, parent=newname)
        return vol

    # Create survey of all interesting volumes
    def update_vol_list(self, arch_vols):
        if not self.online:   return
        self.lvols.clear()
        if self.pooltype == "tlvm":   self.update_lvm_list()
        for vname, vid in arch_vols.items():
            self.new_vol_entry(vname, vid)

    # Retrieves survey of all LVM VGs/LVs
    def update_lvm_list(self, vol=None):
        if not self.online:   return
        if not shutil.which(CP.lvm):   sys.stderr.write("LVM not available.\n"); return

        delim   = ":::"            ; colnames = list(LvmVolume.colnames)
        vgs_all = self.vgs_all     ; LvmVol   = LvmVolume

        cmd = [CP.lvm, "lvs", "--units=b", "--noheadings", "--separator="+delim,
                    "--options=" + ",".join(colnames)]
        if vol and not exists(vol.path):
            return
        elif vol:
            cmd.append(vol.path)
        do_exec([cmd], out=tmpdir+"/volumes.lst")

        for ln in open(tmpdir+"/volumes.lst", "r"):
            m  = dict(zip(colnames, ln.strip().split(delim))); m["tags"]= set(m["tags"].split(","))
            lv = LvmVol(self, "", members=m)
            vgs_all.setdefault(lv.vg_name, {})[lv.name] = lv

        self.lvols = vgs_all[self.vgname]

    # static
    def vg_exists(vgname):
        try:
            do_exec([[CP.lvm, "vgdisplay", vgname]])
        except (SPr.CalledProcessError, FileNotFoundError):
            return False
        else:
            return True

    def get_fs_type(path):
        mtab = {x[1]: x[2] for x in map(str.split, open("/etc/mtab","r"))}
        while path not in mtab and path != "/":   path = os.path.dirname(path)
        return mtab[path]

    # accepts either a volgroup/pool or directory path and returns:
    # type, tvol, tpool, vg or abs path
    def parse_local_path(localpath):
        if type(localpath) is not str or not localpath.isprintable():
            return (None, None, None, None)

        abspath = os.path.abspath(os.path.realpath(localpath))
        if LocalStorage.vg_exists(localpath):
            return ("lvm volgroup", None, None, os.path.basename(localpath))

        lvname, lvpool, vg, lvattr = LocalStorage.get_lv_path_pool(localpath)

        if lvname and not lvpool and lvattr.startswith("t"):
            return ("tlvm pool", None, lvname, vg)
        elif lvname and lvpool and lvattr.startswith("V"):
            return ("tlvm volume", lvpool, lvname, vg)
        elif abspath.startswith("/dev"):
        #and stat.S_ISBLK(os.stat(localpath).st_mode):
            return ("block device", None, None, abspath)
        elif abspath.startswith("/") and exists(abspath) and os.path.isdir(abspath):
            return ("file", None, None, abspath)
        else:
            return (None, None, None, None)

    # Converts a non-cannonical LV path to LV name plus pool and vg names.
    def get_lv_path_pool(path):
        try:
            p = SPr.run([CP.lvm, "lvs", "--separator=:::", "--noheadings",
                                "--options=lv_name,pool_lv,vg_name,attr", path], check=True,
                                stdout=SPr.PIPE, stderr=SPr.DEVNULL)
        except:
            return "", "", "", ""
        else:
            return p.stdout.decode("utf-8").strip().split(":::")


# Base class for local volumes; do not instantiate.

class LocalVolume:
    maxname   = 255    ; maxpath = 4096 - maxname
    __slots__ = ("storage","lockfile","pdir","path","name","vid","snap1","snap2","parent","stat")

    def _my_init(self, storage, name, vid=None, parent=None):
        self.storage   = storage                        ; self.lockfile = None
        self.pdir      = storage.path.rstrip("/")+"/"   ; self.path     = self.pdir+name
        self.name      = name                           ; self.vid      = vid
        self.snap1 = self.snap2 = None                  ; self.parent   = parent

    def lock(self, mode="r+b"):
        self.lockfile = lf = open(self.path, mode)
        fcntl.lockf(lf, fcntl.LOCK_EX|fcntl.LOCK_NB)
        return lf

    def unlock(self):
        if self.lockfile:   self.lockfile.close()
        self.lockfile = None
        return True

    def add_tags(self, addtags):
        raise NotImplementedError()

    def del_tags(self, tags=[]):
        raise NotImplementedError()

    def rotate_snapshots(self, rotate=True, delsnap2=True, timestamp_path=None, addtags=[]):
        assert not self.name.endswith(self.snap_ext)
        if debug:
            print("Rotate Snapshots", self.name, rotate, timestamp_path, addtags, flush=True)

        lvols = self.storage.lvols
        if rotate:
            if lvols[self.snap2].exists():
                lvols[self.snap1].delete(sync=True)
                lvols[self.snap2].rename(self.snap1, addtags=addtags)
                with open(self.pdir+self.snap1,"rb") as vf:   os.fsync(vf)
        else:
            if delsnap2:   lvols[self.snap2].delete(sync=False)
            if addtags and lvols[self.snap1].exists() and type(self) is LvmVolume:
                lvols[self.snap1].del_tags((x for x in lvols[self.snap1].tags
                                              if x.startswith("S_")))
                lvols[self.snap1].add_tags(addtags)

        if timestamp_path and lvols[self.snap1].exists():
            t = lvols[self.snap1].gettime()    ; os.utime(timestamp_path, ns=(t,t))
            return t

        return None

    def setperms(self, perms):
        self.storage.setperms(self.path, perms)

    def exists(self):
        return self.storage.online and exists(self.path)

    def check_pathname(path):
        if not 0 < len(path) <= LocalVolume.maxpath:
            return f"Volume path/name length must be 1 - {LocalVolume.maxpath}."
        if not path.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in path.strip().split("/")]):
            return "Bad volume name."


class FileVolume(LocalVolume):

    snap_ext = (None, None)

    def __init__(self, storage, name, vid=None, parent=None):
        super()._my_init(storage, name, vid, parent=parent)
        self.stat = None; self.update()

    def update(self):
        try:
            self.stat = os.stat(self.path)
        except:
            self.stat = None
        return self

    def resize(self, size):
        with open(self.path, "r+b") as vf:   vf.truncate(size); vf.flush()

    def gettime(self):
        return self.stat.st_mtime_ns

    def getperms(self):
        return self.storage.getperms(self.path, vstat=self.stat)

    def getsize(self):
        return self.stat.st_size


class ReflinkVolume(FileVolume):

    snap_ext = (".wyng1",".wyng2")

    def __init__(self, storage, name, vid=None, parent=None):
        super().__init__(storage, name, vid, parent=parent)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            fdir, fname  = os.path.split(name)    ; subdir = fdir+"/" if fdir else ""
            self.snap1 = "".join((subdir,"sn",self.storage.auuid,"_",self.vid,self.snap_ext[0]))
            self.snap2 = "".join((subdir,"sn",self.storage.auuid,"_",self.vid,self.snap_ext[1]))

    def rename(self, new_name, addtags=[]):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        assert self.exists()

        newvol = self.storage.lvols[new_name]
        if debug:   print("*rename", self.path, newvol.path)
        subdir, fname  = os.path.split(newvol.name)
        os.makedirs(newvol.pdir+subdir, exist_ok=True)
        self.unlock()    ; os.replace(self.path, newvol.path)
        self.stat = None ; newvol.update()

    def delete(self, sync=True, check=False, force=False):
        assert force or self.name.endswith(self.snap_ext)
        self.unlock()
        if self.exists():
            if debug:   print("Del vol", self.name, flush=True)
            os.remove(self.path)
        self.stat = None

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        assert self.storage.online and self.storage.fstype in self.storage.rltypes
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        if debug:   print("Create vol", self.name, "from", snapshotfrom, flush=True)

        subdir, fname  = os.path.split(self.name)
        os.makedirs(self.pdir+subdir, exist_ok=True)
        if snapshotfrom:
            snap_path = self.storage.lvols[snapshotfrom].path
            do_exec([[CP.env, "-u", "LC_ALL", CP.cp, "-p", "--reflink=always",
                      snap_path, self.path]])
        else:
            assert size is not None
            with open(self.path, "wb") as vf:  vf.truncate(size); vf.flush()

        if ro:   rel_chmod(self.path, "-", 0o222)
        self.update()

        self.storage.lvols[self.name] = self
        return self

    def is_arch_member(self):
        fname  = os.path.split(self.name)[-1]
        if not fname.startswith("".join(("sn",self.storage.auuid,"_"))) \
        or not fname.endswith(self.snap_ext) or not self.exists():
            return "na"
        elif self.parent and fname.startswith("".join(("sn",self.storage.auuid,"_"))):
            return "true"
        else:
            return "false"

    def paired_state(self, mapfile, sestag):
        if not self.name.endswith(self.snap_ext):   raise ValueError("Not a snapshot.")

        if self.is_arch_member() != "true" or not exists(mapfile) \
        or self.gettime() != os.stat(mapfile).st_mtime_ns:
            return "N"
        elif os.stat(mapfile).st_blocks != 0:
            return "D"
        else:
            return "Y"
        #Enh: also evaluate sestag


class LvmVolume(LocalVolume):

    snap_ext  = (".tick",".tock")
    maxname   = 112    ; maxpath = maxname    ; alphanumsym = r"^[a-zA-Z0-9\+\._-]+$"
    colnames  = {"vg_name","lv_name","lv_attr","lv_size",
                 "lv_time","pool_lv","thin_id","tags"}
    __slots__ = colnames

    def __init__(self, storage, name=None, vid=None, members={}, parent=None):

        self.tags = set()
        for attr in set(members) & self.colnames:
            setattr(self, attr, members[attr])

        super()._my_init(storage, name or self.lv_name, vid, parent=parent)
        name = self.name    ; assert bool(name)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            self.snap1 = name + self.snap_ext[0]
            self.snap2 = name + self.snap_ext[1]

    def add_tags(self, tags):
        do_exec([[CP.lvm, "lvchange", self.path] + ["--addtag="+x for x in tags]])
        self.tags |= set(tags)

    def del_tags(self, tags):
        do_exec([[CP.lvm, "lvchange", self.path] + ["--deltag="+x for x in tags]])
        self.tags -= set(tags)

    def rename(self, new_name, addtags=[]):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        assert self.exists()
        storage = self.storage

        if addtags:   self.add_tags(addtags)

        m = {x: getattr(self,x) for x in self.colnames - {"lv_name","lv_path","thin_id"}}
        nv = storage.lvols[new_name] = LvmVolume(storage, name=new_name, vid=self.vid, members=m)
        storage.new_vol_entry(self.name, self.vid, replace=True)

        self.unlock()
        do_exec([[CP.lvm, "lvrename", self.path, new_name]])

    def delete(self, sync=True, check=False, force=False):
        # Enh: re-write with asyncio
        sync = (optsync := options.maxsync) or sync    ; clean = options.clean # using global
        assert not (sync == False and check)
        assert force or self.name.endswith(self.snap_ext)

        if self.exists():
            self.unlock()    ; procs = self.storage.gc_procs    ; maxprocs = 16
            if len(procs) == maxprocs:
                for ii in reversed(range(len(procs))):
                    retcode = procs[ii].returncode
                    if retcode is not None:
                        if clean and retcode != 0:
                            raise CalledProcessError("lvremove failed "+str(retcode))
                        del(procs[ii])

            if debug:   print("Del vol", self.name, flush=True)
            cmds = [CP.sh, "-c", CP.lvm + " lvchange -p rw " + self.path + " ; "
                 +  CP.lvm + " lvremove -f " + self.path]
            if not (sync or clean):   cmds = [CP.ionice, "-c3"] + cmds
            p = SPr.Popen(cmds, shell=False,
                                stdout=SPr.DEVNULL, stderr=SPr.DEVNULL)
            if not sync and len(procs) < maxprocs:
                procs.append(p)
            else:
                retcode = p.wait()
                if check and retcode != 0:
                    raise CalledProcessError("lvremove failed "+str(retcode))

            for col in self.colnames:   setattr(self, col, None)

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        vg = self.storage.path    ; rwmode = ["-pr"+("w" if not ro else "")]
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        if debug:   print("Create vol", self.name, "from", snapshotfrom, flush=True)
        if snapshotfrom:
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay"] + rwmode
                               + ["--addtag="+x for x in addtags]
                               + ["-s", vg+"/"+snapshotfrom, "-n", self.name]])
        else:
            assert size is not None
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(size)+"b"] + rwmode
                      + addtags + ["--thin", "-n", self.name, vg+"/"+self.storage.lvpool]])

        return self.update()

    def update(self):
        self.storage.update_lvm_list(vol=self)
        return self.storage.lvols[self.name]

    def gettime(self):
        return int(time.mktime(time.strptime(self.lv_time, r"%Y-%m-%d %H:%M:%S %z"))*1000000000)

    def resize(self, size):
        if not self.exists():   raise FileNotFoundError(self.path)
        do_exec([[CP.lvm, "lvresize", "-L", str(size)+"b", "-f", self.path]])
        return self.update()

    def getperms(self):
        return self.lv_attr[1]

    def getsize(self):
        return int(self.lv_size.split("B")[0])

    def is_arch_member(self):
        if not self.exists() or not self.name.endswith(self.snap_ext) or "wyng" not in self.tags \
        or not self.storage.auuid:
            return "na"
        if not self.tags or "arch-"+self.storage.auuid not in self.tags:
            return "false"
        else:
            return "true"

    def paired_state(self, mapfile, sestag):
        if not self.name.endswith(self.snap_ext):   raise ValueError("Not a snapshot.")

        if self.is_arch_member() != "true" or not exists(mapfile) \
        or self.gettime() != os.stat(mapfile).st_mtime_ns or sestag not in self.tags:
            return "N"
        elif os.stat(mapfile).st_blocks != 0:
            return "D"
        else:
            return "Y"

    def check_pathname(path):
        if res := super.check_pathname(path):
            return res
        if re.match(LvmVolume.alphanumsym, path) is None:
            return "Only characters A-Z 0-9 . + _ - are allowed in LVM volume names."
        return ""

    def convert_pathname(self, path):
        raise NotImplementedError()


# Try to sync only selected filesystem
def fssync(path, force=False, sync=False):
    if options.maxsync or force: # global options
        rc = SPr.check_output([CP.sync,"-f",path]) if sync else SPr.Popen([CP.sync,"-f",path])


def rel_chmod(fpath, op, mask):
    assert op in ("-","+")
    current = stat.S_IMODE(os.stat(fpath).st_mode)
    os.chmod(fpath, (current & ~mask) if op=="-" else (current | mask))


def get_passphrase(prompt="Enter passphrase: ", verify=False, cmd=None):
    if cmd:
        try:
            b = bytearray(SPr.check_output(CP.env + " -u LC_ALL " + cmd,
                                              shell=True, text=False).rstrip(b"\n"))
        except SPr.CalledProcessError as e:
            # Handle as error code so pw output does not get logged
            err_out(f"\nError code {e.returncode} running passcmd: "+cmd)
            raise e

        if len(b) < 10:
            clear_array(b)    ; raise InputError("Passphrase must be 10 or more characters.")
        return b

    def getpassinput(prompt):
        if attended:
            return getpass.getpass(prompt)
        else:
            err_out(prompt)
            return sys.stdin.readline().rstrip("\n")

    attended = sys.stdin.isatty()
    for ii in range(3):
        passphrase  = bytearray(getpassinput(prompt), encoding="UTF-8")
        if not verify or len(passphrase) >= 10 or (debug and len(passphrase) > 0):   break
        err_out("Passphrase must be 10 or more characters.")   ; clear_array(passphrase)
        if ii == 2 or not attended:   raise InputError("Passphrase required.")
    if not attended or not verify:   return passphrase
    passphrase2 = bytearray(getpass.getpass("Re-enter passphrase: "), encoding="UTF-8")
    if passphrase != passphrase2:
        clear_array(passphrase)   ; clear_array(passphrase2)
        raise InputError("Entries do not match.")
    clear_array(passphrase2)
    return passphrase


def clear_array(ar):
    for ii in range(len(ar)):   ar[ii] = 0


# Initialize a new ArchiveSet:

def arch_init(aset):

    if not zstd:   print("Package python3-zstd is not installed.")
    opts = aset.opts    ; data_ci = opts.encrypt or "xchacha20-dgr"
    agent_get(aset.dest.agent_name, -1)  # cancel old auth

    # Fix: duplicates code in aset... move to aset class.
    if data_ci in (x[0] for x in DataCryptography.crypto_codes.values() if x[2]):
        aset.ci_mode, ci_t = [(x,y) for x,y in DataCryptography.crypto_codes.items()
                                    if y[0] == data_ci][0]

        if data_ci != "off":
            # Security Enh: Possibly use mmap+mlock to store passphrase/key values,
            #               and wipe them from RAM after use.
            passphrase      = get_passphrase(prompt="Enter new encryption passphrase: ",
                                             verify=True, cmd=opts.passcmd)
            aset.datacrypto = DataCryptography()
            aset.mcrypto    = DataCryptography()
            aset.mcrypto.load(ci_t[1], aset.saltpath, slot=1,
                                     passphrase=passphrase[:], init=True)
            aset.datacrypto.load(data_ci, aset.mcrypto.keyfile, slot=0,
                                     passphrase=passphrase)
    else:
        raise OptionError("Invalid cipher option.")

    print(); print(f"Encryption    : {data_ci}", "" if data_ci=="off" else f"({ci_t[1]})")

    if opts.hashtype:
        if opts.hashtype not in hash_funcs or opts.hashtype == "sha256":
            raise OptionError("Hash function '"+opts.hashtype+"' is not available on this system.")

    # Use hmac-sha256 as data hash if the mode supports it:
    aset.hashtype = "hmac-sha256" if aset.datacrypto and aset.datacrypto.mhashkey else "blake2b"
    print("Data Hashing  :", aset.hashtype)

    if opts.compression:
        if ":" in opts.compression:
            compression, compr_level = opts.compression.strip().split(":")
        else:
            compression = opts.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not is_num(compr_level):
            raise OptionError("Invalid compression spec.")

        aset.compression = compression      ; aset.compr_level = compr_level
        compressors[compression][2](b"test", int(compr_level))

    print("Compression   : %s:%s" % (aset.compression, aset.compr_level))

    chfactor = 3
    if opts.chfactor:
        # accepts an exponent from 1 to 6
        chfactor = int(opts.chfactor)
        if not ( 0 < chfactor < 7 ):
            raise OptionError("Requested chunk size not supported.")
    aset.chunksize = (aset.min_chunksize//2) * (2** chfactor)
    if aset.chunksize != 256 * 1024:
        print("Chunk size set:", aset.chunksize)

    aset.save_ini(init=True)    ; fssync(aset.path, force=True, sync=True)
    update_dest(aset, pathlist=([aset.saltname, "salt.bak"]
                                if aset.datacrypto else []) + [aset.confname], sync=True)
    return aset


# Check/verify an entire archive

def arch_check(storage, aset, vol_list=None, startup=False, upgrade=False):
    options  = aset.opts                   ; error_cache.clear()
    dest     = aset.dest                   ; attended  = not options.unattended
    compare_digest = hmac.compare_digest   ; b64enc    = base64.urlsafe_b64encode

    if upgrade and aset.format_ver == 2 and sum(len(v.sessions) for v in aset.vols.values()):
        upgrade_from_v2(aset)    ; return
    elif upgrade:
        raise OptionError("Cannot upgrade.")

    if options.change_uuid:
        if not (options.force or attended and
                ask_input("\nChange archive UUID to a new random value;"
                          "\nAre you sure? (y/N): ").lower() in {"y","yes"}):
            return
        print("Old UUID:", oldid := aset.uuid)
        while (newid := str(uuid.uuid4())) == oldid: time.sleep(0.2)
        print("New UUID:", newid)    ; aset.uuid = newid
        catch_signals()    ; aset.save_ini(ext=".tmp")
        update_dest(aset, pathlist=[aset.confname], ext=".tmp")
        aset.rename_saved(ext=".tmp")
        print("UUID updated.")    ; return

    # Test volume metadata loading
    for vid in aset.conf["volumes"] if not startup and dest.online else []:

        # try volume "volinfo" only
        v_errs = aset.load_volumes(1, vids=[vid], handle=True)
        for v, e in v_errs:
            err_out(v+" - "+repr(e)); error_cache.append(v)

        # Enh: Offer to reconstruct volinfo from available session data
        if v_errs:
            raise ValueError(f"\n{vid} - Invalid volume metadata.")

        # try sessions
        vol = [x for x in aset.vols.values() if x.vid == vid][0]
        ses_errs = vol.load_sessions(handle=True, mfdecode=True)
        seslist  = list(vol.sessions.values())    ; removed = []

        # un-loaded sessions cannot be sorted, so remove them from seslist
        for ses, sn, e in ((vol.sessions[x], x, y) for x, y in ses_errs):
            if ses in seslist:    del(seslist[seslist.index(ses)])
            removed.append(sn)   ; err_out(vol.name+" - "+sn+" - "+repr(e))

        if ses_errs:
            # find last good session and reset volume to it:

            # flag volume and sort the session list by sequence#
            error_cache.append(vol.name)    ; seslist.sort(key=lambda x: x.sequence)   ; si = -1

            # find longest coherent chain of sessions
            for si in range(len(seslist)):
                if not check_session_seq(vol, seslist=seslist, seq=si):   break
            else:
                si += 1

            if si < 1:
                err_out(f"\nVolume '{vol.name}' ({vid}) cannot be recovered.")
            else:
                sgood = seslist[si-1].name
                print(f"\nVolume '{vol.name}' error in session: {sn[2:]}"
                        f"\nLast good session: {sgood[2:]}")
                if removed and not options.unattended:
                    ans = ask_input("Rewind volume to this session to recover? ")
                    if ans.lower() in ("y","yes"):

                        print("\nRewinding volume to", sgood[2:])
                        while (ses := seslist.pop()).name != sgood:
                            removed.append(ses.name)
                        for sn in removed:
                            if sn in vol.sessions:   del(vol.sessions[ses.name])
                        vol.save_volinfo(ext=".tmp")
                        catch_signals()
                        update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
                        vol.rename_saved(ext=".tmp")
                        catch_signals(**signormal)
                        dest.run([f"rm -rf {vid}/{x}\n" for x in removed], destcd=dest.path)
                        for s in removed:
                            shutil.rmtree(f"{vol.path}/{s}", ignore_errors=True)
                        x_it(0, "Done.")
            return

        else:
            print(f"{vol.vid}: OK", end="\r")

    # Remove orphan snapshots. Don't test storage.online bc --local might not be CoW path.
    if storage.path and exists(storage.path):
        remove_local_snapshots(storage, aset)

    # Handle --all.  Select only newest single session if vol list not given (implied --all)
    # causing the data check to exclude vols not in that session.
    if options.all:   vol_list = sorted(set(aset.vols.keys()) - set(options.volex))
    all_selected, selses = bool(vol_list), options.session

    if selses == "newest":
        if not all_selected and (res := sorted(x.last for x in aset.vols.values() if x.sessions)):
            selses = res[-1][2:]
        else:
            selses = None
    if selses and verbose:   print(f"Session {selses} selected.")

    vol_list = vol_list or sorted(set(aset.vols.keys()) - set(options.volex))
    vol_dirs = {x.name for x in os.scandir(aset.path) \
                       if x.is_dir() and x.name.startswith("Vol_")}

    vdir_strays = vol_dirs - set(aset.conf["volumes"].keys())
    if vdir_strays:
        print("Stray volume dirs:", vdir_strays)
        if options.clean and dest.online:
            assert len([v for v in aset.vols.values() if v.vid in vdir_strays]) == 0
            dest.run(["rm -rf"] + vdir_strays, destcd=dest.path)
            print("Stray dirs deleted.")

    # Check volume contents at various levels (dirs, metadata, content)
    for volname, vol in ((vn,v) for vn,v in aset.vols.items() if vn in vol_list and dest.online):

        if selses and "S_"+selses not in vol.sessions and not all_selected:
            continue

        if not startup or debug:   print(f"\nVolume '{volname}'", flush=True)

        # Remove session tmp dirs
        for sdir in os.scandir(vol.path):
            if sdir.name.startswith("S_") and sdir.name.endswith("-tmp"):
                if debug:   print("Removing partial session dir '%s'" % sdir.name)
                dest.run(["rm -rf " + vol.vid+"/"+sdir.name], destcd=dest.path)
                shutil.rmtree(vol.path+"/"+sdir.name, ignore_errors=True)

        if len(vol.sessions) > 1 and exists(vol.mapfile(-2)):   os.remove(vol.mapfile(-2))

        if not check_session_seq(vol):   raise ValueError("Session out of sequence error.")
        if startup:   continue

        # Remove stray volume dirs and other files
        deepclean = options.clean and options.force
        for pos in list(range(len(vol.sessions)))[:-1]:
            if exists(vol.mapfile(pos)):   os.remove(vol.mapfile(pos))

        # Check all combined manifests are correct
        print("  Checking indexes,", end="", flush=True)
        for ses in vol.sesnames:
            check_manifest_sequence(vol, [ses], full=ses == vol.sesnames[0])
            #### FIX: implement deepclean

        # Check hashes of each session individually
        print(" data:")    ; newest = options.session in {"newest"}
        for sesname in reversed(vol.sesnames):

            if (selses and "S_"+selses != sesname) \
            or (newest and not selses and sesname != vol.last):
                continue

            print(" ", sesname[2:], end=" :", flush=True)
            bcount = receive_volume(storage, vol, select_ses=sesname[2:], ses_strict=True,
                                    verify_only=2)
            #if attended:   print(bcount, "bytes OK")

def check_session_seq(vol, seslist=None, seq=None):
    seslist = seslist or vol._seslist    ; err = False
    if seq is None:   seq = len(seslist)-1
    if not seslist:   return True

    if seslist[0].previous != "None":
        err_out(f"Missing oldest. [0] = {seslist[0].name} -> {seslist[0].previous}"); err = True
    for si, ses in enumerate(seslist[1:seq+1]):
        if ses.previous != seslist[si].name:
            err_out(f"Prev Out of sequence: {ses.name} -> {seslist[si].name}"); err = True
        if ses.sequence == seslist[si].sequence:
            err_out(f"Duplicate sequence {ses.sequence} in {ses.name}"); err = True

    return not err

def check_manifest_sequence(vol, sesnames, full=False, addcol=False):
    volsize   = vol.sessions[sesnames[-1]].volsize    ; aset = vol.archive    ; addr = 0
    chunksize = aset.chunksize    ; sesname_sz = vol.sesname_sz
    manifest  = merge_manifests(vol, msessions=sesnames, vsize=volsize, addcol=addcol) \
               if len(sesnames) > 1 or addcol else vol.sessions[sesnames[-1]].path+"/manifest"
    with open(manifest, "r") as mrgf:
        for ln in mrgf:
            ln1, ln2 = ln.strip().split(maxsplit=1)
            if addcol:   ln2, ses = ln2.split()
            if ln1 != "0":
                if len(ln1) != aset.mhash_sz:
                    raise ValueError(f"Bad hash size: {ln1}\n{manifest}")
                h1 = base64.urlsafe_b64decode(ln1)
            if len(ln2) != sesname_sz:
                raise ValueError(f"Bad size: {ln2}\n{manifest}")
            a1 = int("0"+ln2, 16)
            if not (a1 <= volsize and a1 % chunksize == 0):
                raise ValueError(f"Chunk addr out of bounds: {ln}\n{manifest}")
            if full:
                if addr != a1:
                    raise ValueError(f"Seq error. Expected {addr} got {ln2} for vsize {volsize}."
                                     f"\n{manifest}")
                addr += chunksize
            else:
                if not a1 >= addr:
                    raise ValueError(f"Out of sequence: {ln2}\n{manifest}")
                addr = a1 + chunksize

    if full and addr < vol.last_chunk_addr(volsize)[0]:
        raise ValueError(f"Range stopped short at {addr}\n{manifest}")

    if not manifest.startswith(vol.path):   os.remove(manifest)


# Remove Wyng snapshots and metadata from local system
#   archive == None: purge all wyng snapshots
#   archive != None and purge_other: purge snapshots except those linked to archive
#   archive != None and not purge other: purge only snapshots linked to archive

def remove_local_snapshots(storage, archive, del_all=False, purge=None):
    purge_other = purge == "other"    ; del_all = purge == "full"
    assert not (purge_other and not archive)

    if not storage.arch_vols and archive:
        storage.update_vol_list({x.name: x.vid for x in archive.vols.values()})

    # Remove LVM snapshots
    for lv in (x for vg in storage.vgs_all.values() for x in vg.values()):
        if lv.name.endswith(lv.snap_ext) and "wyng" in lv.tags:

            if not archive or (purge_other and lv.is_arch_member() == "false") \
            or (del_all and lv.is_arch_member() == "true" ):
                lv.delete()
                if verbose:   print("Removed", lv.name)

    # Remove reflink snapshots by scanning local dir
    if storage.pooltype == "rlnk":
        exts = ReflinkVolume.snap_ext    ; lvols = storage.lvols
        if archive:
            archpaths = [x.path for x in lvols.values() if x.exists()]

        for ldir, _, files in os.walk(storage.path):
            for fname in (x for x in files if x.startswith("sn") and x.endswith(exts)):
                fpath = pjoin(ldir,fname)
                if not archive or (fname.startswith("sn"+archive.uuid)
                                   and (fpath not in archpaths or del_all)) \
                or (purge_other and not fname.startswith("sn"+archive.uuid)):
                    os.remove(fpath)
                    if verbose:   print("Removed", fpath)


# Parse args and global conf file

def parse_options(args, pdefs, altconf=""):
    conffile = altconf or (wyng_conf if exists(wyng_conf) else "")
    conf_block = {"config","debug","force","save-to","session","unattended","volume-desc","tag",
                  "all","all-before","vols-from","import-other-from","upgrade-format", "help",
                  "skip-corrupt-chunks","force-retry"}

    if conffile and (altconf or not any(map(lambda x: x.startswith("--config"), args))):
        confp = configparser.ConfigParser()    ; confp.optionxform = lambda option: option
        confp["var-global-default"] = {}       ; confp.read_file(open(conffile, "r"))
        parser_defs_d = {x[0]: y for x, y in pdefs}
        for k, v in confp["var-global-default"].items():
            if k in conf_block:
                err_out(f"Ignoring config item: {k}")
            elif (kopt := "--"+k) in parser_defs_d:
                if parser_defs_d[kopt].get("action") == "append":
                    parser_defs_d[kopt]["default"] = list(filter(None, v.splitlines()))
                elif parser_defs_d[kopt].get("action") == "count":
                    parser_defs_d[kopt]["default"] = int(v)
                elif parser_defs_d[kopt].get("action") in ("store_true","store_false"):
                    parser_defs_d[kopt]["default"] = bool(int(v))
                else:
                    parser_defs_d[kopt]["default"] = v
            else:
                raise OptionError(f"Unknown item in config: {k}")

    parser = argparse.ArgumentParser(prog=prog_name, description=prog_info[0], epilog=prog_info[1],
                                     formatter_class=argparse.RawTextHelpFormatter)
    for x, y in pdefs:  parser.add_argument(*x, **y)
    options = parser.parse_args(args)   ; options.action = options.action.lower()

    if options.config and not altconf:
        return parse_options(args, pdefs, altconf=options.config)

    # Check consistency of arguments
    if options.vols_from and options.action not in ("send","monitor","receive","verify","diff"):
        raise OptionError("vols-from cannot be used with "+options.action)
    if options.action in write_actions+("monitor",) and not admin_permission:
        raise OptionError(f"Must be root/admin user for {options.action}.")
    if options.save_to and options.action != "receive":
        raise OptionError("Receive required for save-to option.")
    if not (options.volumes or options.all or options.import_other_from or options.vols_from) \
    and options.action not in ("arch-init","arch-check","arch-deduplicate",
                               "list","delete","version"):
        raise OptionError(f"Volume name(s) or --all required for {options.action}.")
    if options.action not in ("send", "diff", "arch-deduplicate"):   options.dedup = False
    if options.action == "arch-deduplicate":   options.dedup = True

    if (mr := options.meta_reduce.split(":"))[0] not in ("off","on","extra") \
    or len(mr) != 2 or not is_num(mr[1]):
        raise OptionError("Malformed --meta-reduce argument.")
    options.unattended = options.unattended or not sys.stdin.isatty()

    return options


# Get configuration settings:

def get_configs(opts):

    dest_url, local_url = opts.dest, opts.local
    dest = Destination(dest_url, user_ssh_opts=opts.opt_ssh, user_qubes_opts=opts.opt_qubes)
    dest.detect_state(opts.dedup, ro=opts.action not in write_actions)

    if debug:   print("\nSSH & VM opts:", dest.ssh_opts, "\n", dest.qubes_opts, "\n")

    # Check online status for certain commands.
    if not dest.online and (opts.remap  \
    or opts.action not in local_actions) and not (opts.action == "delete" and opts.clean):
        raise StorageError(5, "Destination not ready to receive commands.")
    if not dest.dir_exists: # and opts.action not in write_actions:
        raise StorageError(6, f"Path '{dest.path}' not found.")
    if not dest.writable and opts.action in write_actions \
    and not (opts.action == "delete" and opts.clean):
        raise StorageError(5, "Destination not writable.")
    if not dest.umatch and opts.action in set(write_actions) - {"arch-init"}:
        raise StorageError(5, "File ownership mis-match on destination.")
    if dest.archive_ini_hash == "none" and opts.action not in local_actions+("arch-init",):
        raise StorageError(6, f"Archive not found at '{dest.spec}'")
    if opts.force_allow_rollback:
        err_out("Caution: Allowing rollback.")

    if opts.action in ("arch-init",):
        if dest.archive_ini_hash != "none":
            raise StorageError(5, "Archive already exists: "+dest.spec)
        return arch_init(ArchiveSet(tmpdir, dest, opts))

    aset = get_configs_remote(dest, cachedir, opts)    ; os.utime(aset.path)

    if aset.updated_at is None:
        raise StorageError(6, "Archive not found.")
    aset.apdays = opts.apdays

    return aset


# Fetch copy of archive metadata from remote/dest

def get_configs_remote(dest, base_dir, opts, reload=False):

    show = debug and opts.action.lower() == "list"

    confname  = ArchiveSet.confname
    recv_list = [(confname,  ArchiveSet.max_conf_sz)]
    fetch_file_blobs(recv_list, tmpdir, dest, skip0=True, skip_exists=not reload)
    recv_list = [(ArchiveSet.saltname, DataCryptography.max_keyfile_sz),
                 ("salt.bak", DataCryptography.max_keyfile_sz)]
    fetch_file_blobs(recv_list, tmpdir, dest, skip0=True, skip_exists=True)

    # Instantiate ArchiveSet to authenticate archive.ini
    aset = ArchiveSet(tmpdir, dest, opts, children=0, pass_agent=int(opts.authmin), show=debug)

    if aset.format_ver == 2:
        if opts.action == "arch-check" and opts.upgrade_format:
            os.mkdir(v2dir := tmpdir+"/:oldv2:")
            os.replace(aset.path+"/"+confname, v2dir+"/"+confname)
            arch_check(None, OldArchiveSet_V2(v2dir, dest), upgrade=True); x_it(0)
        else:
            x_it(1, "Error:  Old V2 format. "
                    "To upgrade this archive run 'wyng arch-check --upgrade-format'.")
    else:
        assert aset.updated_at is not None

    arch_dir = base_dir + aset.subdir    ; ini = pjoin(arch_dir, confname)
    if debug:   print("\nArch dir:", arch_dir)

    if exists(ini) and not hmac.compare_digest(aset.raw_hashval,
                                        hashlib.sha256(open(ini,"rb").read()).hexdigest()):
        try:
            cache_aset = ArchiveSet(arch_dir, dest, opts, children=0, prior_auth=aset)
        except Exception as e:
            err_out(repr(e)+f"\nCached {confname} didn't load.")
            shutil.rmtree(arch_dir)
        else:
            if debug:   err_out(repr(cache_aset.header) +"\n"+repr(aset.header))
            if cache_aset.header != aset.header:
                raise ValueError("Cache and remote archive headers differ.")

            elif cache_aset.mcrypto and not aset.mcrypto:
                raise ValueError("Encrypted cache, not present on remote!")

            elif cache_aset.updated_at > aset.updated_at:
                # Cache ts is newer, check if contents are different and handle rollback
                if any((x != y for s in aset.conf
                 for x, y in zipln(dict(cache_aset.conf[s]).items(), dict(aset.conf[s]).items())
                 if x[0] not in ("mci_count","dataci_count","updated_at"))):

                    reason = (f"Cached metadata is newer, from {cache_aset.path}\n"
                              f"{cache_aset.updated_at} vs. {aset.updated_at}")
                    if opts.force_allow_rollback:
                        err_out(reason)
                        os.remove(ini)
                    else:
                        raise ValueError(reason)

            elif cache_aset.updated_at < aset.updated_at:
                if not reload:
                    print("Updating metadata cache.")
                    os.remove(ini)
                else:
                    raise ValueError("Cache reload failed.")

            elif cache_aset.updated_at == aset.updated_at and cache_aset.in_process:
                pass # un-initialized in_process

            else:
                if debug:
                    aset.conf.write(sys.stderr)    ; cache_aset.conf.write(sys.stderr)
                raise ValueError("Unhandled cache error.",
                                 f"{aset.updated_at} {aset.in_process[:1]}",
                                 f"{cache_aset.updated_at} {cache_aset.in_process[:1]}")

    if not exists(ini) and exists(aset.confpath):
        os.makedirs(arch_dir, exist_ok=True)
        for f in ("salt.bak", ArchiveSet.saltname, confname):
            if exists(f1 := pjoin(aset.path, f)) and not exists(f2 := pjoin(arch_dir, f)):
                shutil.copyfile(f1, f2)

    # Initial auth successful! Fetch + auth volume metadata...
    if opts.action in ("arch-check","delete"):
        depth = 0
    elif opts.action in ("add",):
        depth = 1
    else:
        depth = 2

    return ArchiveSet(arch_dir, dest, opts, children=depth, allvols=True,
                      prior_auth=aset, show=show, reload=reload)


# Returns either a single LocalStorage obj from --local, or a dict of them
# with associated list of vol name, alias pairs defined in --vols-from json file:
# s = {"vg1/pool1": [("vol1",""), ("vol-xyz","xyz-alias")], ...}

# alias is used for receiving to a local vol name different from the archive vol name

def get_configs_storage(aset, opts, require_local=True):
    storage, storagesets, vol_set = None, {}, set()
    fromlist    = opts.vols_from    ; local_url = opts.local
    arch_vols   = {x.name: x.vid for x in aset.vols.values()}

    if fromlist and exists(fromlist):
        # Init a storage obj for each local source and subset of vols
        try:
            locallists = json.load(open(fromlist, "r"))
        except json.decoder.JSONDecodeError as e:
            raise SelectionError("Empty/malformed json file")

        for url, vols in locallists.items():
            if not vols:   err_out("Empty vol list for "+url)   ; continue

            for vname, alias in vols:
                if debug:   print("alias", vname, "=", alias)
                if alias and vname in aset.vols:
                    aset.vols[vname].alias = alias    ; aset.vols[vname].aliastype = "rename"
                # Test for volname uniqueness
                if vname in vol_set:   raise SelectionError(f"Duplicate volume name: {vname}")
                vol_set.add(vname)

            storagesets[LocalStorage(url, opts, require_online=require_local,
                                     arch_vols=arch_vols, auuid=aset.uuid)] \
                        = [x[0] for x in vols]

        if not (storage or storagesets) and opts.import_other_from:
            storage = LocalStorage(None, opts, require_online=False,
                                   arch_vols=arch_vols, auuid=aset.uuid)
            storagesets[storage] = []

    else:
        storage = LocalStorage(local_url or None, opts, require_online=require_local,
                                arch_vols=arch_vols, auuid=aset.uuid)
        storagesets[storage] = []

    return storage, storagesets


class Destination:

    url_types   = ("ssh", "file", "qubes-ssh", "qubes")

    ssh_opts    = ["-x", "-T", "-e", "none", "-o", "ControlPath=~/.ssh/ctrl-%C",
                   "-o", "ControlMaster=auto", "-o", "ControlPersist=300",
                   "-o", "ServerAliveInterval=120",
                   "-o", "ConnectTimeout=60", "-o", "Compression=no"]

    qubes_opts  = ["--no-color-stderr", "--no-color-output", '--nogui', '--no-filter-escape-chars']

    tmpprefix   = "/tmp/wyngrpc/"    ; magic  = b"\xff\x11\x15"

    def __init__(self, dest_url, user_ssh_opts=[], user_qubes_opts=[]):
        os.makedirs(tmpdir+"/rpc", exist_ok=True)
        Destination.write_helper_program(tmpdir+"/rpc")

        # parse and validate dest spec
        self.ssh_opts   = user_ssh_opts or self.ssh_opts
        self.qubes_opts = user_qubes_opts or self.qubes_opts
        if not dest_url:                raise OptionError("Missing dest specification.")
        if not dest_url.isprintable():  raise InputError("[^control] char not allowed in dest.")
        dparts      = urlparse(dest_url)    ; self.dtype = dtype = dparts.scheme
        if dtype not in self.url_types:
            raise OptionError("'%s' not an accepted dest type." % dtype)
        if (dtype == "file" and (dparts.netloc or not dparts.path)) \
        or (dtype in ("ssh","qubes","qubes-ssh") and not dparts.netloc) \
        or (dtype == "qubes-ssh" and not all(dparts.netloc.partition(":"))) :
            raise OptionError("Malformed --dest specification.")

        self.spec   = dest_url                        ; self.sys = dparts.netloc
        self.path   = os.path.normpath(dparts.path)   ; self.archive_ini_hash = "none"
        self.free   = self.dtmp     =  None           ; self.last_chunkdir    = "\x00\x00"
        self.online = self.writable = self.hlinks = self.dir_exists = self.umatch = False
        self.agent_name = "wyng-agent-" + str(os.getuid()) \
                        + ArchiveSet.get_combo_id(os.getuid(), "", self.spec)

        self.run_map = {"file":       [CP.sh],
                        "ssh":        [CP.ssh] + self.ssh_opts + ["ssh://"+self.sys],
                        "qubes":      [CP.qvm_run] + self.qubes_opts
                                      + ["-p", self.sys],
                        "qubes-ssh":  [CP.qvm_run] + self.qubes_opts
                                      + ["-p", self.sys.split(":")[0]]
                        }


    def remove_dtmp(self):
        if self.online and self.dtmp:
            self.run([f"rm -rf {self.dtmp}"], timeout=10, check=False)
            if self.dtype == "qubes-ssh":
                self.run([f"rm -rf {self.tmpprefix}tmp*"], direct=True, check=False)

    def get_free(self, fpath):
        for ln in open(fpath,"r"):
            if ln.startswith("wyng_check_free"):  self.free = int(ln.split()[1])
        return self.free

    # Run system commands on destination
    def run(self, commands, direct=False, timeout=None, destcd=None, lc="C",
            infile="", inlines=None, out="", check=True, trap=False):

        if direct:
            cmd = self.run_map[self.dtype] + commands
        else:
            cmd = self.run_args(commands, trap=trap, destcd=destcd, lc=lc)

        if debug:   print(cmd)
        return do_exec([cmd], infile=infile, inlines=inlines, out=out,
                       check=check, timeout=timeout)

    # Build command lists that can be shunted to remote systems.
    # The input commands are stored in a temp file and a standard command that
    # runs the temp file is returned.

    def run_args(self, commands, trap=False, dest_type=None, destcd=None, lc="C"):

        dest_type = dest_type or self.dtype    ; tmpprefix = self.tmpprefix
        trapcmd   = "trap '' INT TERM QUIT ABRT ALRM TSTP USR1\n" if trap else ""
        # shunt commands to local tmp file
        with tempfile.NamedTemporaryFile(dir=tmpdir+"/rpc", delete=False) as tmpf:
            cmd = trapcmd + shell_prefix \
                + (f"cd '{destcd}'\n" if destcd else "") \
                + (f"export LC_ALL={lc}\n" if lc else "") \
                + " ".join(commands) + "\n"
            tmpf.write(bytes(cmd, encoding="UTF-8"))
            remotetmp = os.path.basename(tmpf.name)

        if dest_type in {"qubes","qubes-ssh"}:
            do_exec([[CP.qvm_run, "--no-color-stderr", "--no-color-output",
                    "--no-filter-escape-chars", "-p",
                      (self.sys if dest_type == "qubes" else self.sys.split(":")[0]),
                      (CP.mkdir+" -p "+tmpprefix+"; " if not self.dtmp else "")
                      + CP.cat+" >"+tmpprefix+remotetmp
                    ]], infile=pjoin(tmpdir,"rpc",remotetmp))
            if dest_type == "qubes":
                add_cmd = [CP.sh+" "+tmpprefix+remotetmp]
            else:
                add_cmd = [CP.ssh+" "+" ".join(self.ssh_opts)
                        +" ssh://"+self.sys.split(":", maxsplit=1)[1]
                        +' "$('+CP.cat+' '+tmpprefix+remotetmp+')"']

        elif dest_type == "ssh":
            #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
            add_cmd = [cmd]

        elif dest_type == "file":
            add_cmd = [pjoin(tmpdir,"rpc",remotetmp)]

        return self.run_map[dest_type] + add_cmd


    def detect_state(self, dedup, ro=True):

        if debug:   print("\nDest detect:")

        if self.dtype == "qubes-ssh":
            # fix: possibly remove dargs and use dest.run()
            dargs = self.run_map["qubes"][:-1] + [self.sys.split(":")[0]]

            cmd = dargs + [shell_prefix + "mkdir -p /tmp/wyngrpc"]
            do_exec([cmd])

        tmpprefix = self.tmpprefix    ; tmpdigits = 12   ; NL = "\n"   ; error = None ; run = False
        cmd  = [r"printf '\nwyng_run\n'" + (rf" && mkdir -p '{self.path}'" if not ro else "")
                +rf" && cd '{self.path}' && printf '\nwyng_dir_exists\n'"
                +(r" && exec 2>&1 && python3 --version && uname -a" if debug else "")

                # send helper program to remote dest
                +NL+ r"mkdir -p -m 777 " + tmpprefix
                +r"  && tdir=$(mktemp -d " + tmpprefix +"tmp"+ ("X"*tmpdigits) + r")"
                +r"  && echo && echo $tdir && cat >$tdir/dest_helper.py"

                # check free space and archive.ini status on remote
                +NL+ r"set +e && printf '\nwyng_online\n'"
                +NL+ r"printf '\nwyng_check_free ' && df -PT . | tail -1"
                +NL+ r"printf '\nwyng_archive_ini '"
                +r"  && { if [ -e archive.ini ]; then"
                +NL+r"    sha256sum archive.ini"
                +NL+r"    [ ! -O archive.ini ] || printf '\nwyng_owner_match\n'"
                +NL+r"else echo none; fi }"

                # test write access and hardlinks
                +(NL+ r"touch archive.dat && printf '\nwyng_writable\n'"
                + NL+ r"ln -f archive.dat .hardlink && printf '\nwyng_hlink\n'" if not ro else "")
                ]
        try:
            do_exec([self.run_args(cmd),
                    # sanitize remote output:
                    [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]],
                    pipefail=True,
                    out=tmpdir+"/dest-state.log", infile=tmpdir+"/rpc/dest_helper.py")
        except SPr.CalledProcessError as e:
            for log in ("/dest-state.log", "/err.log"):
                if exists(tmpdir+log):
                    do_exec([ [CP.cat, "-v", tmpdir+log],  [CP.tail, "--bytes=1000"],
                            [CP.grep, "-vFx", " --+--"] ],
                            out=tmpdir+log+"-out", check=False)
                    err_out(open(tmpdir+log+"-out", "r").read())
            if debug:   print(cmd); err_out(repr(e))

        for ln in open(tmpdir+"/dest-state.log","r"):
            if debug:   print(ln, end="")
            if ln.startswith("wyng_run"):
                run = True
            elif ln.startswith("wyng_online"):
                self.online = True
            elif ln.startswith("wyng_dir_exists"):
                self.dir_exists = True
            elif ln.startswith("wyng_archive_ini") and len(parts := ln.split()) > 1:
                self.archive_ini_hash = parts[1]
            elif ln.startswith("wyng_owner_match"):
                self.umatch = True
            elif ln.startswith("wyng_check_free"):
                if len(parts := ln.split()) > 5 and is_num(parts[5]):
                    self.free = int(parts[5]) * 1024
            elif ln.startswith("wyng_writable"):
                self.writable = True
            elif ln.startswith("wyng_hlink"):
                self.hlinks   = True
            elif ln.startswith(tmpprefix):
                self.dtmp = ln.strip()

        if not run:   err_out("Dest not accessible.")
        if self.online and (not self.dtmp or len(self.dtmp) != len(tmpprefix)+tmpdigits+3 \
        or not set(self.dtmp[5:]) <= set(string.ascii_letters + string.digits + "/")):
            err_out("Malformed or missing tmp path on dest.")
            self.dtmp, self.online = None, False


    def write_helper_program(path):

        dest_program = r'''#  Copyright Christopher Laprise 2018-2025
#  Licensed under GNU General Public License v3. See wyng-backup/LICENSE file.
import os, sys, time, signal, shutil, subprocess as SPr, gzip, tarfile, json
sighandlers = {}   ; sys_signals = ["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1"]

def catch_signals(sel=sys_signals, iflag=False):
    for sig in list(sighandlers):   signal.signal(sig, sighandlers.pop(sig))
    for sig in (getattr(signal,"SIG"+x) for x in (sel or [])):
        sighandlers[sig] = signal.getsignal(sig)   ; signal.signal(sig, handle_signal)
        signal.siginterrupt(sig, iflag)

def handle_signal(sig, frame):
    if sig == signal.SIGALRM:   raise IOError("Timeout ALRM")

def err_out(t):   sys.stderr.write(t+"\n"); sys.stderr.flush()

def fssync(path, force=False):
    if msync or force:   misc_procs.append(SPr.Popen(["sync","-f",path]))

def x_it(ec, text=None):
    if text:   err_out(text)
    sys.exit(ec)

def helper_send(debug=False):
    def altextract(m, set_attrs, filter):  return tarf.extract(m, set_attrs=set_attrs)
    mkdirs = os.makedirs    ; hlink = os.link    ; dirname = os.path.dirname

    while True:
        try:
            tarf = tarfile.open(mode="r|", fileobj=sys.stdin.buffer)
        except tarfile.ReadError as e:
            if debug:   err_out(repr(e))
            break
        extract = tarf.extract \
                  if (pyv := sys.version_info)[:2] > (3,11) or (pyv[:2] == (3,11) and pyv[2] > 3) \
                  or (pyv[:2] == (3,10) and pyv[2] > 11) or (pyv[:2] == (3,9) and pyv[2] > 16) \
                  or (pyv[:2] == (3,8) and pyv[2] > 16) else altextract
        if debug:   err_out("Got tarfile")
        substitutions = {}    ; pdir = ""
        for member in tarf:
            if debug:   err_out(f"{member.type} {member.name} {member.size}")
            if member.isfile():
                extract(member, set_attrs=False, filter="data")
            elif member.islnk():
                source = src_orig = member.linkname   ; dest = member.name   ; ddir = dirname(dest)
                if debug:   err_out(f"ln: {source}")
                if source in substitutions:   source = substitutions[source]
                if ddir != pdir:   mkdirs(ddir, exist_ok=True)   ; pdir = ddir
                try:
                    hlink(source, dest)
                except OSError as err:
                    if err.errno != 31:   raise err
                    if debug:   err_out(repr(err))
                    err_out(f"sub: {source} {dest}")
                    shutil.copyfile(source, dest)    ; substitutions[src_orig] = dest
            elif member.ischr() and member.name == "mv":
                catch_signals()    ; files = json.load(tarf.extractfile(tarf.next()))
                if debug:   err_out("mv: " + repr(files))
                for f in (x[:-4] for x in files):   os.utime(f) if exists(f) else None
                for f in files:   replace(f, f[:-4])
                catch_signals(None)
            elif member.isdir():
                if debug:   err_out(f"dir: {os.path.abspath(os.curdir)} -- {member.name}")
                vdir = member.name.split("/")[:2]    ; open(tmpdir+"/dir_"+vdir[0], "w").close()
                for i in os.scandir(vdir[0]) if vdir[0].startswith("Vol_") and len(vdir) > 1 \
                                             and str(time.time_ns())[-1] == "5" else []:
                    if i.path.endswith("-tmp") and not i.path.endswith(vdir[1]+"-tmp"):
                        err_out("rmtree: " + i.path)   ; shutil.rmtree(i.path, ignore_errors=True)
            tarf.members = []
        tarf.close()   ; fssync(".", force=True)

def helper_receive(lstf):
    stdout_write = sys.stdout.buffer.write   ; exists = os.path.exists   ; getsize= os.path.getsize
    stdout_flush = sys.stdout.buffer.flush   ; magicm = magic            ; fname, fsize= "(none)",0
    for line in lstf:
        fname = line.strip()
        if not fname:   break
        if not exists(fname):
            err_out(fname+" does not exist!")   ; fsize = 0
        else:
            fsize = getsize(fname)
        stdout_write(magicm + fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:   fdata = dataf.read(fsize)
            if len(fdata) != fsize:   x_it(20, f"Size mismatch: {len(fdata)} vs {fsize}")
            stdout_write(fdata); stdout_flush()
    stdout_write(magicm); stdout_write(b'\xFF'*128); stdout_flush(); time.sleep(0.2)
    sys.stdout.close()
    sys.stderr.write(f"Last item: {fname} {fsize}\n")

def helper_merge():
    exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
    try:
        if resume:
            if exists("merge-init") or not exists("merge"):
                raise RuntimeError("Merge: Init could not complete; Aborting merge.")
        else:
            print("Merge: Initialization.")
            for f in (target+"/info", "volinfo", "../archive.ini"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")    ; fssync(".")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):   replace(i.path, i.name)
        elif not exists("merge"):
            for f in (target+"/info.tmp", target+"/manifest.z.tmp", "volinfo.tmp", "merge.lst.gz"):
                if exists(f):   os.remove(f)
        fssync(".")    ; print(err)    ; sys.exit(50)
    try:
        os.chdir("merge")  #  CD
        if not resume or not exists("CHECK-mv-rm"):
            print("Merge: remove/replace files.")    ; subdirs = set()
            for src in src_list:  # Enh: replace os.scandir w manifest method
                for i in os.scandir(src):
                    if i.is_dir():   subdirs.add(i.name)
            for sdir in subdirs:   os.makedirs(merge_target+"/"+sdir, exist_ok=True)
            for line in lstf:
                ln = line.split() # default split() does strip()
                if ln[0] == "rename" and (not resume or exists(ln[1])):
                    replace(ln[1], ln[2])
                elif ln[0] == "-rm" and exists(ln[1]):
                    remove(ln[1])
            open("CHECK-mv-rm","w").close()
        os.chdir("..")     #  CD
    except Exception as err:
        print(err)    ; sys.exit(60)

def helper_merge_finalize():
    try:
        print("Merge: Finalize target")
        m = "merge/"    ; open(m+"CHECK-start-finalize","w").close()
        for f in (m+target+"/info.tmp", m+merge_target+"/info", m+target+"/manifest.z.tmp",
                  m+merge_target+"/manifest.z", m+merge_target, target, "volinfo.tmp", "volinfo",
                  "../archive.ini.tmp", "../archive.ini", "..", "."):
            if exists(f):   os.utime(f) # touch first
        if not resume or exists(m+target+"/info.tmp"):
            replace(m+target+"/info.tmp", m+merge_target+"/info")
        if exists(m+target+"/manifest.z.tmp"):
            replace(m+target+"/manifest.z.tmp", m+merge_target+"/manifest.z")
        if not resume or not exists(target):         replace(m+merge_target, target)
        if not resume or exists("volinfo.tmp"):      replace("volinfo.tmp", "volinfo")
        if not resume or exists("../archive.ini.tmp"): replace("../archive.ini.tmp", "../archive.ini")
        if not exists(target):   raise FileNotFoundError(target)
        fssync(".")
    except Exception as err:
        x_it(70, repr(err))
    shutil.rmtree("merge", ignore_errors=True)    ; remove("merge.lst.gz")
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

## MAIN ##
cmd = sys.argv[1]   ; debug = "--debug" in sys.argv   ; msync = "--sync" in sys.argv
tmpdir = os.path.dirname(os.path.abspath(sys.argv[0]))  ; misc_procs = []; magic  = b"\xff\x11\x15"
exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
err_out(f"{os.uname()}\nUser {os.getuid()}, Groups: {os.getgroups()}"
        f"\n{sys.implementation}\nArgs:\n{sys.argv}\n")

if cmd in ("receive","dedup") and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
else:
    lstf = None

if cmd == "merge":
    src_list = []    ; resume = "--resume" in sys.argv    ; mtpath = sys.argv[2]
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    if resume and not (exists("merge.lst.gz") and exists("merge")):
        print("Error: Remote dir not initialized.")
        sys.exit(50 if exists(mtpath) else 40)
    lstf = gzip.open("merge.lst.gz", "rt")
    merge_target, target = lstf.readline().split()
    while True:
        ln = lstf.readline().strip()
        if ln == "###":  break
        src_list.append(ln)
    if "--finalize" not in sys.argv:
        helper_merge()
    catch_signals()
    helper_merge_finalize()
elif cmd == "receive":
    helper_receive(lstf if lstf else sys.stdin)
elif cmd == "send":
    helper_send(debug=debug)
elif cmd == "dedup":
    ddcount = 0    ; substitutions = {}
    for line in lstf:
        source, dest = line.split()   ; src_orig = source    ; deststat = os.stat(dest)
        ddcount += deststat.st_size
        if source in substitutions:   source = substitutions[source]
        if os.stat(source).st_ino != deststat.st_ino:
            try:
                os.link(source, dest+"-lnk")    ; replace(dest+"-lnk", dest)
            except OSError as err:
                try:
                    remove(dest+"-lnk")
                except:
                    pass
                if err.errno == 31:
                    # source has too many links; substitute
                    substitutions[src_orig] = dest   ; ddcount -= deststat.st_size   ; continue
                else:
                    print(err)   ; raise err
    print(ddcount, "bytes reduced.")
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

sys.stderr.write("--==--\n")
'''
        with open(path+"/dest_helper.py", "wb") as progf:
            progf.write(bytes(dest_program, encoding="UTF-8"))

    #####>  End dest_helper program  <#####


# Run system commands with pipes, without shell:
#
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'. 'inlines' can be a list-like collection of strings
# to be used as input instead of 'infile'.
# List of commands may include 'None' instead of a child list; these will be ignored.
#
# 'getctl' returns a tuple(control_execs <generator>, close <func>, (inf, outf, errf))
# without waiting. The generator allows communicate() and checks to occur in tandem with other
# commands. The context may be fed into another do_exec invocation via 'otherctl', which will
# automatically process the separate pipe chains in tandem. This allows separate but related chains
# to feed outputs to an aggregating process using extra file descriptors or named pipes.

def do_exec(commands, cwd=None, check=True, out="", infile="", inlines=[], text=False,
            pgroup=None, pipefail=False, timeout=None, getctl=False, otherctl=None):
    ftype   = "t" if text else "b"
    if issubclass(type(out), io.IOBase):
        outf = out
    else:
        outmode = "a" if out.startswith(">>") else "w"    ; out = out.lstrip(">>")
        if cwd and out and out[0] != "/":   out = pjoin(cwd,out)
        outfunc = gzip.open if out.endswith(".gz") else open
        outf    = outfunc(out, outmode+ftype) if out else SPr.DEVNULL

    if inlines:
        inf = SPr.PIPE  #io.StringIO("\n".join(inlines)+"\n")
    else:
        if cwd and infile and infile[0] != "/":   infile = pjoin(cwd,infile)
        infunc  = gzip.open if infile.endswith(".gz") else open
        inf     = infunc(infile, "r"+ftype) if infile else SPr.DEVNULL

    errf = open(tmpdir+"/err.log", "a")  ; print(" --+--", file=errf)   ; err = None
    if debug:   print(commands, file=errf, flush=True)

    # Start each command, linking them via pipes
    commands = list(filter(None, commands))    ; procs = []    ; start_t = time.monotonic()
    for i, clist in enumerate(commands):
        p = SPr.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else SPr.PIPE,
                             stderr=errf, process_group=pgroup)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)    ; pgroup = os.getpgid(p.pid)

    try:
        if inlines:   procs[0].communicate(("\n".join(inlines)+"\n").encode("UTF-8"),
                                            timeout=timeout)
    except SPr.TimeoutExpired:
        pass

    # Monitor and control processes
    def _iterate_execs():
        while True:
            err = None    ; finish = to_flag = False
            for p1 in reversed(procs):
                retcode = p1.poll()
                if not finish and retcode is None:
                    try:
                        p1.communicate(timeout=2)
                    except SPr.TimeoutExpired:
                        to_flag = True
                        continue
                    retcode = p1.returncode   ; finish = True
                    if check and (retcode != 0):
                        err = p1              ; finish = True
                elif finish and retcode is None:
                    p1.terminate()
                    continue

            if err or not to_flag or (timeout and time.monotonic() - start_t > timeout):
                break
            else:
                yield err

    # Closure: Advance our exec iterator, possibly alongside an otherctl iterator
    def control_execs():
        for ii in zipln(_iterate_execs(), otherctl[0]() if otherctl else []):
            yield ii

    # Closure: Recursively complete each exec pipe chain, first other then this one.
    # Check result codes and raise error or combine with other results as needed.
    def close():
        other_rcs = otherctl[1]() if otherctl else [] # close other
        rclist = [0] + list(filter(bool if pipefail else str, ( x.returncode for x in procs )))

        if check and any(rclist): # Enh: test pipefail and/or last rc here
            print(repr(rclist), file=errf, flush=True)
            raise SPr.CalledProcessError([x for x in rclist if x][0], commands)

        for f in [inf, outf, errf]:
            if type(f) is not int: f.close()

        return other_rcs + rclist

    if getctl:
        return control_execs, close, (inf, outf, errf)

    for ii in control_execs() if otherctl else _iterate_execs():   pass
    return close()[-1]


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Session objects.
# Returns False if local and dest hashes match.

def compare_files(arch, pathlist=[], volumes=[], sessions=[], clear=True, manifest=False):
    dest = arch.dest    ; cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions) and not x.meta_checked]
    realses   = [x for x in sessions if not x.meta_checked and not x.just_fetched]
    if len(volumes)+len(sessions)+len(pathlist) == 0:   return False

    with open(cmp_list, "a") as flist:
        for pth in pathlist:
            #if not exists(pth):   raise FileNotFoundError(pth)
            print(pth, file=flist)
        for v in realvols:
            v.meta_checked = True    ; print(v.vid+"/volinfo", file=flist)
        for s in realses:
            s.meta_checked = True
            for sf in ["info"] + (["manifest.z"] if manifest else []):
                print(pjoin(s.volume.vid,s.name,sf), file=flist)

    do_exec([[CP.xargs, CP.sha256sum]], cwd=arch.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest.run(["xargs sha256sum"],
             destcd=dest.path, infile=cmp_list, out=tmpdir+"/compare-hashes.dest")

    files  = [tmpdir+"/compare-hashes.local", tmpdir+"/compare-hashes.dest"]
    result = do_exec([[CP.cmp] + files], check=False) > 0

    return result


## Enh: Create aset functions to supply meta filenames for objects

def update_dest(arch, pathlist=[], volumes=[], sessions=[], ext="", delete=False, sync=False):
    lcd = arch.path    ; dest = arch.dest

    update_list = [x.volume.vid+"/"+x.name+"/manifest.z" for x in sessions] \
                + [x.volume.vid+"/"+x.name+"/info" for x in sessions] \
                + [x.vid+"/volinfo" for x in volumes] + pathlist

    if delete:
        assert not ext;  dest.run(["xargs rm -f"], destcd=dest.path, inlines=update_list)
        return

    do_exec([[CP.tar,"-cf","-","--no-recursion","--verbatim-files-from","--files-from", "-"],
             dest.run_args(["tar -o -xf -"]
                            + [" && mv '"+x+ext+"' '"+x+"'" for x in update_list if ext],
                            destcd=dest.path)
            ], inlines=[x+ext for x in update_list],
            cwd=lcd)
    if sync:   dest.run(["sync"], check=False)


def fetch_file_blobs(recv_list, recv_dir, dest, ext="", skip0=False, skip_exists=False,
                     verifier=None):

    if debug:   print(f"Fetch requested to {recv_dir}:", recv_list)
    magic = dest.magic    ; exists = os.path.exists
    recv_list = [(x,y) for x,y in recv_list if not skip_exists or not exists(recv_dir+"/"+x)]
    if not recv_list:   return []

    cmd = dest.run_args(
            [ f"rm -f {dest.dtmp}/dest.lst.gz && python3 {dest.dtmp}/dest_helper.py receive"
            ], destcd=dest.path)
    recvp = SPr.Popen(cmd, stdout=SPr.PIPE, stdin=SPr.PIPE,
                      stderr=open(tmpdir+"/receive.log","w") if debug else SPr.DEVNULL)
    recvp.stdin.write(("".join((x+"\n" for x,y in recv_list))).encode("UTF-8"))
    recvp.stdin.flush()    ; rc = recvp.poll()    ; recvp.stdin.close()

    for fname, fsz in list(recv_list):
        if debug:   print("Fetch", fname)
        if rc is not None:   raise RuntimeError("Process terminated early. rc="+repr(rc))
        fpath = recv_dir+"/"+fname+ext
        if exists(fpath):   os.remove(fpath)
        if recvp.stdout.read(3) != magic:   raise ValueError("Bad magic.")
        # Read chunk size
        if len(sizeb := recvp.stdout.read(4)) != 4:   raise ValueError("Bad size field.")
        untrusted_size = int.from_bytes(sizeb,"big")
        if untrusted_size == 0 and skip0:   continue
        # Check for reasonable size to avoid overflow:
        if not fsz + 2000 >= untrusted_size > 0:
            raise BufferError(f"Bad file size {untrusted_size}, expected 1-{fsz + 2000} max.\n"
                              f"{fpath} - {skip0}")

        # Size is OK.
        size = untrusted_size
        # Read chunk buffer
        untrusted_buf = recvp.stdout.read(size)    ; rc  = recvp.poll()
        if rc is not None:   raise RuntimeError("Process terminated early. rc="+repr(rc))
        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))
        if verifier:   verifier.auth(untrusted_buf)

        os.makedirs(os.path.dirname(fpath), exist_ok=True)
        with open(fpath, "wb") as outf:   outf.write(untrusted_buf)

    rc  = recvp.poll()
    if recvp.stdout.read(3) != magic:   raise ValueError("Bad magic.")
    recvp.kill()
    return recv_list


# Prepare snapshots and check consistency with metadata:
# Normal use will have a snap1 snapshot of the volume already in place.  Here we
# create a fresh snap2 so 'update_delta_digest' can compare it to the older snap1
# and then rotate snap2 -> snap1.

def prepare_snapshots_lvm(storage, aset, datavols, monitor_only, other_vols={}):
    incr_vols, complete_vols, unchanged_vols = [], [], []   ; lvols = storage.lvols
    opts = storage.opts

    for datavol in datavols + list(other_vols.keys()):

        if not exists(vpath := other_vols.get(datavol) or pjoin(storage.path, datavol)):
            err_out("  Warning: Local '%s' does not exist!" % vpath)
            if datavol in other_vols:   del(other_vols[datavol])
            continue

        if storage.getsize(vpath) > max_address + 1:
            err_out("  Skipping '%s', size is larger than maximum (%d)" % (vpath, max_address+1))
            if datavol in other_vols:   del(other_vols[datavol])
            continue

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, opts.voldesc,
                       storage=None if datavol in other_vols else storage)

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        l_vol = lvols[datavol]   ; snap1, snap2 = l_vol.snap1, l_vol.snap2
        vol   = aset.vols[datavol]    ; mapfile = vol.mapfile()
        if vol.aliastype == "import_other":   continue

        if datavol in other_vols:
            if not ArchiveVolume.mdvol_chk.match(datavol):
                print("  Queuing full scan of import '%s'" % datavol)
            vol.alias, vol.aliastype = other_vols[datavol], "import_other"
            continue

        elif lvols[snap1].is_arch_member() == "false":
            if opts.remap:
                lvols[snap1].delete()
            else:
                print("  Skipping %s; snapshot is from a different archive. Use --remap to clear it."
                      % datavol)   ; error_cache.append(datavol)
                continue

        if len(vol.sessions) and not lvols[snap1].exists() and lvols[snap2].exists() \
        and vol.last in lvols[snap2].tags and "delta" not in lvols[snap2].tags \
        and exists(mapfile) and vol.map_used() == 0 \
        and vol.sessions[vol.last].gettime() == lvols[snap2].gettime():
            l_vol.rotate_snapshots(rotate=True, timestamp_path=mapfile)
            print("  Recovered interrupted snapshot rotation:", datavol)

        # Make fresh session snapshot snap2
        lvols[snap2].delete()
        tagopts = ["wyng", "arch-"+aset.uuid] + (["delta"] if monitor_only else [])
        lvols[snap2].create(snapshotfrom=datavol, ro=True, addtags=tagopts)

        # Make deltamap if necessary. Try to recover paired state
        # by comparing timestamps; a match means remap is unnecessary.
        if len(vol.sessions):
            lastses = vol.sessions[vol.last]    ; s1tags = lvols[snap1].tags

            if not exists(mapfile) and lvols[snap1].exists() \
            and vol.last in s1tags and "delta" not in lvols[snap1].tags:
                # Latest session matches current snapshot; OK to make blank map.
                vol.init_deltamap(timestamp=lvols[snap1].gettime())

            if lvols[snap1].exists() and (not exists(mapfile)
            or (os.stat(mapfile).st_mtime_ns != lvols[snap1].gettime()
                and lastses.gettime() != lvols[snap1].gettime())):
                # Handle inadvertant mapfile snapshot mis-match
                lvols[snap1].delete()

        elif monitor_only:
            print("  Skipping '%s'; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            if datavol not in unchanged_vols:   incr_vols.append(datavol)
        elif not monitor_only:
            print("  Queuing full scan of '%s'" % datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping '%s'; No paired snapshot." % datavol)
            continue


    assert not unchanged_vols
    return incr_vols, complete_vols, unchanged_vols, other_vols


def prepare_snapshots_reflink(storage, aset, datavols, monitor_only, other_vols={}):
    incr_vols, complete_vols, unchanged_vols = [], [], []   ; lvols = storage.lvols
    opts = storage.opts    ; atrs = {}

    for dv in datavols[:] + list(other_vols.keys()):
        if not exists(vp := other_vols.get(dv) or pjoin(storage.path, dv)):
            err_out("  Warning: Local '%s' does not exist!" % vp)
            if dv in other_vols:   other_vols.remove(dv)
            else:                  datavols.remove(dv)
            continue

        if storage.getsize(vp) > max_address + 1:
            err_out("  Skipping '%s', size is larger than maximum (%d)" % (vp, max_address+1))
            if dv in other_vols:   other_vols.remove(dv)
            else:                  datavols.remove(dv)
            continue

        if dv in datavols:   atrs[dv] = vp

    # Get attributes for vol paths
    do_exec([[CP.xargs, CP.lsattr]], inlines=list(atrs.values()), out=tmpdir+"/attr.lst")
    atrs = {x: y[0] for x,y in zip(atrs, map(str.split, open(tmpdir+"/attr.lst")))}

    for datavol in datavols + list(other_vols.keys()):

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, opts.voldesc,
                       storage=None if datavol in other_vols else storage)

        vol   = aset.vols[datavol]    ; mapfile = vol.mapfile()
        if vol.aliastype == "import_other":   continue
        if datavol in atrs and datavol in datavols and "C" in atrs[datavol]:
            if verbose:   print("  Cannot snapshot:")
            other_vols[datavol] = pjoin(storage.path, datavol)
        if datavol in other_vols:
            if not ArchiveVolume.mdvol_chk.match(datavol):
                print("  Queuing full scan of import '%s'" % datavol)
            vol.alias, vol.aliastype = other_vols[datavol], "import_other"
            continue

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        l_vol = lvols[datavol]   ; snap1, snap2 = l_vol.snap1, l_vol.snap2
        lvols[snap2].delete()

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing timestamps; a match means remap is unnecessary.
        if len(vol.sessions):
            lastses = vol.sessions[vol.last]

            if not lvols[snap1].exists() and l_vol.gettime() == lastses.gettime() \
            and lastses.volsize == l_vol.getsize() and not vol.map_used():
                # Create snap1 from datavol if its missing but source timestamp aligns
                lvols[snap1].create(snapshotfrom=datavol, ro=True)
                # Avoid race condition
                if lvols[snap1].gettime() != lastses.gettime() \
                or lvols[snap1].getsize() != lastses.volsize:
                    lvols[snap1].delete()

            if not exists(mapfile) and lvols[snap1].exists() \
            and lastses.gettime() == lvols[snap1].gettime()  \
            and lastses.volsize == lvols[snap1].getsize():
                # Create deltamap if its missing but datavol and lastses timestamps align
                vol.init_deltamap(timestamp=lvols[snap1].gettime())

            if lvols[snap1].exists() and (not exists(mapfile)
            or (os.stat(mapfile).st_mtime_ns != lvols[snap1].gettime()
                and lastses.gettime() != lvols[snap1].gettime())):
                # Handle inadvertant mapfile snapshot mis-match
                lvols[snap1].delete()

            if lvols[snap1].exists() and lvols[snap1].gettime() == l_vol.gettime() \
            and lvols[snap1].getsize() == l_vol.getsize() \
            and l_vol.getperms() == vol.sessions[vol.last].permissions and exists(mapfile):
                # No changes forthcoming.  This vol will not have a 'snap2' created for it
                # and won't be scanned for deltas or have snapshot rotation.
                unchanged_vols.append(datavol)

        elif monitor_only:
            print("  Skipping '%s'; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            if datavol not in unchanged_vols:   incr_vols.append(datavol)
        elif not monitor_only:
            print("  Queuing full scan of '%s'" % datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping '%s'; No paired snapshot." % datavol)
            error_cache.append(datavol)    ; continue

        # Make fresh snap2vol
        lvols[snap2].create(snapshotfrom=datavol, ro=True)

    return incr_vols, complete_vols, unchanged_vols, other_vols


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(storage, aset, datavols):

    #Enh: construct 'poolset' as vg,pool pairs; change vgname handling to track 'apool'
    vgname   = storage.vgname.replace("-","--")    ; lvols = storage.lvols
    poolset  = set(lvols[x].pool_lv for x in datavols)

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    catch_signals()
    for apool in sorted(poolset):
        poolname = apool.replace("-","--")
        try:
            storage.metadata_lock(apool)
            for datavol in datavols:
                lv = lvols[datavol]
                if lv.pool_lv != apool:   continue
                if debug:   print("Get tlvm deltas for", datavol, flush=True)

                cmds = [[CP.thin_delta, "--metadata-snap",
                                        "--thin1=" + lvols[lv.snap1].thin_id,
                                        "--thin2=" + lvols[lv.snap2].thin_id,
                                        "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", r"^\s*<same .*\/>$"],
                        [CP.gzip, "-2"]
                        ]
                do_exec(cmds,  out=tmpdir+"/delta."+datavol)
        except Exception as e:
            err_out("ERROR running thin_delta process.")
            raise e
        finally:
            storage.metadata_unlock(apool)

    catch_signals(**signormal)
    return datavols


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest_lvm(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = lvols[vol.name].snap1      ; snap2vol   = lvols[vol.name].snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    volperms    = vol.sessions[vol.last].permissions
    assert len(vol.sessions) and exists(vol.mapfile())
    assert storage.block_size == 512 and aset.chunksize % storage.block_size == 0

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(gzip.open(tmpdir+"/delta."+datavol,"r")).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    dnewchunks  = isnew  = anynew = dfreedblocks = 0

    # Check for volume perms & size changes;
    # Chunks from 'markall_pos' onward will be marked for backup.
    changed = snap2size != vol.volsize() or storage.lvols[datavol].getperms() != volperms
    next_chunk_addr  = vol.last_chunk_addr()[0] + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile(), "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // storage.block_size
        chunkblocks  = chunksize // storage.block_size
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                isnew = anynew = 1
                #if delta.tag == "right_only":   tally -= blocklen
            elif delta.tag == "left_only":
                isnew = 0    ; dfreedblocks += blockend - blockbegin
                #tally += blocklen
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm, dtree)
        if dnewchunks+dfreedblocks:   bmapf.flush()
        if not debug:   os.remove(tmpdir+"/delta."+datavol)

    catch_signals()
    changed = changed or dnewchunks+dfreedblocks+anynew > 0
    tags = [vol.last, "delta", "delta-"+str(lvols[snap2vol].gettime())]
    t2   = lvols[datavol].rotate_snapshots(rotate=changed and monitor_only, delsnap2=monitor_only,
                                           timestamp_path=vol.mapfile(), addtags=tags)
    if t2 != lvols[snap1vol].gettime():
        raise ValueError("Bad rotation timestamp.")

    if monitor_only:
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if changed else "\r  no change    ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    catch_signals(**signormal)
    return changed, snap2size, volperms


def get_reflink_deltas(storage, aset, datavols):

    # Get filesystem block size
    open(storage.path+"/.tmp.wyng","wb").close()
    res = SPr.check_output([CP.filefrag, "-v", storage.path+"/.tmp.wyng"], text=True)
    r1  = [x for x in res.splitlines() if x.startswith("File size") and x.endswith("bytes)")]
    storage.block_size = int(r1[0].split()[-2])    ; os.remove(storage.path+"/.tmp.wyng")
    if debug:   print("Block size:", storage.block_size)
    assert aset.chunksize and aset.chunksize % storage.block_size == 0

    generation, stpath = storage.metadata_lock()

    for vol in (aset.vols[x] for x in datavols[:]):
        lv = storage.lvols[vol.name]                ; cmds, devs = [], set()
        dpath = tmpdir+"/delta."+vol.vid

        # Create a pair of piped command sets for diffing metadata of the two snapshots:
        for side, snapname, outp in (("11", lv.snap1, ' > "'+dpath+'-p"'), ("22", lv.snap2, "")):
            if not exists(stpath+snapname):
                err_out("'%s' snapshot not found (different subvolume?); Skipping..." % vol.name)
                datavols.remove(vol.name);   break

            # filefrag output: file extent ranges, their 'physical' block ranges, extent length.
            # sort will merge this output from two vols.
            # uniq -u will filter-out exact extent/phys/len matches between the two vols.
            # The result will be a list of extent ranges that have changed.

            devs.add(os.stat(stpath+snapname).st_dev)
            cmds.append([
              [CP.filefrag, "-v", stpath+snapname],
              # BEGIN defines the expected filefrag header layout (PAT);
              # END exits with code 2 if no end line (extents found) was found or HEADER != PAT;
              # a non-matching line (00..wyng) is printed if 0 extents found.
              # Printed fields are arranged to complement 'uniq' as it can only skip fields and
              # then must compare the rest of the line.  gsub removes punctuation.
              # 'outp' is used to redirect output for only one side (other side goes to stdout).
              [CP.awk, r'BEGIN {PAT="^\\s*ext:\\s+logical_offset:\\s+physical_offset:\\s+length:"}'
                       r' NR<4 && $0 ~ PAT {HDR="Y"}'
                       r' /^\s*[0-9]+:/ {gsub(/\.|:/, " ", $0);'
                                        r' print '+side+', $2, $6, $3, $4, $5, $NF'+outp+r'}'
                       r' END {if ($0 ~ /\s0 extents found$/) {print "00 0 1 0 0 0 wyng"'+outp+r'}'
                              r' else if (!(HDR=="Y" && $0 ~ /extents? found$/)) exit 2}'
             ]]
            )

        if vol.name not in datavols:   continue
        assert len(devs) == 1
        if debug:   print("Get rlnk deltas for", vol.name, flush=True)

        # Save output from first volume to a named pipe. 'getctl' means run it async; below we
        # will add it to the sort/uniq pipeline via 'otherctl'. This prevents tmp files
        # being created, which would be very large due to high granularity of fiemap data.
        os.mkfifo(dpath+"-p")
        p11context = do_exec(cmds[0], getctl=True)
        #Enh: maybe use dev+inode as fname

        # Pipe output from second volume to 'sort' and include pipe from first
        retcode = do_exec(cmds[1] +
                             [ [CP.sort, "-s", "-k2n,2", dpath+"-p", "-"],
                               [CP.uniq, "-u", "-f1"],
                               [CP.gzip, "-2"]
                              ],
                                 out=dpath, otherctl=p11context
                            )
        os.remove(dpath+"-p")

    # Check here that Btrfs snapshot gen#s did not change.
    # If changed: retry, x_it(7), use remap mode or pause/cancel the Btrfs op.
    # This could be extended with a fallback mode that checks _get_btrfs_generation() directly
    # while doing one snap pair at a time to increase chances of success.
    if storage.metadata_unlock()[0] != generation:
        raise StorageError(7, "fs generation ids differ.")

    return datavols


def update_delta_digest_reflink(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols              ; l_vol      = lvols[datavol]
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = l_vol.snap1                ; snap2vol   = l_vol.snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    volperms    = vol.sessions[vol.last].permissions
    vol_ts      = vol.sessions[vol.last].gettime()
    assert len(vol.sessions) and exists(vol.mapfile())

    # Check for volume perms & size changes;
    # Chunks from 'markall_pos' onward will be marked for backup.
    changed = snap2size != vol.volsize() or l_vol.getperms() != volperms \
              or lvols[snap2vol].gettime() != vol_ts
    next_chunk_addr  = vol.last_chunk_addr()[0] + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None
    bsize = storage.block_size

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile(), "r+b") as bmapf, gzip.open(tmpdir+"/delta."+vol.vid, "r") as deltaf:
        snap_ceiling = max(snap1size, snap2size) // bsize
        chunkblocks  = chunksize // bsize
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)
        dnewblocks   = dfreedblocks = highwater = 0

        # Cycle through the merged 'filefrag' metadata, marking bits in bmap_mm as needed.
        # Entries carry a beginning block position and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for dln in deltaf:
            side, blockbegin, blocklen = map(int, dln.split()[:3])

            blockend     = min(blockbegin + blocklen, snap_ceiling)
            if blockend <= highwater:   continue
            highwater    = blockend

            if side == 22:
                dnewblocks += blocklen
            else:
                dfreedblocks += blockend - blockbegin

            # blockpos iterates over disk blocks.
            # block_size (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewblocks += (bmap_size - markall_pos) * 8 * (chunksize // bsize)

        #estimate = sum((x.count("1") for x in map(bin, filter(None, memoryview(bmap_mm))))) \
                 #* chunksize if vol.map_used() else 0 ####
        #print(f"Estimate: {estimate}, New {dnewblocks*bsize}, Freed {dfreedblocks*bsize}" ) ####
        del(bmap_mm)
        if dnewblocks+dfreedblocks:   bmapf.flush()
        if not debug:   os.remove(tmpdir+"/delta."+vol.vid)

    catch_signals()
    changed = changed or dnewblocks+dfreedblocks > 0
    lvols[datavol].rotate_snapshots(rotate=changed and monitor_only, delsnap2=monitor_only,
                                    timestamp_path=vol.mapfile())

    if monitor_only:
        print(("\r  %d ch, %d dis" % (dnewblocks, dfreedblocks//chunksize))
                if changed else "\r  no change    ")

    if dnewblocks:   vol.changed_bytes_add(dnewblocks * bsize, save=True)
    catch_signals(**signormal)
    return changed, snap2size, volperms


# Reads addresses from manifest and marks corresponding chunks in a volume's deltamap.

def update_delta_digest_mf(volume, manifest, mapsize=None):
    with open(manifest, "r") as mf, \
         open(volume.mapfile(), "r+b") as bmapf:

        if mapsize:   bmapf.truncate(mapsize)    ; bmapf.flush()
        bmap_mm = mmap.mmap(bmapf.fileno(), 0)       ; chunksize  = volume.archive.chunksize

        for ln in mf:
            addr = int(ln.split()[1][1:], 16)        ; volsegment = addr // chunksize
            bmap_pos = volsegment // 8               ; bmap_mm[bmap_pos] |= 1 << (volsegment % 8)


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a new deltamap and snapshot pair to track
# future changes. After an incremental send, snapshots are rotated and the deltamap is reset.
# An exception is when vol.aliastype is set to "import_other" and no deltamap or snapshot is
# used so the send will perform more slowly like a traditional incremental backup.
#
# Returns (bool, int, int, int) representing whether a session was sent, the current volume size,
# the number of bytes sent and the runtime in seconds.

def send_volume(storage, vol, curtime, ses_tags, send_all=False, changed=True, benchmark=False,
                untar=None):

    def send_chunk_local(tar_info, etag, buf):
        chunkdir = pdirname(_fp := pjoin(destpath, tar_info.name))
        if not _fp.startswith(dest.last_chunkdir):
            makedirs(chunkdir, exist_ok=True)    ; dest.last_chunkdir = chunkdir
        with open(_fp, "wb") as cf:   cf.write(etag); cf.write(buf)

    def send_chunk_remote(tar_info, etag, buf):
        fileobj = BytesIO()
        tar_info.size = fileobj.write(etag) + fileobj.write(buf)   ; fileobj.seek(0)
        tarf_addfile(tarinfo=tar_info, fileobj=fileobj)

    def tar_add_pass(*args, **kwargs):   pass

    dest        = (aset := vol.archive).dest    ; pjoin       = os.path.join
    options     = aset.opts                     ; status1     = not options.unattended
    chunksize   = aset.chunksize                ; dedup       = newmap = False
    chdigits    = max_address.bit_length()//4   ; chformat    = "x%0"+str(chdigits)+"x"
    bksession   = "S_"+curtime                  ; sdir        = pjoin(vol.vid, bksession)
    prior_size  = vol.volsize()
    old_mapfile = vol.mapfile()                 ; addrsplit   = -address_split[1]
    lvols       = storage.lvols
    compress = compressors[aset.compression][2] ; compresslevel = int(aset.compr_level)

    if vol.aliastype == "import_other":
        # Use a plain file or blockdev as a source volume
        volpath = vol.alias                     ; send_all = True    ; assert changed
        vol_ts  = os.stat(volpath).st_mtime_ns  ; volperms = storage.getperms(volpath)
        volsize = storage.getsize(volpath)
    else:
        l_vol   = lvols[lvols[vol.name].snap2]
        if changed:
            volpath = l_vol.path
            vol_ts  = l_vol.gettime()      ; volsize = l_vol.getsize()
        else:
            vol_ts  = vol.sessions[vol.last].gettime()  ; volsize = prior_size
        volperms = lvols[vol.name].getperms()

    if not status1:   print("  [", end="")
    progress, MB = 0, 2**20    ; spinner = status_spinner()
    testtime = time.monotonic()

    if len(vol.sessions):
        # Generate a full manifest to detect unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(vol, vsize=prior_size), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None
    fman_hash     = fman_fname = ""

    ses = vol.new_session(bksession, addtags=list(ses_tags.items()))
    ses.localtime   = str(vol_ts)
    ses.volsize     = volsize
    ses.permissions = volperms
    ses.path        = vol.path+"/"+bksession+"-tmp"

    # Code from init_dedup_indexN() localized here for efficiency.
    if aset.dedupindex:
        dedup       = True
        hashtree, ht_ksize, hash_w, ddataf, chtree, chdigits, ch_w, ses_w  =  aset.dedupindex
        ddataf_read = ddataf.read    ; ddataf_seek = ddataf.seek    ; ddataf_write = ddataf.write
        chtree_max  = 2**(chtree[0].itemsize*8)
        idxcount    = ddataf.seek(0,2) // (ch_w+ses_w)   ; ddblank_ch = bytes(hash_w)
        ddsessions  = aset.dedupsessions                 ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(aset.path)    ; os.makedirs(sdir+"-tmp")
    do_exec([[CP.chattr, "+c", sdir+"-tmp"]], check=False)


    if aset.datacrypto:
        crypto   = True    ; crypto_cadence = aset.datacrypto.ctcadence
        encrypt  = aset.datacrypto.encrypt
    else:
        crypto   = False   ; crypto_cadence = 0    ; etag     = b''


    destpath = dest.path
    if dest.dtype == "file" and options.tar_bypass:
        # Bypass tar stream when sending chunks to local fs
        if debug:   print("\nBypassing tar stream...")
        send_chunk = send_chunk_local
        pdirname   = os.path.dirname   ; makedirs  = os.makedirs
    else:
        send_chunk = send_chunk_remote
        catch_signals(["ALRM","INT"], iflag=True, int_exit=True)    ; signal.alarm(120)

    # Use tar to stream files to destination
    TarInfo = tarfile.TarInfo    ; LNKTYPE = tarfile.LNKTYPE    ; tarf = None
    if not benchmark:
        tarf     = tarfile.open(mode="w|", fileobj=untar.stdin)
        tarf_addfile = tarf.addfile    ; tarf_add = tarf.add              ; stream_started = True
        tar_info = TarInfo(sdir)       ; tar_info.type = tarfile.DIRTYPE  ; tarf_addfile(tar_info)
    else:
        tarf_addfile = tarf_add = tar_add_pass    ; stream_started = False

    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(volpath,"rb", buffering=chunksize) if changed else NulIO() as vf, \
         open(sdir+"-tmp/manifest.tmp", "wt") as hashf, \
         open(old_mapfile,"rb") if not send_all else NulIO() as bmapf:

        vf_seek = vf.seek; vf_read = vf.read   ; BytesIO = io.BytesIO
        gethash = aset.getdatahash             ; b64enc  = base64.urlsafe_b64encode
        b2int   = int.from_bytes               ; islice  = itertools.islice
        compare_digest = hmac.compare_digest   ; zeros   = bytes(chunksize)

        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        addr    = counter = percent = bcount = ddbytes = 0   ; checkpt = 32*MB//chunksize+80
        zdelim  = bytes(64)  ; zdlen = len(zdelim)*8
        minibmap, bmap_list, bmsz   = None, [], zdlen*300

        while addr < volsize:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  next(islice(fullmanifest, zdlen, zdlen), None)
                addr += chunksize*zdlen
                minibmap = bmap_list.pop(0)
            elif not send_all:
                if not changed:   break
                # Get more: split(zdelim) shows where large unmodified zones exist.
                bmap_list.extend(bmapf.read(bmsz).split(zdelim))
                minibmap = bmap_list.pop(0)

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, volsize if send_all
                        else min(volsize, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = chformat % addr    ; signal.alarm(600)

                if fullmanifest:
                    try:
                        fman_hash, fman_fname = fullmanifest_readline().split()
                    except ValueError: # EOF
                        fullmanifest.close()   ; os.remove(fullmanifest.name)
                        fullmanifest = None    ; fman_hash = ""
                    else:
                        if fman_fname != destfile:
                            raise ValueError("expected manifest addr %s, got %s"
                                            % (destfile, fman_fname))

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                # Fetch chunk as buf
                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Process checkpoint
                if counter > checkpt:
                    # Keep updating aset counters if key has a low cadence
                    if 0 < crypto_cadence < 101 and not benchmark:
                        aset.save_ini()    ; tarf_add(aset.confname)
                    # Show progress.
                    if status1:
                        percent = int(addr/volsize*1000)
                        print(f"\r{bcount/MB: 9.1f}MB"[:12], next(spinner),
                              f"{(percent//10):0d}%"[:7], end="", flush=True)
                    elif (addr/volsize)*100 > progress:
                        print("|", end="", flush=True)   ; progress += 5

                    counter = 0

                elif status1 and chunk % checkpt == 0:
                    percent = int(addr/volsize*1000)
                    print(f"\r{bcount/MB: 9.1f}MB"[:12], next(spinner),
                          f"{(percent//10):0d}%"[:7], end="", flush=True)

                # Compress & write only non-empty chunks
                if buf == zeros:
                    if fman_hash != "0":   print("0", destfile, file=hashf)
                    continue

                # Compress chunk and hash it
                buf    = compress(buf, compresslevel)
                bhashb = gethash(buf)   ; b64hash = b64enc(bhashb).decode("ascii")
                # Skip when current and prior chunks are the same
                if compare_digest(fman_hash, b64hash):  continue

                # Add buffer to stream
                print(b64hash, destfile, file=hashf)
                tar_info = TarInfo("%s-tmp/%s/%s" % (sdir, destfile[1:addrsplit], destfile))

                # If chunk already in archive, link to it
                if dedup:
                    i = b2int(bhashb[:ht_ksize], "big")   ; ddses = None   ; pos = 0
                    while (pos := hashtree[i].find(bhashb, pos)) >= 0:
                        if pos % hash_w == 0:
                            # hit: read index item
                            ddataf_seek(chtree[i][pos//hash_w] * (ses_w+ch_w))
                            ddses  = ddsessions[b2int(ddataf_read(ses_w),"big")]
                            if ddses is None:
                                hashtree[i][pos:pos+hash_w] = ddblank_ch # zero-out obsolete entry
                            else:
                                # make dedup link
                                ddchx = ddataf_read(ch_w).hex().zfill(chdigits)
                                tar_info.type = LNKTYPE
                            ddataf_seek(0,2)    ; break
                        pos += 1

                    if ddses is None and bhashb != ddblank_ch and idxcount < chtree_max:
                        # miss: add this chunk as new index item
                        hashtree[i].extend(bhashb)   ; chtree[i].append(idxcount)   ; idxcount += 1
                        ddataf_write(ses_index.to_bytes(ses_w,"big") + addr.to_bytes(ch_w,"big"))

                if tar_info.type == LNKTYPE:
                    # send dedup link
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.vid,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)

                else:
                    # Send data chunk to the archive

                    # Encrypt the data chunk
                    if crypto:   etag, buf = encrypt(buf, bhashb)

                    send_chunk(tar_info, etag, buf)
                    bcount += len(buf)    ; counter += 1

                if stream_started:   tarf.members = []

            # Advance addr, except when minibmap is zero len.
            if minibmap or send_all:  addr += chunksize

    # Send session info, end stream and cleanup
    if fullmanifest:   fullmanifest.close()   ; os.remove(fullmanifest.name)
    if (sent := stream_started):
        # Save session info
        if crypto:   aset.datacrypto.save_counter()
        saved_files = ses.save_info(ext=".tmp")
        for fpath in saved_files:    tarf_add(fpath, arcname=fpath)

        os.link(ses.path+"/manifest.tmp", ses.path+"/manifest")
        if len(vol.sessions) > 1 and (ses.volsize // chunksize) != (prior_size // chunksize):
            check_manifest_sequence(vol, vol.sesnames, full=True)

        # Finalize on VM/remote
        catch_signals()
        tar_info = TarInfo("mv")   ; tar_info.type = tarfile.CHRTYPE   ; tarf_addfile(tar_info)
        tar_info = TarInfo("mv2")  ; flistj = json.dumps(saved_files+[sdir+"-tmp"]).encode("utf-8")
        tar_info.size = len(flistj); tarf_addfile(tar_info, fileobj=BytesIO(flistj))
        tarf.close()    ; untar.stdin.flush()

        ses.path = ses.path.rsplit("-tmp",maxsplit=1)[0]   ; os.replace(ses.path+"-tmp", ses.path)
        if exists(ses.path+"/manifest.tmp"):     os.remove(ses.path+"/manifest.tmp")
        if old_mapfile and exists(old_mapfile):  os.remove(old_mapfile)
        ses.volume.init_deltamap()

    else:
        saved_files = []
        vol.delete_session(bksession, remove=False)

    if vol.aliastype != "import_other":
        if (newmap := benchmark and send_all and len(vol.sessions) > 0):
            # If vol lost mapping, create new map in dry-run mode to prevent re-scan
            vol.init_deltamap()    ; update_delta_digest_mf(vol, ses.path+"/manifest")
            if type(l_vol) is LvmVolume:   l_vol.add_tags(["delta"])

        storage.lvols[vol.name].rotate_snapshots(rotate=changed,
                    timestamp_path=vol.mapfile(), addtags=[vol.last])

    catch_signals(**signormal)    ; signal.alarm(0)
    if not sent:   shutil.rmtree(ses.path)

    runtime  = time.monotonic() - testtime
    t_m, t_s = divmod(runtime, 60); t_p = (f"{int(t_m)}m" if t_m else "") + f"{ceil(t_s)}s"
    if status1:
        print(f"\r{bcount/MB: 9.1f}MB"[:12]+" |"+(t_p[:5].center(5) if bcount else "  - "),
              end="", flush=True)
    else:
        print("|"*((100-progress)//5) + "]", f" 100% {bcount/MB: 0.1f}MB ",
              end="", flush=True)
    if dedup and debug:   show_mem_stats()

    return stream_started, bcount, ddbytes, runtime


# Build deduplication hash index and list

def init_dedup_index(aset, listfile="", extra_vols=0):

    ctime      = time.perf_counter()    ; makelist = bool(listfile)
    addrsplit  = -address_split[1]
    # Define arrays and element widths
    hash_w     = hash_bits // 8
    ht_ksize   = 2 # binary digits for tree key
    hashtree   = tuple(bytearray() for x in range(2**(ht_ksize*8)))
    chtree     = tuple(array("I") for x in range(2**(ht_ksize*8)))
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2

    # Create master session list, limit to ses_w range
    for vol in aset.vols.values():
        aset.dedupsessions += vol.sessions.values()
        vol.decode_manifests(vol.sesnames)
    aset.dedupsessions.sort(key=lambda x: x.name, reverse=True)
    ddsessions = aset.dedupsessions[:2**(ses_w*8)-(len(aset.vols)-extra_vols)-1]

    ddataf  = open(tmpdir+"/hashindex.dat","w+b")
    ddataf_read = ddataf.read    ; ddataf_seek = ddataf.seek    ; b2int  = int.from_bytes
    ddataf_write= ddataf.write   ; bfromhex    = bytes.fromhex  ; b64dec = base64.urlsafe_b64decode
    if makelist:   dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = 0
    for sesnum, ses in enumerate(ddsessions):
        vol = ses.volume    ; vid = vol.vid    ; sesname = ses.name
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":   continue
                bhashb = b64dec(ln1)
                i      = b2int(bhashb[:ht_ksize], "big")   ; pos = 0
                while (pos := hashtree[i].find(bhashb, pos)) >= 0:
                    if pos % hash_w == 0:
                        if makelist:
                            ddataf_seek(chtree[i][pos//hash_w] * (ses_w+ch_w))
                            ddses  = ddsessions[b2int(ddataf_read(ses_w),"big")]
                            ddchx  = ddataf_read(ch_w).hex().zfill(chdigits)
                            print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                                (ddses.volume.vid, ddses.name, ddchx[:addrsplit], ddchx,
                                vid, sesname, ln2[1:addrsplit], ln2),
                                file=dedupf)
                            ddataf_seek(0,2)
                        break
                    pos += 1
                else: # while
                    if count < chtree_max:
                        hashtree[i].extend(bhashb)
                        chtree[i].append(count)
                        ddataf_write(sesnum.to_bytes(ses_w,"big"))
                        ddataf_write(bfromhex(ln2[1:]))  # Enh: scale no. digits to match vol size
                        count += 1

    if listfile:   dedupf.close()

    aset.dedupindex    = (hashtree, ht_ksize, hash_w, ddataf, chtree, chdigits, ch_w, ses_w)
    aset.dedupsessions = ddsessions

    if not debug:  return
    print("\nIndexed in %.1f seconds." % (time.perf_counter()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d, sessions: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count, len(ddsessions))
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


# Deduplicate data already in archive

def dedup_existing(aset):
    dest = aset.dest
    if not dest.hlinks:   x_it(1, "Hardlinks not supported on dest.")

    print("\nBuilding deduplication index...", end="", flush=True)
    init_dedup_index(aset, "dedup.lst.gz")

    print("\n%dMB free on destination." % (dest.free//1024//1024))
    print("linking...", end="", flush=True)
    do_exec( [dest.run_args([
               "/bin/cat >"+dest.dtmp+"/dest.lst.gz"
               +" && /usr/bin/python3 "+dest.dtmp+"/dest_helper.py dedup"
               ], destcd=dest.path),
              [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]
            ], infile=tmpdir+"/dedup.lst.gz", out=tmpdir+"/arch-dedup.log")
    print(" done.")
    dest.get_free(tmpdir+"/arch-dedup.log")
    print("%dMB free after." % (dest.free//1024//1024))
    if verbose:   print("".join(open(tmpdir+"/arch-dedup.log","r")))


# Controls flow of monitor and send_volume procedures:

def monitor_send(storage, aset, datavols, monitor_only, imports={}, ses_tags={},
                 use_sesid=None, benchmark=False, vname_sz=30):

    options, dest = aset.opts, aset.dest    ; status1 = not options.unattended
    ms_start = time.monotonic()    ; MB   = 1024**2
    curtime  = use_sesid or time.strftime("%Y%m%d-%H%M%S", time.localtime(time_start))
    dedup    = options.dedup and dest.hlinks

    if not monitor_only and options.dedup and not dest.hlinks:
        err_out("Hardlinks not supported on dest.")   ; options.dedup = aset.opts.dedup = False

    if options.autoprune.lower() == "full":
        for vol in aset.vols.values():   autoprune(vol, apmode="full")

    storage.check_support()
    if not storage.can_snapshot and datavols:
        reason = "Local not a subvolume or no snapshot feature"
        if not storage.online:   reason = "Local path not found"
        err_out(f"\nError, {reason}: ({storage.pooltype}) {storage.path}")
        error_cache.extend(datavols)
        return curtime, ses_tags

    # Try to rename aliased archive volume before sending
    for dv, vol in ((x, aset.vols[x]) for x in datavols[:] if x in aset.vols and not monitor_only):
        if vol.aliastype and vol.aliastype == "rename" and vol.alias != vol.name \
        and vol.alias not in aset.vols:
            rename_volume(storage, aset, vol.name, vol.alias)
            del(datavols[datavols.index(dv)])   ; datavols.append(vol.alias)

    print(f"\nPreparing snapshots in '{storage.path}'...")
    incrementals, send_alls, unchanged_vols, other_vols \
        = storage.prep_snapshots(storage, aset, datavols, monitor_only, other_vols=imports)
    storage.update_vol_list({x.name: x.vid for x in aset.vols.values()})

    if monitor_only:   send_alls.clear()   ; other_vols.clear()

    # Process session tags
    if (get_tags := options.tag and not ses_tags and not monitor_only):
        for tag_opt in options.tag:
            if tag_opt in ("", "@"):   continue
            tag = ArchiveSession.tag_parse(tag_opt, delim=",")
            if not tag:   raise ValueError("Invalid tag "+tag_opt)
            ses_tags.update((tag,))
    if get_tags and (options.tag == [""] or "@" in options.tag):
        print("Enter tag info as 'tagID[, tag description]'. Blank to end input.")
        while ans := ask_input("[%d]: " % (len(ses_tags)+1)).strip():
            tag = ArchiveSession.tag_parse(ans, delim=",")
            if not tag or tag[0] in ses_tags:   print("(not added)"); continue
            ses_tags.update((tag,))
            if len(ses_tags) == ArchiveSession.max_tags:   break
        print(len(ses_tags), "tags total.")

    if len(incrementals) > 0:
        print("Acquiring deltas.", end="", flush=True)
        incrementals = storage.acquire_deltas(storage, aset, incrementals)
        print("", flush=True)

    if monitor_only:
        print(f"\nMonitor at {curtime}:")
    else:
        cmpvols = [x for x in aset.vols.values()
                         if x.name in incrementals + send_alls and x.sessions]
        cmpses  = [v.sessions[v.sesnames[-1]] for v in cmpvols]
        if compare_files(aset, volumes=cmpvols, sessions=cmpses):
            x_it(1, "Error: Local and archive metadata differ.")

        print(f"\nSending backup session {curtime}:" if not benchmark
              else "\nDry run (not sending):")

    hl = "â€”" * (min(max([vname_sz]+[30]), shutil.get_terminal_size().columns)+20)
    print(hl, flush=True)

    totalvsize = totalbcount = 0    ; untar = tarf = None   ; sent_vols = []
    if not (monitor_only or benchmark):
        cmd = ["python3 "+dest.dtmp+"/dest_helper.py send "
            + (" --sync" if options.maxsync else "") + (" --debug" if debug else "")]
        if debug:   print("\nDest run:\n", cmd, end=" ")
        untar = SPr.Popen(dest.run_args(cmd, destcd=dest.path), process_group=0,
                    stdin=SPr.PIPE, stdout=SPr.DEVNULL, stderr=open(tmpdir+"/send.log","w"))

    for datavol in list(other_vols.keys()) + sorted(incrementals + send_alls + unchanged_vols):
        vol = aset.vols[datavol]    ; svsize = sbcount = stime = ddbytes = 0
        sent, updated = False, datavol not in unchanged_vols
        print(f"Volume '{datavol}'\n" if not status1 else
              f"            |     | {datavol}\r", flush=True, end="")

        # Process delta metadata (update_delta_digest())
        if datavol in incrementals:
            updated, svsize, vperms = storage.process_deltas(storage, aset, datavol, monitor_only)
            if monitor_only and not updated:   print("  no change")
        elif monitor_only:
            print("  no change")

        if monitor_only:   continue

        updated = updated or bool(vol.map_used())

        if updated or options.send_unchanged:
            if dedup and not aset.dedupindex and not vol.is_mdvol() and updated:
                if status1:   print("I/", end="\r", flush=True)
                init_dedup_index(aset, extra_vols=len(other_vols))

            # Enh: include send_alls and other_vols in changed bytes tally
            if vol.changed_bytes > dest.free:
                autoprune(vol, needed_space=vol.changed_bytes, apmode=options.autoprune)

            # Send Volume
            if debug:   print("\nuntar status:", untar.poll())
            sent, sbcount, ddbytes, stime = \
                send_volume(storage, vol, curtime, ses_tags, send_all=datavol in send_alls,
                            changed=updated, benchmark=benchmark, untar=untar)

            dest.free -= int(sbcount + (sbcount * 0.05))    ; sent_vols.append(vol)

        else:
            storage.lvols[vol.name].rotate_snapshots(rotate=False)

        if vol.aliastype == "import_other":   del(imports[datavol])

        p1, p2 = ("\r", " |  -" if not sent else "") if status1 else ("", "")
        print(p1 if sbcount or ddbytes or updated else p1+"  no change"+p2, flush=True)
        if ddbytes and verbose:   print(f"  (reduced {ddbytes/MB:0.1f}MB)")
        totalvsize += vol.volsize()    ; totalbcount += sbcount

    if untar:
        untar.stdin.close()
        try:
            untar.wait(timeout=180)
        except SPr.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 99
            untar.kill()
        else:
            retcode = untar.poll()

        if retcode != 0:
            if debug and exists(lf := tmpdir+"/send.log"):   SPr.run([CP.cat, "-v", lf])
            raise RuntimeError("tar transport failure code %d" % retcode)

    # Local cache finalize
    catch_signals()
    for ses in (v.sessions[v.last] for v in sent_vols):
        ses.rename_saved(ext=".tmp")
    catch_signals(**signormal)

    if not monitor_only and (volct:= len(sent_vols)):
        print(hl, "\n%0d volumes, %0dâ€”â€”>%0d MB in %.1f seconds."
              % (volct, totalvsize/MB, totalbcount/MB, time.monotonic() - ms_start)
              )

    return curtime, ses_tags


# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS or ^tagname format.

def prune_sessions(vol, times, allvols=False):

    options = vol.archive.opts    ; allbefore = options.allbefore
    sesnames = vol.sesnames       ; t1, t2    = "", ""                ; to_prune  = []

    if len(sesnames) < 2:
        return 0, "No extra sessions to prune"
    if options.allbefore and len(times) != 1:
        raise OptionError("--all-before requires one session.")
    for ii in range(len(times)):
        if times[ii] == "newest":   times[ii] = sesnames[-1]

    # Validate date-time params
    for pos, dt in enumerate(times[:]):
        if not dt[0].startswith("^"):
            if not dt.startswith("S_"):   times[pos] = "S_"+dt.strip()
            datetime.datetime.strptime(times[pos][2:], "%Y%m%d-%H%M%S")
        elif dt[1:] not in vol.tags:
            return 0, f"No match for {dt}"
        elif pos == 0:
            for sn in sesnames:
                if dt[1:] in vol.sessions[sn].tags:
                    if len(times) == 1 and not allbefore:
                        to_prune.append(sn)
                    else:
                        t1 = sn   ; break
        elif pos == 1:
            for sn in reversed(sesnames):
                if dt[1:] in vol.sessions[sn].tags:   t2 = sn   ; break

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    if allbefore:   t1 = sesnames[0]    ; t2 = times[0]
    if not t1:   t1 = times[0]
    if not t2 and len(times) > 1:
        t2 = times[1]
        if t2 <= t1:  raise SelectionError("Second date-time must be later than first.")

    # Find specific sessions to prune in contiguous range
    if to_prune:
        pass

    elif t2 == "":
        # find single session
        if t1 in sesnames:   to_prune.append(t1)

    else:
        # find sessions in a date-time range
        start = len(sesnames)   ; end = 0
        if t1 in sesnames:
            start = sesnames.index(t1)
        else:
            for ses in sesnames:
                if ses > t1:   start = sesnames.index(ses)    ; break
        if t2 in sesnames:
            end = sesnames.index(t2) + int(not allbefore)
        else:
            for ses in reversed(sesnames):
                if ses < t2:   end = sesnames.index(ses) + 1  ; break
        to_prune = sesnames[start:end]

    if len(to_prune) and to_prune[-1] == sesnames[-1]:
        if verbose:   print(" Preserving latest session -", end="")
        del(to_prune[-1])
    if len(to_prune) == 0:
        return 0, "No selections in this date-time range"

    return autoprune(vol, apmode=options.autoprune, include=set(to_prune), allvols=allvols)


# Parameters / vars for autoprune:
# oldest (date): date before which all sessions are pruning candidates
# thin_days (int): number of days ago before which the thinning params are applied
# ndays & nsessions: a days/sessions ratio for amount of sessions left after thinning
# nthresh: min number of sessions to prune this time (0 = prune all candidates)
# target_size (0 or MB int): User-selected size cap for archive (future)

def autoprune(vol, needed_space=0, apmode="off", include=set(), allvols=False):
    aset = vol.archive    ; options = aset.opts

    dtdate = datetime.date
    def to_date(sesdate):
        return dtdate(int(sesdate[2:6]),int(sesdate[6:8]),int(sesdate[8:10]))

    if len(vol.sesnames) < 2:   return 0, ""

    today = dtdate.today()
    if (apdays := vol.archive.apdays or "0:62:1:2") and all(map(is_num, tup := apdays.split(":"))):
        ap1, ap2, ndays, nsessions = map(int, tup)
        if ap1 < 0 or ap2 < 0 or ndays < 1 or nsessions < 1:
            raise SelectionError("apdays must be at least '0:0:1:1'")
        oldest    = (today - datetime.timedelta(days=ap1)) if ap1 else 0
        thin_days = datetime.timedelta(days=ap2) if ap2 else 0
    else:
        apmode = "off"
    if not apdays or not (ap1 + ap2):
        apmode = "off"

    if apmode not in ("on","full") and not include:   return 0, ""

    datavol   = vol.name                      ; dest    = vol.archive.dest
    apmode    = apmode.lower()                ; exclude = set()    ; marked  = 0
    sessions  = sorted(aset.get_global_ap_ses())
    startdate = to_date(sessions[0])          ; nthresh = 3 if apmode == "on" else 0
    enddate   = min(to_date(sessions[-2]), today-thin_days) if len(sessions) > 2 else 0

    # Make a 2d array of ordinal dates and populate with session id + flag
    apcal = { day: [] for day in range(dtdate(startdate.year,1,1).toordinal(),
                                       to_date(sessions[-1]).toordinal()+ndays) }
    for ses in sessions:  apcal[to_date(ses).toordinal()].append([ses, True])

    # Build set of excluded sessions
    for sx in options.keep:
        if not sx.startswith("^"):
            datetime.datetime.strptime(sx, "%Y%m%d-%H%M%S")    ; exclude.add("S_"+sx)
        else:
            if sx[1:] == "all":
                exclude += {x.name for x in vol.sessions if x.tags}
            else:
                exclude += {x.name for x in vol.sessions if sx[1:] in x.tags}
    include -= exclude

    # Mark all sessions prior to oldest date setting, plus include list
    for ses in sessions:
        sdate = to_date(ses)
        if (apmode != "off" and oldest and sdate <= oldest) or ses in include:
            for dses in apcal[sdate.toordinal()]:
                if dses[0] == ses and ses in vol.sessions and ses not in exclude:
                    vol.sessions[ses].toggle = False

    # Mark sessions for thinning-out according to ndays + nsessions;
    # avoid for "wyng-*metadata" volumes.
    if apmode != "off" and thin_days and enddate and not vol.is_mdvol():
        for year in range(startdate.year, enddate.year+1):
            for span in range(dtdate(year,1,1).toordinal(),
                            min(dtdate(year,12,31), enddate).toordinal(), ndays):
                dlist = []    ; offset = 0
                for day in range(span, min(span+ndays, enddate.toordinal())):
                    dlist.append(sum( x[1] for x in apcal[day] ))
                while sum(dlist) > nsessions: ## Enh: Make even distribution
                    bigday = dlist.index(max(dlist[offset:]), offset)
                    offset += (ndays//nsessions)+1    ; offset %= min(ndays, len(dlist))
                    for dses in apcal[span+bigday]:
                        if dses[1]:
                            # always decr bigday, but don't toggle if session is excluded
                            dlist[bigday] -= 1    ; dses[1] = False
                            if dses[0] in vol.sessions:
                                vol.sessions[dses[0]].toggle = dses[0] in exclude
                            break

    # Find contiguous marked ranges and merge/prune them. Repeat until free >= needed space.
    pruned, factor = set(), 1    ; sessions.append("End")
    while True:
        to_prune = []     ; removed_ct = 0    ; skipped = False
        for ses in sessions:
            if ses and ses != "End" and (ses not in vol.sesnames
            or (vol.is_mdvol() and not allvols)):
                sessions[sessions.index(ses)] = None   ; continue
            if ses is None:   continue
            if ses == "End" or vol.sessions[ses].toggle :
                if to_prune:
                    # prioritize ranges that overlap with requested includes
                    if include and not (set(to_prune) & include):
                        to_prune.clear()    ; skipped = True    ; continue

                    target_s = vol.sesnames[vol.sesnames.index(to_prune[-1]) + 1]
                    merge_sessions(vol, to_prune, target_s, clear_sources=True)
                    pruned.update(to_prune)

                    for i in to_prune:   sessions[sessions.index(i)] = None
                    include -= set(to_prune)    ; removed_ct += len(to_prune);   to_prune.clear()
                    if not include and nthresh and removed_ct >= nthresh*factor:  break # for ses
            else:
                to_prune.append(ses)

        if removed_ct:
            dest.get_free(tmpdir+"/merge.log")    ; os.remove(tmpdir+"/merge.log")

        if skipped and apmode != "off":   continue # while
        if removed_ct == 0 or nthresh == 0 or needed_space <= dest.free:
            break # while
        elif factor > 4:
            nthresh = 0
        else:
            factor += 2

    if debug:   print(pruned)
    return len(pruned), ""


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session name.

def merge_manifests(vol, msessions=None, mtarget=None, vsize=None, addcol=False, rdiff=[]):
    # Enh: implement mtarget to support merge_sessions()
    aset      = vol.archive                    ; slist  = []
    msessions = msessions or vol.sesnames
    sespaths  = [ os.path.basename(vol.sessions[x].path) for x in msessions ]
    tmp       = aset.big_tmpdir if vol.volsize() > 128000000000 else tmpdir
    outfile   = tempfile.NamedTemporaryFile(dir=tmp, prefix="mout_", delete=False)

    if not aset.dedupsessions:   vol.decode_manifests(msessions)
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, prefix="sl_", delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(sespaths)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir_obj = tempfile.TemporaryDirectory(dir=tmp, prefix="m_")
        cdir     = cdir_obj.name    ; slsort  = slist[1]

        # fix: extrapolate path with filename
        for fname in sespaths:
            open(cdir+"/"+fname, "w").close()
            do_exec([[CP.awk, r'{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                                +cdir+r'/"FILENAME}', fname+"/manifest"]], cwd=vol.path)
    else:
        cdir  = vol.path     ; slsort  = slist[0]

    cmds = [[CP.sort, "-umsd", "-k2,2", "--batch-size=16", "--files0-from="+slsort]]

    if rdiff:
        # merge a different set of manifests, use their address column to filter the main merge:
        mfilter = merge_manifests(vol, msessions=list(reversed(rdiff)), vsize=vsize, addcol=False)
        cmds.append([CP.sort, "-msd", "-k2,2", mfilter, "-"])
        cmds.append([CP.awk, r'{if (PREV==$2) print $0} {PREV=$2}'])

    if vsize is not None and vsize < max(vol.sessions[x].volsize for x in msessions):
        # session vol size shrank, so filter result by requested vsize
        cmds.append([CP.awk, r'$2<="'+vol.last_chunk_addr(vsize)[1]+'"'])

    do_exec(cmds, out=outfile.file, cwd=cdir)

    for f in slist:  os.remove(f)
    if rdiff:        os.remove(mfilter)
    if addcol:       cdir_obj.cleanup()
    return outfile.name


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume, source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(volume, sources, target, clear_sources=False):

    aset       = volume.archive    ; dest = aset.dest    ; resume = bool(aset.in_process)
    opts       = aset.opts         ; vid  = volume.vid
    m_tmp      = tmpdir if volume.volsize() < 128000000000 else aset.big_tmpdir
    if debug:   print(f"Merge {vid} '{volume.name}'\n", sources, "T", target)

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)

    if not resume:
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max(x.volsize for x in volume.sessions.values()
                                    if x.name in sources)
        lc_filter  = '"'+volume.last_chunk_addr(volsize)[1]+'"'

        with open("merge.lst", "wt") as lstf:
            print(merge_target, target, file=lstf)
            volume.decode_manifests(merge_sources + [merge_target])

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                do_exec([[CP.sed, "-E", r"s|$| "+ses+r"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umsd", "-k2,2", "--batch-size=16"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umsd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]],
                out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, r"$2<="+lc_filter+r" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, r"$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                r"s|^0 x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"-rm " +merge_target+ r"/\1/x\1\2|; t; "

                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"rename \3/\1/x\1\2 " +merge_target+ r"/\1/x\1\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.lst")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, r"$2>"+lc_filter, m_tmp+"/manifest.two"],
                     [CP.sed, "-E", r"s|^\S+\s+x(\S{" + str(address_split[0]) + r"})(\S+)|"
                                    r"-rm " + merge_target + r"/\1/x\1\2|"]
                    ], out=">>merge.lst")

        do_exec([[CP.gzip, "-f", "merge.lst"]])

    if not resume:
        # Set archive in_process state to "merge"
        aset.set_in_process(["merge", vid, str(clear_sources), target, sources],
                            tmp=False, todest=False)

    # Update & send new metadata and process lists to dest
    if clear_sources:
        for ses in sources:
            if ses in volume.sessions:
                if exists((_f := volume.sessions[ses].path+"/manifest")):   os.remove(_f)
                volume.delete_session(ses, remove=False)

    lcmds, dest_cmds = [], ""
    if not resume:
        aset.set_in_process(None, tmp=True, todest=False)
        files = volume.sessions[target].save_info(".tmp")
        lcmds += [CP.tar, "-cf", "-"] + files + [vid+"/merge.lst.gz", "archive.ini"]
        dest_cmds += "tar -o -xf -"

    # Start merge operation on dest
    catch_signals()
    retcode = do_exec([lcmds, dest.run_args([dest_cmds + "\n cd "+vid
                            +" && python3 "+dest.dtmp+"/dest_helper.py merge "+merge_target
                            + (" --resume" if resume else "")+(" --sync" if opts.maxsync else "")
                            ], destcd=dest.path),
                       [CP.cat, "-v"], [CP.tail, "--bytes=2000"]
                      ], cwd=aset.path, pgroup=0, check=False, pipefail=True,
                         out=tmpdir+"/merge.log"
                     )

    if retcode == 50:
        # Initialization didn't complete, so reload aset and abort
        aset.stop()
        aset = ArchiveSet(aset.path, aset.dest, aset.opts, prior_auth=aset)
        aset.set_in_process(None)
        for f in ("merge.lst.gz", "volinfo.tmp", target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest.run([f"cd {vid}  &&  rm -rf merge merge.lst.gz"],
                 destcd=dest.path, check=False)
        x_it(1, "Error: Merge could not initialize!")
    elif retcode != 0:
        x_it(retcode, "Error: Remote exited!")

    # Local finalize
    volume.sessions[target].rename_saved(ext=".tmp")
    aset.set_in_process(None, save=False)
    catch_signals(**signormal)

    # Check consistency after resuming merge
    if resume and compare_files(aset, volumes=[volume], sessions=[volume.sessions[target]],
                                manifest=True):
        x_it(1, "Error: Local and dest metadata differ.")

    os.remove("merge.lst.gz")
    for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(storage, vol, select_ses="", ses_strict=False, save_path="",
                   diff=False, verify_only=0):

    def punch_zero_hole(fn, loc):
        if not punch_hole(fn, loc, chunksize):
            volf_seek(loc)   ; volf_write(zeros)


    dest        = (aset := vol.archive).dest      ; options      = aset.opts
    attended    = not options.unattended          ; debug        = options.debug
    verbose     = (options.verbose or attended) and verify_only != 2    ; MB = 2**20
    sparse      = options.sparse                  ; sparse_write = options.sparse_write or sparse
    chunksize   = aset.chunksize                  ; use_snapshot = False
    sessions    = vol.sesnames                    ; zeros        = bytes(chunksize)
    # functions
    compress    = compressors[aset.compression][2]; compresslevel = int(aset.compr_level)
    decompress  = compressors[aset.compression][0].decompress
    decrypt     = aset.datacrypto.decrypt if aset.datacrypto else None
    compare_digest = hmac.compare_digest          ; b64enc    = base64.urlsafe_b64encode
    gethash     = aset.getdatahash

    if not (storage.online or verify_only or options.save_to):
        err_out("Please specify a valid --local storage.")   ; error_cache.append(vol.name)
        return None

    if diff or verify_only:
        save_path = ""

    # Set the session to retrieve
    if not sessions:
        err_out("No sessions available.")
        return None

    if select_ses:
        if select_ses[0] == "^":
            # match tag to session id
            tag = select_ses[1:]
            if tag in vol.tags:
                select_ses = sorted(vol.tags[tag])[-1]
                print("Matched tag to", select_ses)
        else:
            # validate date-time input
            datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
            select_ses = "S_"+select_ses

        if (ses_strict and select_ses not in sessions) \
        or (not ses_strict and select_ses < sessions[0]):
            err_out(f"No volume '{vol.name}' in session {select_ses}.")
            return None

        if not ses_strict and select_ses not in sessions:
            print(f"Selecting {select_ses[2:]} as ", end="")
            select_ses = [s for s in sessions if s < select_ses][-1]   ; print(select_ses[2:]+".")

    else:
        # default to last session
        select_ses = sessions[-1]

    ses_obj     = vol.sessions[select_ses]           ; volsize     = ses_obj.volsize
    addrsplit   = -address_split[1]                  ; rc = l_vol  = volf = None
    lchunk_addr, last_chunkx = vol.last_chunk_addr(volsize)
    assert vol.aliastype in (None, "rename")

    # Prepare save volume
    if not (diff or verify_only):

        if not LocalStorage.fallocate:
            sparse = sparse_write = use_snapshot = False
            print("Sparse and snapshot modes disabled.")

        # Decode save path semantics
        if save_path:
            save_storage = None       ; returned_home = False
            save_type,_,_,_ = LocalStorage.parse_local_path(ldir := os.path.dirname(save_path))
            if save_type:
                save_storage = LocalStorage(ldir, options, auuid=aset.uuid,
                                            arch_vols=storage.arch_vols.copy())
        else:
            if not storage.online:   x_it(1, "Local storage is offline!")
            save_storage = storage    ; returned_home = storage.online
            save_type = "tlvm pool" if storage.pooltype=="tlvm" else "file"
            l_vol = save_storage.new_vol_entry(vol.alias, vol.vid)    ; save_path = l_vol.path

        # possibly use snapshot as baseline for receive
        if returned_home and options.use_snapshot and save_storage.pooltype in ("rlnk","tlvm") \
        and (snap_lv := save_storage.lvols[l_vol.snap1]).paired_state(vol.mapfile(), vol.last)=="Y":
            assert l_vol.path == save_path
            sparse_write = use_snapshot = True    ; sparse = False
            l_vol.delete(force=True)
            l_vol.create(snapshotfrom=snap_lv.name, ro=False)

        elif save_type == "tlvm pool":
            if not l_vol.exists():
                print("Creating '%s' in thin pool %s[%s]." %
                      (l_vol.name, save_storage.path, save_storage.lvpool))
                # Fix:  translate to safe lvm name
                l_vol.create(volsize, ro=False)    ; sparse_write = sparse = False

            elif l_vol.getsize() != volsize:
                if debug:   print("Re-sizing LV to %d bytes." % volsize)
                l_vol.resize(volsize)

        if save_type in("tlvm pool","block device") and exists(save_path):
            punch_hole = save_storage.block_discard_chunk
            if not sparse_write:
                do_exec([[CP.env, "-u", "LC_ALL", CP.blkdiscard, save_path]])
            volf = open(save_path, "r+b")

        elif os.path.abspath(save_path).startswith("/dev/"):
            x_it(1, "Cannot create new volume from ambiguous /dev path.\n"
                    " Please create the volume before using 'receive', or specify"
                    " --save-to=volgroup/pool/volname in case of a thin LVM volume.")
        else:
            save_type = "file"   ; punch_hole = save_storage.file_punch_hole
            if not exists(save_path) or not sparse_write:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                open(save_path, "wb").close()    ; sparse_write = sparse = False

            volf = open(save_path, "r+b")    ; volf.truncate(volsize)

    elif diff:
        l_vol = storage.lvols[vol.alias]     ; snap1vol = l_vol.snap1
        paired = storage.lvols[snap1vol].paired_state(vol.mapfile(), vol.last)
        if not l_vol.exists():
            err_out(f"{vol.name}: Local volume does not exist.")
            return None

        if paired == "Y":
            l_vol = storage.lvols[snap1vol]
        elif paired == "D":
            l_vol = storage.lvols[snap1vol]
            print("Snapshot is type delta:")
        elif storage.lvols[snap1vol].exists():
            l_vol = storage.lvols[snap1vol]
            print("Snapshot is not paired:")
        else:
            print("Snapshot not available; Comparing with source volume instead.")

        if volsize != l_vol.getsize():
            err_out("Volume sizes differ:\n  Archive = %d \n  Local   = %d"
                    % (volsize, l_vol.getsize()))   ; return None

        volf  = open(l_vol.path, "r+b")

    if volf:
        volf_read = volf.read     ; volf_write = volf.write    ; volf_seek = volf.seek
        volfno    = volf.fileno() ; fcntl.lockf(volf, fcntl.LOCK_EX|fcntl.LOCK_NB)

    if verify_only != 2:
        print("\nReceiving" if save_path else "\nVerifying", f"volume '{vol.name}'",select_ses[2:])
        if save_path:    print("Saving to %s '%s'" % (save_type, save_path))

    # Collect session manifests
    diff_ses = []
    if use_snapshot and save_storage and save_storage.online:
        # sessions after selected, except if last ses (then we're already done)
        if select_ses == vol.last:
            save_storage.setperms(save_path, ses_obj.permissions)
            if is_num(ses_obj.localtime):   save_storage.settime(save_path, int(ses_obj.localtime))
            print("Snapshot retrieved...Done.")
            return volsize
        print("Using snapshot as baseline.")
        incl_ses = sessions[:sessions.index(select_ses)+1]
        diff_ses = sessions[sessions.index(select_ses)+1:]
    elif verify_only == 2:
        # only selected session
        incl_ses = [select_ses]
    else:
        # everything upto & including selected
        incl_ses = sessions[:sessions.index(select_ses)+1]

    # Verify metadata then merge manifests and send chunk list to dest system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    if compare_files(aset, volumes=[vol], sessions=[vol.sessions[x] for x in incl_ses]):
        x_it(1, "Error: Local and archive metadata differ.")
    vol.decode_manifests(incl_ses+diff_ses, force=True)
    manifest = merge_manifests(vol, msessions=incl_ses, addcol=True, rdiff=diff_ses)

    if debug:
        print(f"{vol.vid} CHUNK SIZE: {chunksize}\nEND SEQUENCE:")
        import collections
        print("\n".join(collections.deque(  (" ".join(x.split()[1:]) for x in open(manifest)
                                            if not x.startswith("0 ")), 10)))

    if not sparse:
        cmds = [[CP.sed, "-E", r"/" +last_chunkx+ r"/q", manifest],  ## Enh: detect vol_shrank
                [CP.sed, "-E", r"/^0\s/ d; "
                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|\3/\1/x\1\2|;"],
                [CP.gzip, "-c", "-4"
                ],
                dest.run_args(["cat >"+dest.dtmp+"/dest.lst.gz"]),
            ]   # Enh: replace list with range
        do_exec(cmds)

    # Create retriever process using py program
    cmd = dest.run_args(
            ["cd " + vol.vid
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ], destcd=dest.path)
    getvol   = SPr.Popen(cmd, stdout=SPr.PIPE, stdin=SPr.PIPE if sparse else SPr.DEVNULL,
                         stderr=open(tmpdir+"/receive.log","w") if debug else SPr.DEVNULL)
    gv_stdin = io.TextIOWrapper(getvol.stdin.raw, encoding="utf-8", write_through=True) \
                if sparse else None

    # Open manifest then receive, check and save data
    if verify_only != 2 and not verbose:   print("[", end="")
    addr = -1    ; bcount = diff_count = progress = 0    ; buf = b''    ; magic = dest.magic
    for mfline in open(manifest, "r") if volsize else []:
        if lchunk_addr <= addr >= 0:   break
        cksum, faddr, ses = mfline.split()    ; addr = int(faddr[1:], 16)

        if debug:
            print(f"\r{(addr/volsize):.1%}", faddr, ses, end=" ")
        elif verbose:
            print("".join(("\r[", ("|" * ceil((x:=addr/volsize)*20)).ljust(20,"."),
                  f"] {x:.1%}")), end=" ", flush=True)
        elif (addr/volsize)*100 > progress and verify_only != 2:
            print("|", end="", flush=True)   ; progress += 5

        # Process zeros quickly
        if cksum == "0":
            if save_path:
                volf_seek(addr)
                if sparse_write and volf_read(chunksize) != zeros:
                    punch_zero_hole(volfno, addr)    ; diff_count += chunksize
            elif diff:
                volf_seek(addr)   ; diff_count += chunksize if volf.read(chunksize) != zeros else 0
                if diff_count:   break

            continue

        # Request chunks on-demand if local chunk doesn't match cksum
        if sparse and save_path:
            volf_seek(addr)    ; localbuf = volf_read(chunksize)
            if localbuf == zeros:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)
            elif b64enc(gethash(compress(localbuf, compresslevel))).decode("ascii") == cksum:
                continue
            else:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)

        # Read chunk size
        if getvol.stdout.read(3) != magic:   raise ValueError("Bad magic.")
        untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

        # allow for slight expansion from compression algo
        if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
            if options.skip_corrupt_chunks and save_path:
                print("Skipping wrong-sized chunk at", addr)
                punch_zero_hole(volfno, addr)
                continue
            else:
                if save_path and save_type == "file":
                    volf.truncate(0)
                elif save_path:
                    do_exec([[CP.env, "-u", "LC_ALL", CP.blkdiscard, save_path]])
                raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

        ##  Size is OK  ##
        size = untrusted_size

        # Read chunk buffer
        untrusted_buf = getvol.stdout.read(size)
        rc  = getvol.poll()
        if rc is not None and len(untrusted_buf) == 0:
            break

        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)    ; rc  = getvol.poll()
            raise BufferError("rc=%s. Got %d bytes, expected %d" % (repr(rc), len(untrusted_buf), size))


        # Decrypt the data chunk
        # Validation MUST be next step!
        if decrypt:
            untrusted_buf = decrypt(untrusted_buf)

        # Validate data chunk
        if not compare_digest(cksum, b64enc(gethash(untrusted_buf)).decode("ascii")):
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            print(size, mfline)
            if options.skip_corrupt_chunks and save_path:
                print("Skipping corrupt chunk at", addr)
                punch_zero_hole(volfno, addr)
                continue
            else:
                if save_path and save_type == "file":
                    volf.truncate(0)
                elif save_path:
                    do_exec([[CP.env, "-u", "LC_ALL", CP.blkdiscard, save_path]])
                raise ValueError("Bad hash "+faddr+" :: "+str(b64enc(gethash(untrusted_buf))))

        ##  Buffer is OK  ##
        buf = untrusted_buf   ; bcount += len(buf)

        if verify_only:   continue

        # Proceed with decompress.
        decomp = decompress(buf)
        if len(decomp) != chunksize and addr < lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        if addr == lchunk_addr and len(decomp) != volsize - lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        buf = decomp

        # Write data to volume (receive)
        if save_path:
            volf_seek(addr)
            # Don't re-check buffer for sparse mode, check for sparse_write:
            if sparse:
                volf_write(buf)    ; diff_count += len(buf)
            elif sparse_write:
                if use_snapshot or volf_read(chunksize) != buf:
                    volf_seek(addr)    ; volf_write(buf)    ; diff_count += len(buf)
            else:
                volf_write(buf)
        elif diff:
            volf_seek(addr)    ; diff_count += len(buf) if buf != volf.read(chunksize) else 0
            if diff_count:   break

    if debug:   print("\nEND receive stream.")
    if gv_stdin:   gv_stdin.close()
    rc = getvol.poll()
    if rc and debug:
        err_out("Warn: Helper exited rc="+repr(rc))
    elif getvol.stdout.read(3) != magic and debug:
        err_out("Warn: No magic.")
    getvol.kill()

    if ((not sparse and verify_only != 2 and not use_snapshot and not diff)) \
    and addr != lchunk_addr:
        err_out("Last chunk at %d does not match volume %d." % (addr, lchunk_addr))
        #return None
        diff_count += chunksize

    if verbose:   print("\r[" + "|"*20, end="")
    print("] 100% " if verify_only != 2 else "", ": OK " if not diff_count or save_path else " ",
          f" Diff bytes: {diff_count}" if diff_count else "",
          end="" if verbose else "\n", sep="")
    if verbose:
        print(f" Data bytes: {bcount}", f"/ {volsize}" if verify_only != 2 else "")

    if save_path:
        volf.flush()    ; os.fsync(volf.fileno())    ; volf.close()
        if save_storage and save_storage.online:
            save_storage.setperms(save_path, ses_obj.permissions)
            save_storage.settime(save_path, ses_obj.gettime())
        if returned_home and save_storage.pooltype in ("rlnk","tlvm") \
        and select_ses == sessions[-1]:
            if debug:   print(f"Pairing snapshot.")
            vol.init_deltamap()
            tags = ["wyng", "arch-"+aset.uuid, sessions[-1]]
            save_storage.lvols[l_vol.snap2].delete()
            save_storage.lvols[l_vol.snap2].create(snapshotfrom=vol.alias, addtags=tags)
            l_vol = l_vol.update()    ; save_storage.lvols[l_vol.snap2].update()
            l_vol.rotate_snapshots(timestamp_path=vol.mapfile())

    elif diff and diff_count:
        return None

    if not debug:   os.remove(manifest)
    return bcount


# Rename a volume in the archive

def rename_volume(storage, aset, oldname, newname):

    os.chdir(aset.path)    ; vol = aset.vols[oldname]
    if not aset.rename_volume_meta(oldname, newname, desc=aset.opts.voldesc, ext=".tmp"):
        x_it(1, "Error: Cannot rename '%s' to '%s'." % (oldname,newname))

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    vol.rename_saved(ext=".tmp")
    catch_signals(**signormal)
    if verbose:   print("Archive volume renamed.")

    # move snapshots to new pathname
    if not storage or not storage.online:
        print("Note: A valid --local location was not specified; no local snapshots"
              " will be renamed which may result in a slow rescan on the next 'send'.\n")
        return

    new_lvol = storage.new_vol_entry(newname, vol.vid)    ; old_lvol = storage.lvols[oldname]
    for atr in ("snap1","snap2"):
        oldsnap, newsnap = storage.lvols[getattr(old_lvol, atr)], \
                           storage.lvols[getattr(new_lvol, atr)]
        newsnap.delete()
        if new_lvol.exists() and oldsnap.name != newsnap.name:
            if oldsnap.exists():
                oldsnap.rename(newsnap.name)
                if verbose:   print("Local snapshot renamed.")
        else:
            oldsnap.delete()


def add_volume(aset, datavol, desc, storage=None):
    vol = aset.add_volume_meta(datavol, desc=desc, ext=".tmp")
    if not vol:   return

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    vol.rename_saved(ext=".tmp")
    catch_signals(**signormal)

    if storage and storage.online:
        new_lvol = storage.new_vol_entry(datavol, vol.vid)
        for atr in ("snap1","snap2"):
            if (lv := storage.lvols[getattr(new_lvol, atr)]).is_arch_member() == "true":
                lv.delete()


# Remove a volume from the archive

def delete_volume(storage, aset, dv=None, vid=None):
    assert not (dv and vid)   ; inproc = aset.in_process and aset.in_process[0] == "delete"

    if not storage or not storage.online:
        print("Note: A valid --local location was not specified; no volume snapshots"
              " will be removed.\n")

    if vid:
        if vid not in aset.conf["volumes"] or not vid.startswith("Vol_"):
            if inproc:   aset.set_in_process(None)
            err_out(f"Vid '{vid}' not found.")
    else:
        aset.load_volumes(1)
        if dv not in aset.vols:
            x_it(1, f"Volume '{dv}' not found.")
        vid = (vol := aset.vols[dv]).vid

    if dv and vol.is_mdvol() and not aset.opts.force:
        raise OptionError("Use --force to delete a metadata volume.")

    if not aset.opts.unattended and not aset.opts.force and not inproc:
        print("\nWarning! Delete will remove ALL metadata AND archived data",
              f"for volume '{dv or vid}'")

        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:   x_it(0)

    if vid in aset.conf["volumes"]:
        print(f"\nDeleting volume '{dv or vid}' from archive.")
        catch_signals()    ; inproc = True
        aset.set_in_process(["delete", vid])
        dvid = aset.delete_volume_meta(vid=vid)
        update_dest(aset, pathlist=[aset.confname])
        aset.set_in_process(None)
        catch_signals(**signormal)

    if dvid.startswith("Vol_") and inproc:
        aset.dest.run(["rm -rf '%s'" % dvid], destcd=aset.dest.path)

    if storage and storage.online and dv:
        storage.new_vol_entry(dv, dvid)
        for lvol_name in (storage.lvols[dv].snap1, storage.lvols[dv].snap2):
            storage.lvols[lvol_name].delete()
    else:
        print("(Skipping snapshot removal.)")

    x_it(int(not inproc))


# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.

def retry_in_process(aset):
    ipskip = False    ; options = aset.opts

    if options.action == "delete" and (aset.in_process[1] in [options.vid]) \
    or (options.volumes and aset.in_process[1] == aset.vols[options.volumes[0]].vid):
        # user is currently deleting the in_process volume or using --clean --force
        if options.clean and options.force:
            ipskip = True
        else:
            aset.set_in_process(None)
    elif exists(aset.path+"/in_process_retry") and not options.force_retry:
        raise RuntimeError("Interrupted process already retried.")
    elif aset.in_process:
        open(aset.path+"/in_process_retry","w").close()

    if not ipskip and aset.in_process and aset.in_process[0] in ("delete","merge") \
    and aset.in_process[1].startswith("Vol_"):
        print("Completing prior operation in progress.")

        if aset.in_process[0] == "delete":
            delete_volume(storage, aset, vid=aset.in_process[1])

        elif aset.in_process[0] == "merge":
            vname = [x.name for x in aset.vols.values() if x.vid == aset.in_process[1]][0]
            merge_sessions(aset.vols[vname], aset.in_process[4].split(":|"),
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))
    elif aset.in_process:
        raise ValueError("Bad in_process descriptor: "+repr(aset.in_process[0]))

        aset.stop()
        aset = ArchiveSet(aset.path, aset.dest, aset.opts, allvols=True, prior_auth=aset)
        if compare_files(aset, pathlist=[aset.confname], volumes=[aset.vols[vname]]):
            raise RuntimeError("Local and archive metadata differ.")


def show_list(aset, selected_vols, depth=3, json_out=None):

    options = aset.opts
    if json_out:
        try:
            print("",file=json_out); json.dump({"wversion": prog_version, "fver": aset.format_ver,
              "wdate": prog_date, "uuid": aset.uuid, "updated_at": aset.updated_at,
              "encrypted": bool(aset.mcrypto and aset.datacrypto), "is_auth": bool(aset.mcrypto),
              "url": aset.dest.spec,
              "volumes": {v.name: {"vid": v.vid, "desc": v.desc, "last": v.last,
                "ts": v.sessions[v.last].gettime() if v.sessions else None,
                "sessions": {s.name: {"volsize": s.volsize, "sequence": s.sequence,
                  "ts": s.gettime(), "chg": s.is_changed(), "tags": s.tags} if depth > 2 else ""
                  for s in v.sessions.values()} } for v in aset.vols.values() }},
              json_out)
        except BrokenPipeError:
            pass
        return

    if options.verbose:
        print("\nArchive Settings:")
        for key, val in aset.conf["var"].items():
            print(" %-15s = %s" % (key, val))

    # Print list of sessions grouped by tag. First, organize by tag:
    if options.tag:
        print("\nTag Assignments")    ; ltags = {}
        for dv in selected_vols or sorted(aset.vols.keys()):
            vol = aset.vols[dv]
            for tag, tses in vol.tags.items():
                tset = ( (dv, x[2:], vol.sessions[x].tags[tag] ) for x in tses )
                if tag not in ltags:
                    ltags[tag] = list(tset)
                else:
                    ltags[tag] += tset
        # Print result:
        for tag in sorted(ltags):
            print("\n"+tag+":")
            for ses in sorted(ltags[tag]):   print(" ", ses)
        return

    # Print list of volumes if no volume is selected.
    if not aset.vols:
        print("\nNo volumes.")    ; return
    elif not selected_vols and not len(options.volumes):
        print("\nVolumes:")
        maxwidth = max(len(x.name) for x in aset.vols.values())    ; session = options.session
        fmt    = "%7.2f GB  %s  %-" + str(maxwidth+2) + "s  %s (%3d)"
        for vname in sorted(x.name for x in aset.vols.values()
                            if not session or "S_"+session in x.sesnames):
            vol = aset.vols[vname]   ; sname = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            if options.verbose:
                print(fmt % ((vol.volsize() / 1024**3), vol.vid, vname, sname, len(vol.sessions)))
                if vol.desc:   print("  desc:", vol.desc)
            else:
                print("",vname)
        return

    # Print list of sessions grouped by volume.
    # Get the terminal column width and set cols to number of list columns.
    cols = max(4, min(10, shutil.get_terminal_size().columns // 17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        if not aset.vols[dv].sessions:   print("None.")    ; continue
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for sname in vol.sesnames + [""]:
            month = sname[2:8]    ; ses = vol.sessions[sname] if sname else None
            if options.unattended or options.verbose:
                # plain listing
                print(sname[2:], f"({ses.volsize} bytes)" if ses and options.verbose else "",
                      "Chg" if ses and ses.is_changed() else "")
                if ses and options.verbose:
                    for tag in sorted(ses.tags.items()):
                        print(" tag:", tag[0]+(", "+tag[1] if tag[1] else ""))
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(sname)
            else:
                # print the month list: 'rows' & 'c' are adjusted to reduce ragged columns.
                rows, extra = divmod(size := len(slist), c := cols)   ; rows += bool(extra)
                if extra and rows < 3 and (cols - extra) > 1:   c -= (cols - extra) // 2
                for r in range(rows):
                    print("", "  ".join(slist[x*rows+r][2:] for x in range(c) if x*rows+r < size))

                # start new month list
                print()    ; lmonth = month    ; slist = [sname]


def ts_to_datetime(ts):
    raw = datetime.datetime.fromtimestamp(ts, datetime.timezone.utc)
    return raw.astimezone().isoformat(sep=u" ")


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def get_mem_stat(field):
    with open("/proc/meminfo") as f:
        for ln in f:
            if ln.startswith(field+":"):
                return int(ln.split()[1]) # kbytes
    raise ValueError("Field not found.")


def is_num(val):
    try:
        float(val)
    except:
        return False
    else:
        return True


def os_kill(pid, sig=signal.SIGTERM):
    try:
        os.kill(pid, sig)
    except ProcessLookupError:
        pass


class NulIO(io.BytesIO):
    def __enter__(self):   self.close(); return self


def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1","USR2"],
                  sig_ign=False, ign=False, iflag=False, int_exit=False):
    # Remove existing handlers
    for sig in list(signal_handlers):   signal.signal(sig, signal_handlers.pop(sig))
    # Set handler on requested signals
    h = signal.SIG_IGN if sig_ign else functools.partial(handle_signal, int_exit=int_exit)
    for sig in (getattr(signal,"SIG"+x) for x in (sel or [])):
        signal_handlers[sig] = signal.getsignal(sig)    ; signal.signal(sig, h)
        signal.siginterrupt(sig, iflag)

    while not sel and signals_caught:
        s = signals_caught.pop(0)
        if not ign:   os.kill(os.getpid(), s)


class WTimeoutError(Exception):
    def __init__(self, *args):    super().__init__(*args); err_out("\n")
    def __del__(self):            os._exit(1)


def handle_signal(signum, frame, int_exit=False):
    if signum == signal.SIGINT and int_exit:   x_it(8, " *** Interrupt.")
    if signum == signal.SIGALRM:
        raise WTimeoutError("Timeout.\nSee send.log for details.")
    if signum not in signals_caught:   signals_caught.append(signum)
    sys.stderr.write(" *** Caught signal "+str(signum))


def ask_input(text=None):
    if text:   sys.stderr.write(text)
    return input()


def status_spinner():
    while True:
        for ii in "\\|/â€“":   yield ii


# Exit with simple message
def x_it(code, text=None):
    if text:   err_out(text)
    sys.exit(code)


def err_out(text):
    sys.stdout.write(text+"\n")


@atexit.register
def cleanup():
    if not admin_permission:   return

    try:
        if aset and not debug:
            aset.dest.remove_dtmp()
            shutil.rmtree(aset.big_tmpdir, ignore_errors=True)

        if tmpdir and exists(tmpdir):
            if purgetmp:
                shutil.rmtree(tmpdir, ignore_errors=True)
            else:
                shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
                shutil.move(tmpdir, "/tmp/"+prog_name+"-debug")

        if cachedir and meta_reduce in ("on","extra"):
            SPr.run([CP.find, cachedir, "(", "-name", "manifest", "-o", "-name", "manifest.tmp"]
                    + (["-o", "-name", "manifest.z"] if meta_reduce=="extra" else [])
                    + [")", "-cmin", "+"+str(meta_min), "-delete"])

        if error_cache:
            err_out("\nError on volumes: " + ", ".join(sorted(set(error_cache))))
            if aset and aset.opts.vols_from:
                json.dump({"rc": 2, "volumes": sorted(set(error_cache))},
                          open(aset.opts.vols_from+".error", "w"))
            os._exit(2)

    except Exception as e:
        err_out("Cleanup error: "+repr(e))




##  MAIN  #########################################################################################

# Constants / Globals
prog_name        = "wyng"
prog_version     = "0.8 beta"      ; prog_date = "20250815"
prog_info        = ("Backup logical volumes and disk images.",
                    "License: GPLv3 (see 'LICENSE' file)"
                    "\nWebsite: https://codeberg.org/tasket/wyng-backup")
release_string   = "%s %s release %s" % (prog_name.capitalize(), prog_version, prog_date)
format_version   = 3

debug = purgetmp = False
meta_reduce = meta_min = vardir = cachedir = tmpdir = None
aset = storage = storagesets = None
time_start, monotonic_start = time.time(), time.monotonic()
signal_handlers  = {}    ; signals_caught = []    ; error_cache = []
admin_permission = os.getuid() == 0


if sys.hexversion < 0x3080000:
    x_it(1, "Python ver. 3.8 or greater required.")

cpu_flags = [x for x in open("/proc/cpuinfo") if x.startswith("flags")] [0].split()[1:]
tmpdir    = tempfile.mkdtemp(prefix=prog_name, dir="/tmp")

max_address     = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split   = [len(hex(max_address))-2-7, 7]
hash_bits       = 256    ; hash_bytes   = hash_bits // 8

LC_ALL          = os.environ.get("LC_ALL")   ; os.environ["LC_ALL"] = "C"
shell_prefix    = "set -e\n"

pjoin           = os.path.join     ; exists = os.path.exists   ; zipln = itertools.zip_longest


## Parse Arguments & Etc Config ##

wyng_conf       = "/etc/wyng/wyng.ini"
local_actions   = ("monitor","version")
write_actions   = ("add","send","prune","delete","rename","arch-deduplicate","arch-init")
dest_actions    = ("list","receive","verify","diff","arch-check") + write_actions

parser_defs = [
 [["action"],         {"choices": local_actions+dest_actions, "help": "Wyng Commands"}],
 [["volumes"],        {"nargs": "*"}],
 [["--dest"],         {"default": "", "help": "URL: Location of archive"}],
 [["--local"],        {"default": "", "help": "Path or LVM vg/pool containing local volumes"}],
 [["--unattended", "-u"], {"action": "store_true", "help": "Non-interactive, supress prompts"}],
 [["--authmin"],      {"default": "5", "help": "Minutes to remember authentication (default=5)"}],
 [["--all-before"],   {"dest": "allbefore", "action": "store_true",
                       "help": "Select all sessions before --session date-time."}],
 [["--all", "-a"],    {"action": "store_true", "help": "Select all known volumes"}],
 [["--session"],      {"help": "YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS] or ^tag_id"
                             " select session date|tag, singular or range."}],
 [["--session-strict"],{"choices": ["on","off"], "default": "on",
                        "help": "Accept/reject inclusive session date"}],
 [["--dedup", "-d"],  {"action": "store_true", "help": "Data deduplication (send)"}],
 [["--volume-desc"],  {"dest": "voldesc", "default": "",
                       "help": "Set volume description (add, rename, send)"}],
 [["--tag"],          {"action": "append", "default": [],
                       "help": "tag_id[,description] Add tags when sending"}],
 [["--keep"],         {"action": "append", "default": [],
                       "help": "YYYYMMDD-HHMMSS or ^tag_id exclude session by date|tag (prune)"}],
 [["--volex"],        {"action": "append", "default": [],
                       "help": "Exclude volume"}],
 [["--autoprune"],    {"default": "off",       "help": "Automatic pruning"}],
 [["--apdays"],       {"help": "Parameters for autoprune"}],
 [["--save-to"],      {"help": "Path to store volume for receive"}],
 [["--sparse"],       {"action": "store_true", "help": "Retrieve differences only"}],
 [["--sparse-write"], {"action": "store_true", "help": "Save differences only"}],
 [["--skip-corrupt-chunks"], {"action": "store_true",
                       "help": "Continue despite corrupt chunks (receive)"}],
 [["--use-snapshot"], {"action": "store_true",
                       "help": "Use local snapshot as baseline for receive"}],
 [["--import-other-from"], {"action": "append", "default": [],
                       "help": "Import volume from non-snapshot file"}],
 [["--remap"],        {"action": "store_true", "help": "Remap volumes from other archive"}],
 [["--send-unchanged"], {"action": "store_true", "help": "Record unchanged volumes (send)"}],
 [["--vols-from","--local-from"], {"dest": "vols_from",
                       "help": "Specify multiple volume sets grouped by local storage"}],
 [["--encrypt"],      {"help": "Encryption mode"}],
 [["--compression"],  {"default": "", "help": "Init: compression type:level"}],
 [["--hashtype"],     {"default": "", "help": "Init: hash function type"}],
 [["--chunk-factor"], {"dest": "chfactor",
                       "help": "Init: set chunk size to N*64kB"}],
 [["--vid"],          {"help": "Reference a volume by ID number (delete)"}],
 [["--passcmd"],      {"help": "Command to fetch auth passphrase"}],
 [["--config"],       {"help": "Use alternate config .ini file"}],
 [["--meta-dir"],     {"dest": "metadir", "default": "", "help": "Use alternate metadata path"}],
 [["--meta-reduce"],  {"default": "on:3000", "help": "Metadata retention policy (see doc)"}],
 [["--json"],         {"action": "store_true", "help": "Output as json format (list)"}],
 [["--force"],        {"action": "store_true", "help": "Force action"}],
 [["--force-retry"],  {"action": "store_true", "help": "Retry in-process transaction again"}],
 [["--force-allow-rollback"], {"action": "store_true", "help": "Allow rollback (caution)"}],
 [["--clean"],        {"action": "store_true", "help": "Delete orphan snapshots, metadata"}],
 [["--purge"],        {"choices": ["other","full"], "default": None,
                       "help": "Delete valid snapshots"}],
 [["--maxsync"],      {"action": "store_true", "help": "Use more fs sync"}],
 [["--upgrade-format"], {"action": "store_true",
                       "help": "Upgrade archive to current spec (arch-check)"}],
 [["--tar-bypass"],   {"action": "store_true", "help": "Bypass tar for file: archives (send)"}],
 [["--opt-ssh"],      {"action": "append", "default": [], "help": "Override ssh options"}],
 [["--opt-qubes"],    {"action": "append", "default": [], "help": "Override qvm-run options"}],
 [["--change-uuid"],  {"action": "store_true", "help": "Change the archive UUID (arch-check)"}],
 [["--dry-run"],      {"action": "store_true", "help": "Dry run, do not make changes (send)"}],
 [["--purge-tmp"],    {"action": "store_true", "help": "Purge logs before exiting"}],
 [["--debug"],        {"action": "store_true", "help": "Debug mode"}],
 [["--quiet"],        {"action": "store_true"}],
 [["--verbose"],      {"action": "count", "default": 0}]
]

try:
    options   = parse_options(sys.argv[1:], parser_defs)   ; debug  = options.debug
    purgetmp  = options.purge_tmp and not debug            ; action = options.action
    verbose   = options.verbose or debug
    with open(tmpdir+"/err.log", "wt") as f:
        print("Python", sys.version, "\n", os.uname(), "\n", file=f)
        print("Options:\n", vars(options) | {"passcmd":""}, "\n", file=f)
        print(release_string, file=f)
    if debug and not options.json:
        print("Python", sys.version, "\n", os.uname())
        print("Options:\n", vars(options) | {"passcmd":""}, "\n")
        options.verbose = True; options.quiet = False
    if options.quiet and options.action not in ("list","version"):
        # Set stdout to devnull if --quiet is specified
        options.unattended = True
        sys.stdout = open(os.devnull, "w")

    print(release_string, flush=True)
    if options.action == "version":   x_it(0)

    signormal = {"sel": ["INT"], "iflag": True, "int_exit": True} if not debug else {"sel": None}
    catch_signals(**signormal)

    # Allow only one instance at a time
    lockpath = "/var/lock/"+prog_name
    try:
        os.makedirs(os.path.dirname(lockpath), exist_ok=True)
        lockf = open(lockpath, "w")
        fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
    except PermissionError:
        x_it(1, "ERROR: No writing permission on "+lockpath)
    except IOError:
        x_it(1, "I/O ERROR obtaining lock on "+lockpath)


    ## General Configuration ##

    # vardir  : holds data not directly related to an archive, such as nicknames for archive URLs.
    # cachedir: holds cached archive metadata.
    vardir      = "/var/lib/"+prog_name
    cachedir    = options.metadir or vardir
    meta_reduce, meta_min = options.meta_reduce.split(":")

    os.makedirs(vardir, exist_ok=True)
    shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)

    # Dict of compressors in the form: (library, default_compress_level, compress_func)
    compressors =    {"zlib":   (zlib, 4, zlib.compress),
                      "bz2" :   (bz2,  9, bz2.compress)}
    if zstd:   compressors["zstd"] = (zstd, 3, lambda data, lvl: zstd.compress(data, lvl, 3))

    hash_funcs  = {"hmac-sha256": None,
                   "sha256"     : lambda x: hashlib.sha256(x).digest(),
                   "blake2b"    : lambda x: hashlib.blake2b(x, digest_size=hash_bytes).digest()}

    try:
        with open(cachedir+"/UUIDs", "r") as f:  uuids = json.load(f)
    except:
        uuids = {}


    # Create ArchiveSet, LocalStorage and Destination objects with get_configs().
    # Passphrase input happens here; no stdin before this point!
    aset        = get_configs(options)
    if action in {"archi-init"}:
        x_it(0)
    uuids[aset.subdir]   = [aset.uuid, aset.dest.spec]
    storage, storagesets = get_configs_storage(aset, options, require_local=not options.save_to and
                                            action in ("monitor","send","receive","diff"))

    # Handle unfinished in_process
    if aset.in_process and aset.dest.online:
        print("** Prior operation in progress:", " ".join(aset.in_process[0:2]))

        if action in write_actions:   retry_in_process(aset)


    # Display archive update time as local time
    arch_time = ts_to_datetime(float(aset.updated_at))
    print("Encrypted" if (aset.mcrypto and aset.datacrypto) else "Un-encrypted", "archive",
          f"'{aset.dest.spec}'\nLast updated {arch_time[:-6]} ({arch_time[-6:]})", flush=True)


    ## Process Volume Selections ##

    exclude_vols  = set(options.volex or [])
    datavols      = sorted(set(aset.vols.keys()) - exclude_vols) if options.all else []
    selected_vols = sorted(set(options.volumes[:] + datavols))
    for v in selected_vols[:]:
        if v not in aset.vols and action not in {"add","delete","rename",
                                                 "send","arch-check"}:
            print(f"Volume '{v}' not found; Skipping.")
            selected_vols.remove(v)


    ## Process Commands ##

    if action   == "monitor":
        for storage, vols in storagesets.items():
            monitor_send(storage, aset, vols or selected_vols, monitor_only=True)
        print()


    elif action == "send":
        sid, ses_tags = None, {}    ; stvols = [v for x in storagesets.values() for v in x]
        other_vols = {x: os.path.abspath(y)
                    for x,y in map(lambda a: a.split(":|:",maxsplit=1), options.import_other_from)}
        vname_sz   = max(map(len, list(other_vols) + selected_vols + stvols), default=30)

        for storage, vols in storagesets.items():
            sid, ses_tags = monitor_send(storage, aset, vols or selected_vols, monitor_only=False,
                                        use_sesid=sid, ses_tags=ses_tags, imports=other_vols,
                                        benchmark=options.dry_run, vname_sz=vname_sz)
            selected_vols.clear()   ; other_vols.clear()

        if other_vols:   error_cache.extend(list(other_vols.keys()))


    elif action == "prune":
        ap = options.autoprune.lower()   ; dvs = selected_vols
        if ap == "off" and not options.session:
            x_it(1, "Specify --autoprune or --session for prune.")


        if not options.unattended and len(dvs):
            print("This operation will delete session(s) from the archive;")
            ans = ask_input("Are you sure? [y/N]: ").strip()
            if ans.lower() not in {"y","yes"}:
                x_it(0)

        print()
        mdvols  = [x for x in dvs if aset.vols[x].is_mdvol()]
        allvols = options.all and (set(dvs) == set(aset.vols))
        for dv in [x for x in dvs if x not in mdvols] + mdvols:
            print(dv, end=" -", flush=True)   ; n, status = 0, ""
            if options.session:
                n, status = prune_sessions(aset.vols[dv], options.session.split(",", maxsplit=1),
                                           allvols=allvols)
            elif ap in ("on","full"):
                n, status = autoprune(aset.vols[dv], apmode=ap, allvols=allvols)
            print(n, status if verbose else "")


    elif action == "receive":

        if len(selected_vols) != 1 and options.save_to:
            x_it(1, "Specify one volume for receive --save-to.")
        if options.session and len(options.session.split(",")) > 1:
            x_it(1, "Specify only one session for receive.")

        if not options.unattended and not options.force:
            print("\nWarning:  Receiving to existing volumes will overwrite them!")
            ans = ask_input("Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:   x_it(1)

        for storage, vols in storagesets.items():
            dvlist = vols or selected_vols
            if not storage.online and vols:
                err_out("\n Offline volumes: " + ", ".join(vols))
                error_cache.extend(vols)   ; continue

            for dv in dvlist:
                count = receive_volume(storage, aset.vols[dv], select_ses=options.session or "",
                                    ses_strict=options.session_strict=="on",
                                    save_path=options.save_to or "")
                if count is None:   error_cache.append(dv)


    elif action == "verify":
        for storage, vols in storagesets.items():
            dvlist = vols or selected_vols
            for dv in dvlist:
                count = receive_volume(storage, aset.vols[dv],
                            select_ses="" if not options.session
                                          else options.session.split(",")[0],
                            ses_strict=options.session_strict=="on",
                            verify_only=1, save_path="")
                if count is None:   error_cache.append(dv)


    elif action == "diff":
        for storage, vols in storagesets.items():
            dvlist = vols or selected_vols
            if not storage.online and vols:
                err_out("\n Offline volumes: " + ", ".join(vols))
                error_cache.extend(vols)   ; continue

            for dv in dvlist:
                count = receive_volume(storage, aset.vols[dv], save_path="", diff=True)
                if count is None:   error_cache.append(dv)


    elif action == "list":
        show_list(aset, selected_vols, json_out=sys.stdout if options.json else None)


    elif action == "add":
        if len(options.volumes) < 1:
            x_it(1, "Volume name(s) required for add.")

        for dv in options.volumes:
            if dv not in aset.vols:   add_volume(aset, dv, options.voldesc)


    elif action == "rename":
        if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
        rename_volume(storage, aset, options.volumes[0], options.volumes[1])


    elif action == "delete":
        if options.clean or options.purge:
            if options.volumes:
                x_it(1, "Cannot delete volumes with clean/purge.")
            elif not storage or not storage.online:
                x_it(1, "Local storage required for clean/purge.")

            print(f"Remove local Wyng snapshots from path '{storage.path}'.")
            if not options.unattended and not options.force:
                ans = ask_input("Are you sure? [y/N]: ")
                if ans.lower() not in {"y","yes"}:
                    x_it(0)
            elif not options.force:
                x_it(1, "Ignoring clean/purge without --force.")

            if options.all:   meta_reduce, meta_min = "extra", 0
            remove_local_snapshots(storage, None if options.all else aset, purge=options.purge)

        elif len(options.volumes) + bool(options.vid) != 1:
            x_it(1, "Specify one volume to delete.")

        else:
            delete_volume(storage, aset, **{"vid": options.vid} if options.vid
                                    else { "dv": options.volumes[0]})


    elif action == "arch-init":
        pass


    elif action == "arch-check":
        arch_check(storage, aset, selected_vols)


    elif action == "arch-deduplicate":
        if not aset.dest.hlinks:
            x_it(1, "Hardlinks not supported on dest.")
        dedup_existing(aset)


    # Save overview to cache
    if not exists(f:=cachedir+"/cached") or action in write_actions + ("arch-check",):
        json.dump({x:y for x,y in uuids.items()
                       if exists(f"{cachedir}/{x}/archive.ini")}, open(f,"w"))

        if aset:
            show_list(aset, None, depth=2, json_out=gzip.open(f:=aset.path+"/archive.cached","wt"))
            ts = os.stat(aset.confpath).st_mtime_ns    ; os.utime(f, ns=(ts,ts))


except (InputError, SelectionError, OptionError, AuthenticationError) as e:
    #if debug:   raise e
    x_it(3 if type(e) is AuthenticationError else 9,
         f"\n{e.__class__.__name__}: {e}\n")

except StorageError as e:
    x_it(e.args[0], f"\n{e.__class__.__name__}: {e.args[-1]}\n")

except Exception as e:
    print()
    if e is BrokenPipeError:   e.add_note("See Wyng log for details.")
    raise e

## END ##
