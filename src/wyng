#!/usr/bin/env python3
# editor width: 100   -----------------------------------------------------------------------------


###  Wyng â€“ The Logical Volume Backup Tool
###  Copyright Christopher Laprise 2018-2023 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.
###  Permission to redistribute under similar naming "Wyng", "wyng backup", etc.
###  is granted only for un-modified code (GPLv3 S.7-c).


# Return codes:
# 1 - Fatal error
# 2 - Error on specific volume(s)
# 3 - Auth error
# 4 - Timeout
# 5 - Destination not ready
# 6 - Archive not found
# 7 - Local storage not ready


import sys, signal, os, stat, shutil, subprocess as SPr, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, hashlib, hmac, functools, uuid
import getpass, base64, platform, resource, itertools, string, struct
import xml.etree.ElementTree, ctypes, ctypes.util, atexit
from array import array         ; from urllib.parse import urlparse

try:
    import zstd, warnings
    # Required for crippled zstd library
    warnings.filterwarnings("ignore", category=DeprecationWarning)
except:
    zstd = None

try:
    from Cryptodome.Cipher import ChaCha20 as Cipher_ChaCha20
    from Cryptodome.Cipher import ChaCha20_Poly1305 as Cipher_ChaCha20_Poly1305
    from Cryptodome.Random import get_random_bytes
    import Cryptodome, Cryptodome.Protocol.KDF, Cryptodome.Hash.SHA512
except:
    Cipher_ChaCha20 = Cipher_ChaCha20_Poly1305 = None


# ArchiveSet manages archive metadata incl. volumes and sessions

class ArchiveSet:

    confname      = "archive.ini"   ; max_volumes  = 4096
    mhash_sz      = 44              ; max_sessions = 16384          ; comment_len = 100
    sesname_sz    = 17              ; tag_len      = 25             ; max_tags = 5
    min_chunksize = 0x10000         ; volname_len  = 4000           ; vid_len  = 10

    max_conf_sz = (volname_len + comment_len + mhash_sz + 25) * max_volumes + 1000
    max_infosz  = (tag_len+comment_len + 10) * max_tags + 1000
    max_volinfosz= (mhash_sz + sesname_sz + 10) * max_sessions + 1000

    attr_ints   = {"format_ver","chunksize","mci_count","dataci_count"}

    def __init__(self, top, dest, ext="", allvols=False, children=2,
                 pass_agent=0, passphrase=None, prior_auth=None):
        self.time_start      = time_start
        self.monotonic_start = monotonic_start
        self.dest        = dest
        self.path        = top
        self.confpath    = pjoin(self.path, self.confname)
        self.confprefix  = b"[WYNG%02d]\n" % format_version
        self.modeprefix  = b"ci = "
        self.mcrypto     = mcrypto = None
        self.datacrypto  = datacrypto = None
        self.vols        = {}
        self.dedupindex  = {}
        self.dedupsessions = []
        self.raw_hashval = None
        self.just_fetched= False
        self.Volume      = ArchiveVolume

        self.big_tmpdir  = self.path+"/.tmp"
        self.subdir      = None

        # persisted:
        self.format_ver  = 0
        self.chunksize   = self.min_chunksize * 2
        self.compression = "zstd" if zstd else "zlib"
        self.compr_level = str(compressors[self.compression][1])
        self.hashtype    = "hmac-sha256"
        self.vgname      = None
        self.poolname    = None
        self.uuid        = None
        self.updated_at  = None
        self.data_cipher = self.ci_mode = None
        self.mci_count   = self.dataci_count  =  0
        self.in_process  = []

        shutil.rmtree(self.big_tmpdir, ignore_errors=True)
        os.makedirs(self.big_tmpdir, exist_ok=True)

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"], cp["volumes"], cp["in_process"]  =  {},{},{}

        # halt configuration if this is a new or temp config
        os.makedirs(self.path, exist_ok=True)
        if not exists(self.confpath+ext):
            self.uuid = str(uuid.uuid4())
            return

        # decode archive.ini file
        # Format "magic" is "[WYNGvv]\n" where vv = format version: 9 bytes
        # Format mode is next as "ci = m0\n" where m = cipher mode: 8 bytes
        with open(self.confpath+ext, "rb") as f:
            self.header = (header0 := f.read(9)) + (header1 := f.read(8))
            buf = f.read(self.max_conf_sz)
        self.raw_hashval = hashlib.sha256(header0 + header1 + buf).hexdigest()
        if not (header0 == self.confprefix and header1.startswith(self.modeprefix)):
            if not header0.startswith(b"[var]\n"):
                raise ValueError("Not a Wyng format.")
            else:
                old_v2_format = True  ; raise NotImplementedError("Old format conversion")
        else:
            old_v2_format = False    #; untrusted_ver = 2

        # use existing auth if specified
        if isinstance(prior_auth, ArchiveSet):
            self.mcrypto    = mcrypto = prior_auth.mcrypto    ; self.ci_mode = prior_auth.ci_mode
            self.datacrypto = datacrypto = prior_auth.datacrypto
            self.data_cipher = prior_auth.data_cipher
        else:
            self.ci_mode = header1[-3:-1]

        # parse metadata crypto mode and instantiate it
        if self.ci_mode != b"00" and not mcrypto:
            ci_types = DataCryptography.crypto_codes[self.ci_mode]
            if debug:   print("metadata cipher =", ci_types[1])

            # access a key agent, if available for this UUID and archive URL
            agent_name = "wyng-agent-" + str(os.getuid()) + hashlib.blake2b(
                           bytes(self.dest.spec, encoding="UTF-8"), digest_size=20).hexdigest()
            agentkeys  = agent_get(agent_name, pass_agent)

            # get passphrase if we got no agentkeys
            if not passphrase and not agentkeys:   passphrase = ask_passphrase()
            if passphrase:
                assert passphrase != bytes(len(passphrase))
            elif not agentkeys:
                raise ValueError("No authentication input.")
            passphrase_s1 = passphrase[:] if passphrase else None

            # create both crypto instances and load metadata key
            self.datacrypto = datacrypto = DataCryptography()
            self.mcrypto    = mcrypto    = DataCryptography()
            mcrypto.load(ci_types[1], self.confpath+".salt", slot=1,
                         passphrase=passphrase_s1, agentkeys=agentkeys)

        # initial decryption + auth of archive.ini (root body)
        try:
            if mcrypto:   buf = mcrypto.decrypt(buf)
        except ValueError as e:
            if "MAC check failed" in e.args:
                x_it(3, "Error: Could not decrypt/authenticate archive.\n")
            raise e

        # read attributes from archive.ini
        if debug:   print(gzip.decompress(buf).decode("UTF-8"))
        cp.read_string(gzip.decompress(buf).decode("UTF-8"))
        for key, value in cp["var"].items():
            setattr(self, key, int(value) if key in self.attr_ints else value)
        self.in_process  = [ ln for ln in cp["in_process"].values() ]

        if self.format_ver > format_version or not self.format_ver:
            x_it(1,"Archive format ver = "+str(self.format_ver)+
                                ". Expected = "+str(format_version))

        # use blake2b for metadata if hmac-sha256 is used for data:
        self.gethash     = hash_funcs["blake2b"] if self.hashtype == "hmac-sha256" \
                           else hash_funcs[self.hashtype]
        self.compress    = compressors[self.compression][2]
        self.decompress  = compressors[self.compression][0].decompress
        self.subdir      = "/a_"+hashlib.blake2b(
                           bytes(str(os.getuid())+self.uuid+self.dest.spec, encoding="UTF-8"),
                           digest_size=20).hexdigest()

        # init data crypto
        if mcrypto and datacrypto.key is None:
            if not float(self.updated_at) < self.time_start:
                raise ValueError("Current time is less than archive timestamp!")

            cadence = 20 + (self.data_cipher.endswith(("-t1","-t2","-t3","-t4")) * 100)
            datacrypto.load(self.data_cipher, mcrypto.keyfile, slot=0, cadence=cadence,
                            passphrase=passphrase, agentkeys=agentkeys)

            self.mci_count    = mcrypto.set_counter(self.mci_count)
            self.dataci_count = datacrypto.set_counter(self.dataci_count)

            # datacrypto may not have AE, so test data chunk against a volinfo hash
            for vol, vf in ((x, x.path+"/vi.dat") for x in self.vols.values()):
                if exists(vf):
                    if not hmac.compare_digest(vol.hashval, self.b64hash(
                        datacrypto.decrypt(open(vf,"rb").read()))[0]):
                        x_it(3, "Error: Data hash check failed.")
                    break

            # persist keys to allow future invocations w/o passphrase input
            if pass_agent > 0 and not agentkeys:
                agent_make(agent_name, pass_agent, [datacrypto.key, mcrypto.key])

        # Enh: use 'hashtype' value for non-test modes ###
        if datacrypto and datacrypto.mhashkey:
            self.getdatahash = datacrypto.getmhash_hmac
        else:
            self.getdatahash = hash_funcs[self.hashtype]

        # load volume metadata objects
        if children:
            self.load_volumes(1)

            if children > 1:
                ses_list =[(x.volume.vid+"/"+x.name, x.name, x.volume.name) for sub in
                        (vol.sessions.values() for vol in self.vols.values()) for x in sub]
                fetch_file_blobs([(x+"/info", self.max_infosz) for x,_,_ in ses_list],
                                self.path, self.dest, skip_exists=True)

                self.load_volumes(children) # Enh: don't re-load vols
                fetched = fetch_file_blobs([(sp+"/manifest.z",
                                             self.vols[dv].sessions[s].manifest_max())
                                                for sp,s,dv in ses_list],
                                self.path, self.dest, verifier=self.mcrypto, skip_exists=True)
                self.just_fetched = bool(fetched)

    def load_volumes(self, children):
        recv_list = [(x+"/volinfo", self.max_volinfosz)
                       for x in self.conf["volumes"].keys()]
        fetch_file_blobs(recv_list, self.path, self.dest, skip_exists=True)
        do_exec([[CP.chattr, "+c"] + list(self.conf["volumes"].keys())],
                cwd=self.path, check=False)

        for key, value in self.conf["volumes"].items():
            # conditions when a volume must be loaded:
            # - allvols flag is set
            # - in_process flag from an unfinished action
            # - volume specified on command line
            # - no volumes specified (hence all)
            # - deduplication is in effect
            #if children: ## and (allvols or self.in_process or options.from_arch or \
            ##(len(options.volumes)==0 or key in options.volumes or options.dedup)):
                # instantiate:
            vid = key; hashval = value; vname = None

            #loadses = allvols or self.in_process or options.from_arch or options.dedup or
            vol  = self.Volume(self, vid, hashval, pjoin(self.path,vid),
                                    self.vgname, name=vname, children=children)
            self.vols[vol.name] = vol


    def save_conf(self, ext=""):
        c = self.conf['var']    ; c.clear()    ; mcrypto = self.mcrypto
        c['uuid']        = self.uuid or str(uuid.uuid4())
        c['updated_at']  = self.updated_at = \
                            str(self.time_start - self.monotonic_start + time.monotonic())
        c['format_ver']  = str(format_version)
        c['chunksize']   = str(self.chunksize)
        c['compression'] = self.compression
        c['compr_level'] = self.compr_level
        c['hashtype']    = self.hashtype
        c['vgname']      = self.vgname
        c['poolname']    = self.poolname
        c['data_cipher'] = self.data_cipher
        if mcrypto:
            if self.datacrypto.counter > self.datacrypto.ctstart:
                self.dataci_count = self.datacrypto.counter
            self.mci_count = mcrypto.counter
            c['dataci_count']= str(self.dataci_count)
            c['mci_count']   = str(self.mci_count)
        self.conf['in_process'] = { x: ln if type(ln) is str else ":|".join(ln)
                                          for x,ln in enumerate(self.in_process) }

        os.makedirs(self.path, exist_ok=True)    ; etag = b''
        with io.StringIO() as fs:
            self.conf.write(fs)    ; fs.flush()
            buf = gzip.compress(fs.getvalue().encode("UTF-8"), 4)
        if mcrypto:   etag, buf = mcrypto.encrypt(buf)
        with open(self.confpath+ext, "wb") as f:
            f.write(b''.join((self.confprefix, self.modeprefix, self.ci_mode, b"\n")))
            f.write(etag)    ; f.write(buf)

    def rename_saved(self, ext=".tmp"):
        os.replace(self.confpath+ext, self.confpath)

    # Set or clear state for the archive as 'in_process' in case of interruption during write.
    # Format is list containing strings or list of strings. For latter, ':|' is the delimiter.
    def set_in_process(self, outer_list, tmp=False, save=True, todest=True):
        fssync(aset.path)
        self.in_process = [] if outer_list is None else outer_list
        if not save:   return
        self.save_conf(".tmp" if tmp else "")

        #if not tmp:   os.replace(self.confpath+".tmp", self.confpath)
        if todest:      update_dest(aset, pathlist=[self.confname], ext=".tmp" if tmp else "")

    def add_volume_meta(self, datavol, desc="", ext=""):
        errs = []
        if len(self.conf["volumes"]) >= self.max_volumes:   x_it(1, "Too many volumes")
        if datavol in self.vols:
            print(datavol+" is already configured.")    ; return None

        namecheck = self.Volume.volname_check(datavol)
        if namecheck:
            errs.append(namecheck+"\n")
        if len(desc.encode("UTF-8")) > self.comment_len:
            errs.append("Error: Max "+self.comment_len+" size for volume desc.\n")
        if not desc.isprintable():
            errs.append("Error: [^control] not allowed in volume desc.\n")
        if errs:
            sys.stderr.write("".join(errs))    ; error_cache.append(datavol)    ; return None

        while (vid := "Vol_"+os.urandom(3).hex()) in self.conf["volumes"]:   pass

        self.vols[datavol] = vol = self.Volume(self, vid, "0", pjoin(self.path,vid),
                                               self.vgname, name=datavol, children=0)
        vol.save_volinfo(ext)
        return vol

    def delete_volume_meta(self, datavol):
        # Enh: add delete-by-vid
        vid = self.vols[datavol].vid    ; vpath = self.vols[datavol].path
        del(self.vols[datavol], self.conf["volumes"][vid])
        self.save_conf()

        if exists(vpath):    shutil.rmtree(vpath)
        return vid

    def rename_volume_meta(self, datavol, newname, ext=""):
        vol = self.vols[datavol]
        if newname in self.vols or self.Volume.volname_check(newname):   return False

        vol.name = newname
        vol.save_volinfo(ext)
        return True

    def b64hash(self, buf):
        return base64.urlsafe_b64encode(self.gethash(buf)).decode("ascii")

    def encode_file(self, fname, fdest=None, get_digest=True, compress=True):
        # Enh: optimize memory use
        mcrypto = self.mcrypto    ; digest = None    ; etag = b''
        destname= fdest or fname+(".z" if compress else "")

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            inbuf = bytes(inmap) if self.compression == "zstd" else inmap
            mbuf  = self.compress(inbuf, int(self.compr_level)) if compress else inbuf
            if get_digest:   digest = self.b64hash(mbuf)
            if mcrypto:      etag, mbuf = mcrypto.encrypt(mbuf)
            with open(destname,"wb") as f:   f.write(etag); f.write(mbuf)

        return digest

    def decode_file(self, fname, fdest=None, digest=None, max_sz=16000000):
        # Enh: optimize memory use
        destname= fdest or (fname[:-2] if fname.endswith(".z") else fname)
        mcrypto = self.mcrypto         ; buf_start = mcrypto.buf_start if mcrypto else 0

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            assert buf_start < len(inmap) <= max_sz ## Fix: move to get_configs_remote()
            mbuf = mcrypto.decrypt(inmap) if mcrypto else bytes(inmap)
            assert hmac.compare_digest(digest, self.b64hash(mbuf))
            open(destname,"wb").write(self.decompress(mbuf))

    def set_local(self, localpath):
        parts = localpath.split("/")
        if len(parts) == 2 and all(parts) and not localpath.startswith("/"):
            self.vgname, self.poolname = parts
        else:
            self.vgname, self.poolname = "", localpath


class ArchiveVolume:

    __slots__ = ("vid","name","hashval","archive","path","vgname","sessions","sesnames",
                 "_seslist","last","meta_checked","tags","desc","changed_bytes")

    def __init__(self, archive, vid, hashval, path, vgname, name=None, children=2):
        self.vid       = vid                       ; self.tags    = {}
        self.archive   = archive                   ; self.path    = path
        self.vgname    = vgname                    ; self.hashval = hashval
        self.last      = "None"                    ; self.meta_checked = False
        # persisted here:
        self.name      = name                      ; self.desc      = ""
        self.sessions  = {}                        ; self.sesnames= []
        #self.fshint    = None
        # other:
        self.changed_bytes = 0

        Ses = ArchiveSession
        if debug:  print("\n", vid, "-", name)
        if path and not exists(path):   os.makedirs(path)
        if path and hashval != "0":
            with open(pjoin(path,"volinfo"), "rb") as f:
                fsize = os.fstat(f.fileno()).st_size
                if fsize > ArchiveSet.max_volinfosz:   raise ValueError("volinfo too large")
                buf = f.read(fsize)
            if archive.mcrypto:   buf = archive.mcrypto.decrypt(buf)
            if not hmac.compare_digest(hashval, archive.b64hash(buf)):
                raise ValueError("Volume %s hash %s, expected %s" \
                                 % (vid, archive.b64hash(buf), hashval))
            if debug:   print(archive.decompress(buf).decode("UTF-8"))
            with io.StringIO(archive.decompress(buf).decode("UTF-8")) as f:
                for ln in f:
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if vname.startswith("S_"):
                        self.sessions[vname] = Ses(self, vname, value,
                                                   path+"/"+vname if children > 1 else "")
                    else:
                        setattr(self, vname, value)
        if not self.name: raise ValueError("Vol name missing")

        # session name list sorted by sequence field
        self._seslist = seslist = list(self.sessions.values()) if children > 1 else []
        seslist.sort(key=lambda x: x.sequence)
        self.sesnames = sesnames = [y.name for y in seslist]
        if sesnames:   self.last = sesnames[-1]

        if exists(pjoin(path,"volchanged")):
            self.changed_bytes = int(open(pjoin(path,"volchanged"),"r").readlines()[0].strip())


    def save_volinfo(self, ext=""):
        os.makedirs(self.path, exist_ok=True)   ; fname = "volinfo"   ; etag = b''
        with io.StringIO() as f:
            print("name =", self.name, file=f)
            print("desc =", self.desc, file=f)
            for ses in self.sessions.values():
                if ses.saved:   print(ses.name, "=", ses.hashval, file=f)

            buf = self.archive.compress(f.getvalue().encode("UTF-8"),
                                        int(self.archive.compr_level))
            self.archive.conf["volumes"][self.vid] = self.archive.b64hash(buf)

        if self.archive.mcrypto:
            # First encrypt as data, for data cipher verification at startup
            with open(pjoin(self.path,"vi.dat"+ext), "wb") as df:
                df.write(b''.join(self.archive.datacrypto.encrypt(buf,
                                    self.archive.gethash(buf))))
                self.archive.datacrypto.save_counter()
            # encrypt as metadata
            etag, buf = self.archive.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
            f.flush()      ; os.fsync(f.fileno())
        self.archive.save_conf(ext)

    def rename_saved(self, ext=".tmp"):
        assert ext and exists(pjoin(self.path,"volinfo")+ext)
        for rpath in (pjoin(self.path,"volinfo"), pjoin(self.path,"vi.dat")):
            if exists(rpath+ext):   os.replace(rpath+ext, rpath)
        self.archive.rename_saved(ext)

    def volname_check(vname): # Fix: move to LocalStorage class
        if not 0 < len(vname) <= ArchiveSet.volname_len:
            return f"Volume path/name length must be 1 - {ArchiveSet.volname_len}."
        if not vname.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in vname.strip().split("/")]):
            return "Bad volume name."
        return ""

    def volsize(self):
        return self.sessions[self.last].volsize if self.sessions else 0

    def last_chunk_addr(self, vsize=None):
        if vsize is None:  vsize = self.volsize()
        return (vsize-1) - ((vsize-1) % self.archive.chunksize)

    def mapfile(self, pos=None):
        if not self.sessions:   return None
        s = self.sessions[self.sesnames[pos] if pos else self.last]
        return f"{self.path}/{s.name}_{s.sequence}.deltamap"

    # Based on last session size unless volume_size is specified.
    def mapsize(self, volume_size=None):
        vs = volume_size or self.volsize()
        return (vs // self.archive.chunksize // 8) + 1

    def map_used(self, ext=""):
        return os.stat(self.mapfile()+ext).st_blocks if exists(self.mapfile()+ext) else 0

    def changed_bytes_add(self, amount, reset=False, save=False):
        if reset:
            if exists(self.path+"/volchanged"):   os.remove(self.path+"/volchanged")
            self.changed_bytes = 0  ; return

        self.changed_bytes += amount
        if save:
            with open(self.path+"/volchanged", "w") as f:
                print(self.changed_bytes, file=f)
                f.flush()    ; os.fsync(f.fileno())

    def init_deltamap(self, timestamp=None):
        self.changed_bytes_add(0, reset=True)    ; bmfile = self.mapfile()
        if exists(bmfile):
            os.remove(bmfile)
        with open(bmfile, "wb") as bmapf:
            bmapf.truncate(self.mapsize())    ; bmapf.flush()
        if timestamp:   os.utime(bmfile, ns=(timestamp,)*2)

    def new_session(self, sname, addtags={}):
        ns = ArchiveSession(self, sname, "0", addtags=addtags)
        ns.path = pjoin(self.path, sname)
        ns.sequence = self.sessions[self.last].sequence + 1 if self.sessions else 0
        ns.previous = self.last

        self.last = sname
        self.sesnames.append(sname)
        self.sessions[sname] = ns
        if self.archive.dedupindex:    self.archive.dedupsessions.append(ns)
        return ns

    def delete_session(self, sname, remove=True, force=False):
        ses     = self.sessions[sname]
        index   = self.sesnames.index(sname)    ; affected = None
        if sname == self.last and ses.saved and not force:
            raise ValueError("Cannot delete last session")

        for tag in list(ses.tags.keys()):   self.sessions[sname].tag_del(tag)
        del(self.sesnames[index], self.sessions[sname])

        if self.archive.dedupsessions:
            indexdd = self.archive.dedupsessions.index(ses)
            self.archive.dedupsessions[indexdd] = None

        # Following condition means:
        #   * sesnames cannot be empty
        #   * ses wasn't deleted from end of list

        if len(self.sesnames) > index:
            affected = self.sesnames[index]
            self.sessions[affected].previous = ses.previous

        self.last  = self.sesnames[-1] if len(self.sesnames) else "None"

        if remove and exists(pjoin(self.path, sname)):   shutil.rmtree(ses.path)
        return affected

    def decode_one_manifest(self, ses, force=False):
        if not exists(ses.path+"/manifest") or force:
            self.archive.decode_file(ses.path+"/manifest.z",
                                        digest=ses.manifesthash, max_sz=ses.manifest_max())

    def decode_manifests(self, sesnames, force=False):
        for ses in (self.sessions[x] for x in sesnames):
            if ses.path and force:
                os.makedirs(ses.path, exist_ok=True)
                do_exec([[CP.chattr, "+c", ses.path]], check=False)
            self.decode_one_manifest(ses, force=force)


class ArchiveSession:

    attr_str  = ("localtime","previous","permissions","manifesthash")
    attr_int  = ("volsize","sequence")
    attr_misc = ("tags","volume","archive","name","path","saved","loaded","toggle",
                 "hashval","meta_checked")
    __slots__ = attr_str + attr_int + attr_misc

    def __init__(self, volume, name, hashval, path="", addtags={}):
        self.volume   = volume;    self.archive = arch = volume.archive
        self.name     = name
        self.path     = path
        self.saved    = self.loaded = False
        self.toggle   = True
        self.hashval  = hashval
        self.meta_checked = False
        # persisted:
        self.localtime= 0
        self.volsize  = None
        self.sequence = None
        self.previous = "None"
        self.tags     = {}
        self.permissions  = ""
        self.manifesthash = None

        if path and hashval != "0":
            if debug:   print(name, ":", path)
            with open(pjoin(path,"info"), "rb") as sf:
                fsize = os.fstat(sf.fileno()).st_size
                if fsize > ArchiveSet.max_infosz:   raise ValueError("info too large")
                buf = sf.read(fsize)
            if arch.mcrypto:   buf = arch.mcrypto.decrypt(buf)
            if not hmac.compare_digest(hashval, arch.b64hash(buf)):
                raise ValueError("Session %s hash %s, expected %s" \
                                 % (name, arch.b64hash(buf), hashval))
            with io.StringIO(arch.decompress(buf).decode("UTF-8")) as sf:
                for ln in sf:
                    vname, value = map(str.strip, ln.split("=", maxsplit=1))
                    #vname = vname.strip()    ; value = value.strip()
                    if vname == "uuid":  continue
                    if value == "none":   value = "None"
                    if vname == "tag":
                        self.tag_add(ArchiveSession.tag_parse(value))
                        continue

                    setattr(self, vname, 
                        int(value) if vname in self.attr_int else value)

            self.saved = self.loaded = True

        for tag in addtags:   self.tag_add(tag)


    def manifest_max(self):
        return self.volsize // self.archive.chunksize * ((hash_bits//4) + 20)

    def tag_parse(tag, delim=" "):
        result = tuple()    ; errs = []      ; parts   = tag.strip().split(delim, maxsplit=1)
        tag_id = parts[0].strip().lower()    ; comment = parts[1].strip() if len(parts)>1 else ""

        if len(tag_id.encode("UTF-8")) > ArchiveSet.tag_len:
            errs.append("Error: Max "+ArchiveSet.tag_len+" size for tag ID.\n")
        if len(comment.encode("UTF-8")) > ArchiveSet.comment_len:
            errs.append("Error: Max "+ArchiveSet.comment_len+" size for comment.\n")
        if re.match(".*[,=\^]", tag_id) or any(map(str.isspace, tag_id)) \
        or not tag_id.isprintable():
            errs.append("Error: [^control], [space], and ',^=' not allowed in tag ID.\n")
        if not comment.isprintable():
            errs.append("Error: [^control] not allowed in tag comment.\n")
        if tag_id == "all":   errs.append("Error: tag 'all' is reserved.\n")
        sys.stderr.write("".join(errs))

        if not errs:
            result = (tag_id, "" if len(parts) == 1
                                    else parts[1].strip()[:ArchiveSet.comment_len])
        return result

    def tag_add(self, tag):
        if len(self.tags) >= ArchiveSet.max_tags:
            x_it(1, ArchiveSet.max_tags+" maximum tags.")
        self.saved = False    ; voltags = self.volume.tags    ; tid = tag[0]
        if tid not in self.tags:   self.tags[tid] = tag[1]
        if tid in voltags:
            voltags[tid].add(self.name)
        else:
            voltags[tid] = {self.name}
        return True

    def tag_del(self, tag):
        del(self.tags[tag])   ; voltags = self.volume.tags
        if tag in voltags:
            if self.name in voltags[tag]:   voltags[tag].remove(self.name)
            if len(voltags[tag]) == 0:    del(voltags[tag])

    def gettime(self):
        if is_num(self.localtime):
            return int(self.localtime)
        else:
            return int(time.mktime(time.strptime(self.localtime, "%Y%m%d-%H%M%S"))*1000000000)

    def save_info(self, ext=""):
        assert self.path   ; etag = b''   ; fname = "info"   ; arch = self.volume.archive
        self.manifesthash = arch.encode_file(self.path+"/manifest"+ext,
                                             fdest=self.path+"/manifest.z"+ext)
        with io.StringIO() as f:
            for attr in self.attr_str+self.attr_int:
                print(attr, "=", getattr(self, attr), file=f)
            for tkey, tdesc in self.tags.items():
                print("tag =",   tkey, tdesc, file=f)
            buf = arch.compress(f.getvalue().encode("UTF-8"),
                                int(arch.compr_level))
            self.hashval = arch.b64hash(buf)

        if arch.mcrypto:   etag, buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
            f.flush()      ; os.fsync(f.fileno())
        self.saved = self.loaded = True
        self.volume.save_volinfo(ext)

    def rename_saved(self, ext=".tmp"):
        assert ext and exists(pjoin(self.path,"info")+ext)
        for rpath in (pjoin(self.path,x) for x in ("info", "manifest", "manifest.z")):
            if exists(rpath+ext):   os.replace(rpath+ext, rpath)
        self.volume.rename_saved(ext)

# END class ArchiveSet, ArchiveVolume, ArchiveSession


def agent_helper_write(path):
    agent_program = r'''#  Copyright Christopher Laprise 2018-2023
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, signal

def sighandler(s,f):
    if s == ALRM:   raise TimeoutError("SIGALRM")

def do_mkpipe():
    if exists("/tmp/"+agent_name):   os.remove("/tmp/"+agent_name)
    os.mkfifo("/tmp/"+agent_name, mode=0o600)

class KeyHandler:
    def __init__(self):
        self.keys = []

    def __del__(self):
        for key in self.keys:
            for ii in range(len(key)):   key[ii] = 0
        os.remove("/tmp/"+agent_name)

## MAIN ##
cmd    = sys.argv[1]      ; agent_name = sys.argv[2]     ; inread = sys.stdin.buffer.read
KH     = KeyHandler()     ; magic  = b"\xff\x11\x15"     ; exists = os.path.exists
SIGINT = signal.SIGINT    ; USR1   = signal.SIGUSR1      ; ALRM   = signal.SIGALRM
for s in (SIGINT,USR1,ALRM):   signal.signal(s, sighandler)   ; signal.siginterrupt(s, True)

signal.alarm(10)    ; do_mkpipe()    ; duration = min(inread(1)[0], 60) * 60
for slot in range(2):
    KH.keys.append(bytearray(inread(inread(1)[0])))   ; assert inread(3) == magic

while True:
    try:
        signal.alarm(0)   ; res = signal.sigtimedwait({USR1}, duration)   ; signal.alarm(10)
        if res is None:   break
        with open("/tmp/"+agent_name,"wb") as npipe:
            for key in KH.keys:
                npipe.write(magic + len(key).to_bytes(1,"big"))   ; npipe.write(key)
    except TimeoutError:
        print("timeout")    ; do_mkpipe()    ; continue
'''
    with open(path+"/agent_helper.py", "wb") as progf:
        progf.write(bytes(agent_program, encoding="UTF-8"))


def agent_get(agname, duration):
    ps = SPr.check_output([CP.ps, "-u"+str(os.getuid()), "-o", "pid,command"], text=True)
    findp  = [x for x in ps.splitlines() if (agname in x and "agent_helper.py store" in x)]
    if not findp:   return None
    pid    = int(findp[0].split()[0])   ;  os_kill(pid, signal.SIGUSR1)
    result = []   ; catch_signals(["ALRM"], iflag=True) ;   signal.alarm(10)
    try:
        with open("/tmp/"+agname, "rb") as npipe:
            for slot in range(2):
                if m := npipe.read(3) != Destination.magic:
                    raise ValueError("*Magic not found, got "+repr(m))
                result.append(key := bytearray(npipe.read(ksz := npipe.read(1)[0])))
                if len(key) != ksz:   raise ValueError("Key length")
    except (TimeoutError, ValueError) as e:
        for k in result:   clear_array(k)
        result = None    ; os_kill(pid)
    finally:
        if duration == -1:   os_kill(pid)
        signal.alarm(0)  ; catch_signals(None)
        return result


def agent_make(agname, duration, keys):
    p = SPr.Popen([CP.python, tmpdir+"/agent_helper.py","store",agname], text=False,
                          shell=False, stdin=SPr.PIPE, stdout=SPr.DEVNULL, stderr=SPr.DEVNULL)
    pwrite = p.stdin.write   ; pwrite(min(duration,60).to_bytes(1,"big"))
    for k in keys:   pwrite(len(k).to_bytes(1,"big"))   ; pwrite(k)   ; pwrite(Destination.magic)
    p.poll(); p.stdin.flush(); p.stdin.close()
    return p


# DataCryptography(): Handle crypto functions and state for volume data

class DataCryptography:

    crypto_key_bits = 256      ; max_ct_bits = 128        ; salt_sz = 64
    max_keyfile_sz  = (salt_sz + (max_ct_bits // 8)) * 4  # max 4 slots
    time_headroom   = int(60*60*24*365.25*50)             ; timesz  = 32 // 8

    # Matrix of recommended mode pairs = 'formatcode: (data, metadata, selectable)'
    # User selects a data cipher which is automatically paired w a metadata authentication cipher.
    crypto_codes    = {b"00":  ("off",                "off", 1),
                       b"10":  ("n/a",                "n/a", 0),
                       b"20":  ("n/a",                "n/a", 0),
                       b"30":  ("xchacha20",          "xchacha20-poly1305",    0),
                       b"31":  ("xchacha20-t2",       "xchacha20-poly1305-t2", 1),
                       b"32":  ("xchacha20-tc",       "xchacha20-poly1305",    1),
                       b"33":  ("n/a",                "n/a", 0),
                       b"34":  ("xchacha20-t3",       "xchacha20-poly1305-t3", 1),
                       b"35":  ("xchacha20-t4",       "xchacha20-poly1305-t3", 1),
                       b"40":  ("n/a",                "n/a", 0)}

    __slots__ = ("key","keyfile","ci_type","counter","ctstart","ctcadence","countsz","max_count",
                 "slot","slot_offset","key_sz","nonce_sz","tag_sz","randomsz","buf_start","mode",
                 "encrypt","decrypt","auth","ChaCha20_new","ChaCha20_Poly1305_new",
                 "time_start","monotonic_start","get_rnd","noncekey","mhashkey")

    def __init__(self):
        self.key = self.noncekey = self.keyfile = self.counter = self.ctstart = self.countsz \
                 = self.mhashkey = self.slot_offset = None

    def load(self, ci_type, keyfile, slot, passphrase, agentkeys=None, cadence=1, init=False):

        if tuple(time.gmtime(0))[:6] != (1970, 1, 1, 0, 0, 0):
            x_it(1, "System time epoch is not 1970-01-01.")
        assert passphrase is None or type(passphrase) == bytearray
        assert type(cadence) is int and cadence > 0

        if ci_type.startswith("xchacha20") and Cryptodome.version_info[0:2] < (3,9):
            raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")

        self.keyfile    = keyfile          ; self.ci_type  = ci_type
        self.slot       = slot             ; self.slot_offset = self.get_slot_offset(slot)
        self.ctcadence  = cadence          ; mknoncekey    = mkmhashkey = False
        self.time_start = time_start       ; self.monotonic_start = monotonic_start
        self.get_rnd    = get_random_bytes ; self.auth     = False

        # xchacha20 common
        self.key_sz  = self.crypto_key_bits//8 ; self.max_count = 2**80-64
        self.nonce_sz= 24                      ; self.buf_start = self.nonce_sz
        self.countsz = 10
        self.randomsz=self.nonce_sz - self.countsz - self.timesz
        self.ChaCha20_new = Cipher_ChaCha20.new
        self.ChaCha20_Poly1305_new = Cipher_ChaCha20_Poly1305.new
        self.decrypt = self._dec_chacha20

        if ci_type == "xchacha20":
            self.encrypt = self._enc_chacha20

        elif ci_type == "xchacha20-poly1305":
            self.tag_sz  = 16                  ; self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305

        elif ci_type == "xchacha20-tc":
            self.encrypt = self._enc_chacha20
            mkmhashkey   = True

        elif ci_type == "xchacha20-t2":
            self.encrypt = self._enc_chacha20_t2
            mkmhashkey   = True

        elif ci_type == "xchacha20-poly1305-t2":
            self.tag_sz  = 16                  ; self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305_t2

        elif ci_type == "xchacha20-t3":
            self.encrypt = self._enc_chacha20_t3
            mknoncekey   = mkmhashkey = True

        elif ci_type == "xchacha20-poly1305-t3":
            self.tag_sz  = 16                  ; self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305_t3
            mknoncekey   = True

        elif ci_type == "xchacha20-t4":
            self.encrypt = self._enc_chacha20_t4
            mknoncekey   = mkmhashkey = True

        else:
            raise ValueError("Invalid cipher spec "+ci_type)


        # Load counter and key
        if not issubclass(type(keyfile), io.IOBase):
            if not exists(keyfile) and init:
                open(keyfile, "wb").close()    ; self.keyfile = open(keyfile, "r+b", buffering=0)
                for ii in range(4):
                    self.keyfile.seek(self.get_slot_offset(ii))
                    self.keyfile.write(bytes(self.countsz) + self.get_rnd(self.salt_sz))
            else:
                self.keyfile = open(keyfile, "r+b", buffering=0)

        if os.fstat(self.keyfile.fileno()).st_size != self.max_keyfile_sz:
            self.keyfile.truncate(self.max_keyfile_sz)

        ct, salt     = self.load_slot(self.slot)
        # Kludge for original counter mode salt
        if ci_type in ("xchacha20","xchacha20-poly1305"):   salt = salt[:32]
        # Advance counter with a safe margin 'cadence X2' if cadence is quick (<101)
        self.counter = self.ctstart = ct + (cadence * 2 * int(cadence < 101))

        if agentkeys:
            self.key = agentkeys[slot]
        else:
            self.key = self.derive_key(salt, passphrase, self.key_sz)
        assert len(self.key) == self.key_sz and type(self.key) is bytearray

        if mknoncekey:
            _na, nk_salt  = self.load_slot(2)
            self.noncekey = self.derive_subkey(self.key, 64, 0, nk_salt,
                                               b"Wyng_Nonces" + str(self.slot).encode())

        if mkmhashkey:
            _na, mhk_salt = self.load_slot(3)
            self.mhashkey = self.derive_subkey(self.key, 64, 1, mhk_salt,
                                               b"Wyng-Manifest-Hash" + str(self.slot).encode())

        return self.counter


    def __del__(self):
        if self.counter and self.counter > self.ctstart:   self.save_counter()
        if self.key:        clear_array(self.key)
        if self.noncekey:   clear_array(self.noncekey)
        if self.mhashkey:   clear_array(self.mhashkey)


    def get_slot_offset(self, slot):
        return (self.salt_sz + (self.max_ct_bits//8)) * slot

    def load_slot(self, slot):
        self.keyfile.seek(self.get_slot_offset(slot))
        counter = int.from_bytes(self.keyfile.read(self.countsz), "big")
        salt    = self.keyfile.read(self.salt_sz)
        assert len(salt) == self.salt_sz and salt != bytes(len(salt))
        return counter, salt

    def derive_key(self, salt, passphrase, size):
        key = bytearray(hashlib.scrypt(passphrase, salt=salt, n=2**19, r=8, p=1,
                                        maxmem=640*1024*1024, dklen=size))
        clear_array(passphrase)
        return key

    def derive_subkey(self, key, size, subslot, salt, context):
        skeys = Cryptodome.Protocol.KDF.HKDF(key, size, salt, Cryptodome.Hash.SHA512,
                                             max(subslot+1, 2), context)
        return bytearray(skeys[subslot])

    # Provide keyed hash func for manifests (send/receive)
    def getmhash_hmac(self, buf):
        return hmac.digest(self.mhashkey, buf, "sha256")

    # Update key counter on disk; call directly at end of transaction if cadence > 1
    def save_counter(self):
        self.keyfile.seek(self.slot_offset)
        self.keyfile.write(self.counter.to_bytes(self.countsz, "big"))
        self.keyfile.flush()

    # Update counter with a new value, if greater
    def set_counter(self, ct):
        if ct > self.counter:
            self.counter = ct    ; self.save_counter()
        return self.counter

    # Decrypt [X]ChaCha20:
    def _dec_chacha20(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        return cipher.decrypt(untrusted_buf[self.nonce_sz:])

    # Decrypt [X]ChaCha20-Poly1305:
    def _dec_chacha20_poly1305(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)

    # Encrypt [X]ChaCha20:
    def _enc_chacha20(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.time_start - self.monotonic_start + time.monotonic())
                               - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        buf    = cipher.encrypt(buf)
        return  nonce, buf

    # Encrypt [X]ChaCha20-Poly1305:
    def _enc_chacha20_poly1305(self, buf):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.time_start - self.monotonic_start + time.monotonic())
                               - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (random nonce)
    def _enc_chacha20_t2(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from rnd
        nonce   = self.get_rnd(self.nonce_sz)
        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)

    # Encrypt [X]ChaCha20-Poly1305 (random nonce)
    def _enc_chacha20_poly1305_t2(self, buf):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from rnd
        nonce   = self.get_rnd(self.nonce_sz)
        cipher  = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (HMAC nonce)
    def _enc_chacha20_t3(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || buf
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(buf)
        nonce   = nonce_h.digest()[:24]

        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)

    # Encrypt [X]ChaCha20-Poly1305 (HMAC nonce)
    def _enc_chacha20_poly1305_t3(self, buf):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || buf
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(buf)
        nonce   = nonce_h.digest()[:24]

        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (HMAC nonce)
    def _enc_chacha20_t4(self, buf, mhash):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || Hm (manifest hash)
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(mhash)
        nonce   = nonce_h.digest()[:24]

        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)


# Define absolute paths of commands

class CP:
    awk    = "/usr/bin/awk"     ; sed   = "/bin/sed"        ; sort     = "/usr/bin/sort"
    cat    = "/bin/cat"         ; mkdir = "/bin/mkdir"      ; python   = "/usr/bin/python3"
    mv     = "/bin/mv"          ; grep  = "/bin/grep"       ; ssh      = "/usr/bin/ssh"
    sh     = "/bin/sh"          ; tar   = "/bin/tar"        ; tail     = "/usr/bin/tail"
    rm     = "/bin/rm"          ; lvm   = "/usr/sbin/lvm"   ; qvm_run  = "/usr/bin/qvm-run"
    tee    = "/usr/bin/tee"     ; sync  = "/bin/sync"       ; dmsetup  = "/sbin/dmsetup"
    chattr = "/usr/bin/chattr"  ; xargs = "/usr/bin/xargs"  ; sha256sum= "/usr/bin/sha256sum"
    cmp    = "/usr/bin/cmp"     ; gzip  = "/bin/gzip"       ; env      = "/usr/bin/env"
    ps     = "/usr/bin/ps"      ; uniq  = "/usr/bin/uniq"   ; filefrag = "/usr/sbin/filefrag"
    thin_delta = "/usr/sbin/thin_delta"    ; cp     = "/bin/cp"
    blkdiscard = "/sbin/blkdiscard"        ; ionice = "/usr/bin/ionice"
    btrfs      = "/usr/sbin/btrfs" if os.path.exists("/usr/sbin/btrfs") else "/bin/btrfs"


# Manage local (source) data volumes, thin lvm and reflink image files and snapshots.
# Volume names are mapped into the 'lvols' cache based on whether they are found
# under the current --local path or if a volume name exists in the archive.
# lvols enties may point to local volumes that are non-existant, so use v.exists().

class LocalStorage:

    BLKDISCARD               = 0x1277           ; BLKDISCARDZEROES        = 0x127c
    FALLOC_FL_KEEP_SIZE      = 0x01             ; FALLOC_FL_PUNCH_HOLE    = 0x02
    FALLOC_FL_COLLAPSE_RANGE = 0x08             ; FALLOC_FL_ZERO_RANGE    = 0x10
    FALLOC_FL_INSERT_RANGE   = 0x20             ; FALLOC_FL_UNSHARE_RANGE = 0x40
    FALLOC_FL_PUNCH_FULL     = FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE

    fallocate = ctypes.CDLL(ctypes.util.find_library("c")).fallocate
    fallocate.restype = ctypes.c_int
    fallocate.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int64, ctypes.c_int64]

    def __init__(self, localpath_t, auuid=None, arch_vols=[], clean=False, sync=False,
                 require_online=False):

        self.stypes  = { "tlvm":  LvmVolume,   "rlnk": ReflinkVolume }
        self.rltypes = { "btrfs", "xfs" }

        assert len(auuid) > 8
        self.gc_procs  = []          ; self.locked  = False      ; self.auuid     = auuid
        self.clean     = clean       ; self.sync    = sync
        self.arch_vols = arch_vols
        self.lvols, self.vgs_all = {}, {}
        self.users, self.groups  = {}, {}
        self.path  = self.pooltype = self.fstype = self.lvpool  = None

        if localpath_t[0]:
            self.pooltype       = "tlvm"
            self.block_size     = 512
            self.path           = "/dev/"+localpath_t[0]+"/"    ; self.vgname = localpath_t[0]
            self.online         = LocalStorage.vg_exists(localpath_t[0]) and exists(self.path)
            self.lvpool         = localpath_t[1]

            self.acquire_deltas = get_lvm_deltas
            self.process_deltas = update_delta_digest_lvm
            self.prep_snapshots = prepare_snapshots_lvm

        elif not localpath_t[0] and localpath_t[1].startswith("/"):
            self.pooltype       = "rlnk"
            self.block_size     = 4096   # Test this with XFS ###
            self.path           = localpath_t[1]
            self.online         = exists(self.path)
            if self.online:
                if (fs := LocalStorage.get_fs_type(self.path)) in self.rltypes:   self.fstype = fs
                self.snappath = self.path+("wyng_snapshot_tmp/" if self.fstype == "btrfs" else "")

            self.users  = {x[0]: int(x[2]) for x in
                            (ln.split(":") for ln in open("/etc/passwd","r") if ln.strip())}
            self.groups = {x[0]: int(x[2]) for x in
                            (ln.split(":") for ln in open("/etc/group","r") if ln.strip())}

            self.acquire_deltas = get_reflink_deltas
            self.process_deltas = update_delta_digest_reflink
            self.prep_snapshots = prepare_snapshots_reflink

        elif require_online:
            raise ValueError("Indeterminate local path.")

        else:
            self.online = False

        if require_online and not self.online:
            x_it(7, "Local storage is offline: "+repr(localpath_t))

        if self.online:  #Fix: make conditional on send/monitor/receive
            self.LVolClass    = self.stypes[self.pooltype]
            self.path = self.path.rstrip("/")+"/"
            self.update_vol_list(arch_vols)
            self.metadata_unlock()

        if debug:
            print("**fstype is", self.fstype)
            print("**pooltype", self.pooltype, "not" if not self.online else "", "online")
            print(self.path, self.lvpool)

    def __del__(self):
        if self.clean:
            for p in self.gc_procs:   p.wait()


    # Note: file_punch_hole() and block_discard_chunk() have the same arg signature...
    def file_punch_hole(self, fn, start, length):
        return self.fallocate(fn, self.FALLOC_FL_PUNCH_FULL, start, length)

    def block_discard_chunk(self, fn, start, length):
        try:
            return fcntl.ioctl(fn, self.BLKDISCARD, struct.pack("LL", start, length))
        except Exception as e:
            return None

    def setperms(self, path, perms):
        if not perms:   return
        p_t = perms.split(":")    ; user = group = None

        if len(p_t) == 1:
            # set lvm type perm
            assert p_t[0] in ("r","w")
            pbits = 0o600 if p_t[0]=="w" else 0o400
        else:
            pbits, user, group = p_t    ; pbits = int(pbits)

        if os.path.isfile(path):
            os.chmod(path, stat.S_IMODE(pbits))
            if admin_permission and user and user in self.users:
                shutil.chown(path, user, group if group in self.groups else None)
        elif self.pooltype == "tlvm" and path.startswith(self.path):
            p  = "w" if pbits & stat.S_IWUSR else ""
            old= SPr.check_output([CP.lvm, "lvs", "--noheadings", "-o","lv_attr", path], text=True)
            if p != old.strip()[1]:
                do_exec([[CP.lvm, "lvchange", path, "-p", "r"+p]])

    def settime(self, path, t):
        if t > 0 and os.path.isfile(path):
            os.utime(path, ns=(t,t))
            return True
        else:
            return False

    def metadata_lock(self, lvpool=None):
        mark = 0    ; spath = self.path
        if self.pooltype == "tlvm":
            self._lvm_meta_snapshot("reserve", pool=lvpool)
        elif self.pooltype == "rlnk":
            if not self.fstype or self.fstype not in ("btrfs","xfs"):
                raise ValueError("Bad fstype "+repr(self.fstype))
            if self.fstype == "btrfs":
                mark, spath = self._btrfs_subvol_snapshot()
            elif self.fstype == "xfs":
                pass # possibly file-lock and chmod -r rlnk snapshots
        self.locked = True
        return mark, spath

    def metadata_unlock(self, lvpool=None):
        mark_t = (0, self.path)
        if self.pooltype == "tlvm":
            for pl in {lvpool} if lvpool else {getattr(x, "pool_lv", self.lvpool)
                                               for x in self.lvols.values()}:
                if pl:   self._lvm_meta_snapshot("release", pool=pl)

        elif self.pooltype == "rlnk":
            if self.fstype == "btrfs":
                mark_t = self._btrfs_subvol_snapshot(delete=True)

        self.locked = False
        return mark_t

    def check_support(self):
        if self.pooltype == "tlvm":
            for prg in (CP.lvm, CP.dmsetup, CP.thin_delta ):
                if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)
            try:
                p = SPr.check_output([CP.thin_delta, "-V"])
            except:
                p = b""
            ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
            if p and ver < target_ver:
                raise RuntimeError("Thin provisioning tools version >= "+target_ver+" required.")
        elif self.pooltype == "rlnk" and self.fstype == "btrfs":
            if not shutil.which(CP.btrfs):  raise RuntimeError("Required command not found: btrfs")
            # maybe also check kernel version and xfs reflink support...

    def _btrfs_subvol_snapshot(self, delete=False):
        svpath = self.path    ; dest = self.snappath    ; gen = 0

        if exists(dest):
            if delete:   gen = self._get_btrfs_generation(dest)
            do_exec([[CP.btrfs, "subvolume", "delete", dest]])
        if not delete:
            do_exec([[CP.btrfs, "subvolume", "snapshot", "-r", svpath, dest]])
            gen = self._get_btrfs_generation(dest)
            # possibly check /sys/fs/btrfs/uuid#/exclusive_operation

            if debug:   print("Created subvol snapshot at", dest)
        return gen, dest

    def _get_btrfs_generation(self, path):
        res = SPr.check_output([CP.btrfs, "subvolume", "list", path])
        if debug:   print("*generation", res)
        return int(res.split()[3])

    # Reserve or release lvm thinpool metadata snapshot.
    # action must be "reserve" or "release".
    def _lvm_meta_snapshot(self, action, pool=None):
        vgname   = self.vgname.replace("-","--")
        poolname = (pool or self.lvpool).replace("-","--")
        do_exec([[CP.dmsetup,"message", vgname+"-"+poolname+"-tpool",
                "0", action+"_metadata_snap"]], check= action=="reserve")

    def new_vol_entry(self, newname, vid, replace=False):
        if newname not in (lvols := self.lvols) or replace:
            lvols[newname] = vol = self.LVolClass(self, newname, vid=vid)
        else:
            vol = lvols[newname]    ; vol.vid = vid

        for sv in (vol.snap1, vol.snap2) if vol.snap1 else []:
            if sv and sv not in lvols:   lvols[sv] = self.LVolClass(self, sv)
        return vol

    # Create survey of all interesting volumes
    def update_vol_list(self, arch_vols):
        self.lvols.clear()
        if self.pooltype == "tlvm":   self.update_lvm_list()
        for vname, vid in arch_vols:
            self.new_vol_entry(vname, vid)

    # Retrieves survey of all LVM VGs/LVs
    def update_lvm_list(self):
        if not shutil.which(CP.lvm):   sys.stderr.write("LVM not available.\n"); return

        delim   = ":::"            ; colnames = LvmVolume.colnames
        vgs_all = self.vgs_all     ; LvmVol   = LvmVolume

        do_exec([[CP.lvm, "lvs", "--units=b", "--noheadings", "--separator="+delim,
                    "--options=" + ",".join(colnames)]],
                out=tmpdir+"/volumes.lst")

        for ln in open(tmpdir+"/volumes.lst", "r"):
            lv = LvmVol(self, "", members=zip(colnames, ln.strip().split(delim)))
            vgs_all.setdefault(lv.vg_name, {})[lv.name] = lv

        self.lvols = vgs_all[self.vgname]

    # static
    def vg_exists(vgname):
        try:
            do_exec([[CP.lvm, "vgdisplay", vgname]])
        except SPr.CalledProcessError:
            return False
        else:
            return True

    def get_fs_type(path):
        mtab = {x[1]: x[2] for x in map(str.split, open("/etc/mtab","r"))}
        while path not in mtab and path != "/":   path = os.path.dirname(path)
        return mtab[path]

    # accepts either a volgroup/pool or directory path and returns type, tpool, absolute path
    def parse_local_path(localpath):
        lvname, lvpool, vg, lvattr = LocalStorage.get_lv_path_pool(localpath)
        abspath = os.path.abspath(localpath)

        if lvname and not lvpool and lvattr.startswith("t") and exists("/dev/"+vg):
            return ("logical volume", lvname, "/dev/"+vg)
        elif abspath.startswith("/dev"):
        #and stat.S_ISBLK(os.stat(localpath).st_mode):
            return ("block device", "", abspath)
        elif abspath.startswith("/") and exists(abspath) and os.path.isdir(abspath):
            return ("file", "", abspath)
        else:
            return (None, None, None)

    # Converts a non-cannonical LV path to LV name plus pool and vg names.
    def get_lv_path_pool(path):
        try:
            p = SPr.run([CP.lvm, "lvs", "--separator=:::", "--noheadings",
                                "--options=lv_name,pool_lv,vg_name,attr", path], check=True,
                                stdout=SPr.PIPE, stderr=SPr.DEVNULL)
        except:
            return "", "", "", ""
        else:
            return p.stdout.decode("utf-8").strip().split(":::")


# Base class for local volumes; do not instantiate.

class LocalVolume:
    maxname   = 255    ; maxpath = 4096 - maxname
    __slots__ = ("storage","lockfile","pdir","path","name","vid","snap1","snap2")

    def _my_init(self, storage, name, vid=None):
        assert storage.online

        self.storage   = storage                        ; self.lockfile = None
        self.pdir      = storage.path.rstrip("/")+"/"   ; self.path     = self.pdir+name
        self.name      = name                           ; self.vid      = vid
        self.snap1 = self.snap2 = None

    def lock(self, mode="r+b"):
        self.lockfile = lf = open(self.path, mode)
        fcntl.lockf(lf, fcntl.LOCK_EX|fcntl.LOCK_NB)
        return lf

    def unlock(self):
        if self.lockfile:   self.lockfile.close()
        return True

    def rotate_snapshots(self, rotate=True, timestamp_path=None, addtags=[]):
        assert not self.name.endswith(self.snap_ext)
        lvols = self.storage.lvols
        if rotate:
            if lvols[self.snap2].exists():
                t = lvols[self.snap2].gettime()    ; os.utime(timestamp_path, ns=(t,t))
                lvols[self.snap2].rename(self.snap1, addtags=addtags)
                with open(self.pdir+self.snap1,"rb") as vf:   os.fsync(vf)
                return t
        else:
            lvols[self.snap2].delete(sync=False)
        return None

    def setperms(self, perms):
        self.storage.setperms(self.path, perms)

    def settime(self, t):
        self.storage.settime(self.path, t)

    def exists(self):
        return exists(self.path)

    def check_pathname(path):
        if not 0 < len(path) <= LocalVolume.maxpath:
            return f"Volume path/name length must be 1 - {LocalVolume.maxpath}."
        if not path.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in path.strip().split("/")]):
            return "Bad volume name."


class ReflinkVolume(LocalVolume):

    snap_ext = (".wyng1",".wyng2")

    def __init__(self, storage, name, vid=None):
        super()._my_init(storage, name, vid)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            fdir, fname  = os.path.split(name)    ; subdir = fdir+"/" if fdir else ""
            self.snap1 = "".join((subdir,"sn",self.storage.auuid,"_",self.vid,self.snap_ext[0]))
            self.snap2 = "".join((subdir,"sn",self.storage.auuid,"_",self.vid,self.snap_ext[1]))

    def rename(self, new_name, addtags=[]):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        assert self.exists()

        newvol = self.storage.lvols[new_name]
        if debug:   print("*rename", self.path, newvol.path)
        if not newvol.exists():   newvol.create(size=0, ro=False)
        self.unlock()    ; os.replace(self.path, newvol.path)

    def delete(self, sync=True, check=False, force=False):
        assert force or self.name.endswith(self.snap_ext)
        self.unlock()
        if self.exists():   os.remove(self.path)

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")

        subdir, fname  = os.path.split(self.name)
        os.makedirs(self.pdir+subdir, exist_ok=True)
        if snapshotfrom:
            snap_path = self.storage.lvols[snapshotfrom].path
            do_exec([[CP.cp, "-p", "--reflink=always", snap_path, self.path]])
        else:
            assert size is not None
            with open(self.path, "wb") as vf:  vf.truncate(size); vf.flush()

        if ro:   rel_chmod(self.path, "-", 0o222)

        self.storage.lvols[self.name] = self

    def gettime(self):
        return os.stat(self.path).st_mtime_ns

    def resize(self, size):
        if not self.exists():   raise FileNotFoundError(self.path)
        with open(self.path, "r+b") as vf:   vf.truncate(size); vf.flush()

    def getsize(self):
        return os.path.getsize(self.path)

    def getperms(self):
        vstat = os.stat(self.path)    ; uid, gid = vstat.st_uid, vstat.st_gid
        # use names in place of uid/gid numbers:
        user  = [x for x, num in self.storage.users.items()  if num == uid][0]
        group = [x for x, num in self.storage.groups.items() if num == gid][0]
        return f"{stat.S_IMODE(vstat.st_mode)}:{user}:{group}"

    def is_arch_member(self):
        if not self.name.endswith(self.snap_ext) or not self.exists():
            return "na"
        elif self.storage.arch_vols and self.name not in self.storage.arch_vols:
            return "false"
        else:
            return "true"

    def is_paired(self, mapfile, sestag):
        if not self.name.endswith(self.snap_ext):   raise ValueError("Not a snapshot.")
        return self.exists() and exists(mapfile) and self.gettime() == os.path.getmtime(mapfile)
        #Enh: also evaluate sestag

    def convert_pathname(self, path):
        raise NotImplementedError()


class LvmVolume(LocalVolume):

    snap_ext  = (".tick",".tock")
    maxname   = 112    ; maxpath = maxname    ; alphanumsym = r"^[a-zA-Z0-9\+\._-]+$"
    colnames  = ("vg_name","lv_name","lv_attr","lv_size",
                 "lv_time","pool_lv","thin_id","tags")
    __slots__ = colnames

    def __init__(self, storage, name=None, vid=None, members=[]):

        self.tags = ""
        for attr, val in members:   setattr(self, attr, val)

        super()._my_init(storage, name or self.lv_name, vid)
        name = self.name    ; assert bool(name)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            self.snap1 = name + self.snap_ext[0]
            self.snap2 = name + self.snap_ext[1]

    def rename(self, new_name, addtags=[]):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        assert self.exists()
        storage = self.storage

        if addtags:   do_exec([[CP.lvm, "lvchange", self.path] + addtags])

        m = ((x, getattr(self,x)) for x in set(self.colnames) - {"lv_name","lv_path","thin_id"})
        nv = storage.lvols[new_name] = LvmVolume(storage, name=new_name, vid=self.vid, members=m)
        storage.new_vol_entry(self.name, self.vid, replace=True)

        nv.delete()
        self.unlock()
        do_exec([[CP.lvm, "lvrename", self.path, new_name]])

    def delete(self, sync=True, check=False, force=False):
        # Enh: re-write with asyncio
        sync = (optsync := options.maxsync) or sync    ; clean = options.clean
        assert not (sync == False and check)
        assert force or self.name.endswith(self.snap_ext)

        if self.exists():
            self.unlock()    ; procs = self.storage.gc_procs    ; maxprocs = 16
            if len(procs) == maxprocs:
                for ii in reversed(range(len(procs))):
                    retcode = procs[ii].returncode
                    if retcode is not None:
                        if clean and retcode != 0:
                            raise CalledProcessError("lvremove failed "+str(retcode))
                        del(procs[ii])

            cmds = [CP.sh, "-c", CP.lvm + " lvchange -p rw " + self.path + " ; "
                 +  CP.lvm + " lvremove -f " + self.path]
            if not (sync or clean):   cmds = [CP.ionice, "-c3"] + cmds
            p = SPr.Popen(cmds, shell=False,
                                stdout=SPr.DEVNULL, stderr=SPr.DEVNULL)
            if not sync and len(procs) < maxprocs:
                procs.append(p)
            else:
                retcode = p.wait()
                if check and retcode != 0:
                    raise CalledProcessError("lvremove failed "+str(retcode))

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        vg = self.storage.path    ; rwmode = ["-pr"+("w" if not ro else "")]
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        if snapshotfrom:
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay"] + rwmode + addtags + [
                        "-s", vg+"/"+snapshotfrom, "-n", self.name]])
        else:
            assert size is not None
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(size)+"b"] + rwmode
                      + addtags + ["--thin", "-n", self.name, vg+"/"+self.storage.lvpool]])

        self.storage.lvols[self.name] = self

    def gettime(self):
        return int(time.mktime(time.strptime(self.lv_time, r"%Y-%m-%d %H:%M:%S %z"))*1000000000)

    def settime(self, t):
        if verbose:
            err_out("Not setting time on LVM object.")
        return False

    def resize(self, size):
        if not self.exists():   raise FileNotFoundError(self.path)
        do_exec([[CP.lvm, "lvresize", "-L", str(size)+"b", "-f", self.path]])

    def getsize(self):
        return int(re.sub("[^0-9]", "", self.lv_size))

    def getperms(self):
        return self.lv_attr[1]

    def is_arch_member(self):
        if not self.name.endswith(self.snap_ext) or "wyng" not in self.tags or not self.exists():
            return "na"
        if "arch-"+self.storage.auuid not in self.tags:
            return "false"
        else:
            return "true"

    def is_paired(self, mapfile, sestag):
        if not self.name.endswith(self.snap_ext):   raise ValueError("Not a snapshot.")

        return self.exists() and exists(mapfile) and self.gettime() == os.path.getmtime(mapfile) \
               and sestag in self.tags.split(",")

    def check_pathname(path):
        if res := super.check_pathname(path):
            return res
        if re.match(LvmVolume.alphanumsym, path) is None:
            return "Only characters A-Z 0-9 . + _ - are allowed in LVM volume names."
        return ""

    def convert_pathname(self, path):
        raise NotImplementedError()


# Try to sync only selected filesystem
def fssync(path):
    if options.maxsync:   SPr.Popen([CP.sync,"-f",path])


def rel_chmod(fpath, op, mask):
    assert op in ("-","+")
    current = stat.S_IMODE(os.stat(fpath).st_mode)
    os.chmod(fpath, (current & ~mask) if op=="-" else (current | mask))


def ask_passphrase(prompt="Enter passphrase: ", verify=False):
    for ii in range(3):
        passphrase  = bytearray(getpass.getpass(prompt), encoding="UTF-8")
        if not verify or len(passphrase) >= 10:   break
        print("Passphrase must be 10 or more characters.")   ; clear_array(passphrase)
        if ii == 2 or options.unattended:   x_it(3, "Passphrase required.")
    if options.unattended or not verify:   return passphrase
    passphrase2 = bytearray(getpass.getpass("Re-enter passphrase: "), encoding="UTF-8")
    if passphrase != passphrase2:
        clear_array(passphrase)   ; clear_array(passphrase2)
        x_it(3, "Entries do not match.")
    clear_array(passphrase2)
    return passphrase


def clear_array(ar):
    for ii in range(len(ar)):   ar[ii] = 0


# Initialize a new ArchiveSet:

def arch_init(aset, opts):
    if not opts.local:
        x_it(1,"--local is required.")

    aset.set_local(opts.local)

    aset.data_cipher = opts.encrypt or "xchacha20-t3"
    # Fix: duplicates code in aset... move to aset class.
    if aset.data_cipher in (x[0] for x in DataCryptography.crypto_codes.values() if x[2]):
        aset.ci_mode, ci= [(x,y) for x,y in DataCryptography.crypto_codes.items()
                                 if y[0] == aset.data_cipher][0]

        if aset.data_cipher != "off":
            # Security Enh: Possibly use mmap+mlock to store passphrase/key values,
            #               and wipe them from RAM after use.
            passphrase      = ask_passphrase(prompt="Enter new encryption passphrase: ",
                                             verify=True)
            aset.datacrypto = DataCryptography()
            aset.mcrypto    = DataCryptography()
            aset.mcrypto.load(ci[1], aset.confpath+".salt", slot=1,
                                     passphrase=passphrase[:], init=True)
            aset.datacrypto.load(aset.data_cipher, aset.mcrypto.keyfile, slot=0,
                                     passphrase=passphrase, init=True)
            shutil.copyfile(aset.path+"/"+aset.confname+".salt", aset.path+"/salt.bak")
    else:
        x_it(1,"Error: Invalid cipher option.")

    print(); print(f"Encryption    : {aset.data_cipher} ({ci[1]})")

    if opts.hashtype:
        if opts.hashtype not in hash_funcs or opts.hashtype == "sha256":
            x_it(1, "Hash function '"+opts.hashtype+"' is not available on this system.")

    # Use hmac-sha256 as data hash if the mode supports it:
    aset.hashtype = "hmac-sha256" if aset.datacrypto and aset.datacrypto.mhashkey else "blake2b"
    # Always use blake2b for metadata:
    aset.gethash  = hash_funcs["blake2b"]
    print("Data Hashing  :", aset.hashtype)

    if opts.compression:
        if ":" in opts.compression:
            compression, compr_level = opts.compression.strip().split(":")
        else:
            compression = opts.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not is_num(compr_level):
            x_it(1, "Invalid compression spec.")
        #if compression == "zstd":
        #    print("Warning: zstd does not support reproducible output; future zstd updates may "
        #          "prevent Wyng from deduplicating data.")
        aset.compression = compression      ; aset.compr_level = compr_level
        compressors[compression][2](b"test", int(compr_level))

    print("Compression   : %s:%s" % (aset.compression, aset.compr_level))

    if opts.chfactor:
        # accepts an exponent from 1 to 6
        chfactor = int(opts.chfactor)
        if not ( 0 < chfactor < 7 ):
            x_it(1, "Requested chunk size not supported.")
        aset.chunksize = (aset.min_chunksize//2) * (2** chfactor)
        if aset.chunksize > 256 * 1024:
            print("Large chunk size set:", aset.chunksize)

    aset.save_conf()
    update_dest(aset, pathlist=[aset.confname] + ([aset.confname+".salt", "salt.bak"]
                                                  if aset.datacrypto else []))


# Check/verify an entire archive

def arch_check(storage, aset, vol_list=None, startup=False):
    dest     = aset.dest                   ; attended  = not options.unattended
    chunksize= aset.chunksize
    compare_digest = hmac.compare_digest   ; b64enc    = base64.urlsafe_b64encode

    decrypt  = aset.datacrypto.decrypt if aset.datacrypto else None

    vol_list = vol_list or list(aset.vols.keys())
    vol_dirs = set((x.name for x in os.scandir(aset.path) \
                            if x.is_dir() and x.name.startswith("Vol_")))

    vdir_strays = vol_dirs - set(aset.conf["volumes"].keys())
    if vdir_strays:
        print("Stray volume dirs:", vdir_strays)

    # Remove orphan snapshots
    for lv in (x for x in storage.lvols.values() if x.is_arch_member() == "false"):
        if options.clean:
            lv.delete()
            print("Removed orphan snapshot:", lv.name)

    # Check volume contents at various levels (dirs, metadata, content)
    for volname in vol_list if dest.online else []:
        if volname not in aset.vols:   continue

        vol = aset.vols[volname]
        if not startup or debug:   print("Volume", volname, flush=True)

        # Remove session tmp dirs
        for sdir in os.scandir(vol.path):
            if sdir.name.startswith("S_") and sdir.name.endswith("-tmp"):
                if debug:   print("Removing partial session dir '%s'" % sdir.name)
                dest.run([dest.cd + " && rm -rf " + vol.vid+"/"+sdir.name])
                shutil.rmtree(vol.path+"/"+sdir.name, ignore_errors=True)

        # Check session sequencing
        seslist = vol._seslist
        for si, ses in enumerate(seslist[1:]):
            if ses.previous != seslist[si].name:
                raise ValueError(f"Prev Out of sequence: {ses.name} -> {seslist[si].name}")
            if ses.sequence == seslist[si].sequence:
                raise ValueError(f"Duplicate sequence {ses.sequence} in {ses.name}")
        vol._seslist = None

        if len(vol.sessions) > 1 and exists(vol.mapfile(-2)):   os.remove(vol.mapfile(-2))
        if exists(vol.path+"/deltamap"):
            if not exists(vol.mapfile()):
                os.replace(vol.path+"/deltamap", vol.mapfile())
            else:
                os.remove(vol.path+"/deltamap")

        if startup:   continue

        # Remove stray volume dirs and other files
        if options.clean:   dest.run([dest.cd + " && rm -r"] + vdir_strays)
        deepclean = options.clean and options.force
        for pos in list(range(len(vol.sessions)))[:-1]:
            if exists(vol.mapfile(pos)):   os.remove(vol.mapfile(pos))

        # Check all combined manifests are correct
        print("  Checking indexes,", end="", flush=True) ; mset = []
        for ses in vol.sesnames:
            mset.append(ses)    ; check_manifest_sequence(vol, mset)
            #### FIX: implement deepclean

        # Check hashes of each session individually
        print(" data:")
        for sesname in reversed(vol.sesnames):
            if options.session and ((options.session.lower() == "newest" \
            and sesname != vol.sesnames[-1]) or "S_"+options.session < sesname):
                continue # Enh: use seq

            if attended:   print(" ", sesname[2:], end="... ", flush=True)
            bcount = receive_volume(storage, vol, select_ses=sesname[2:], verify_only=2)
            #if attended:   print(bcount, "bytes OK")


def check_manifest_sequence(vol, sesnames, addcol=False):
    volsize = vol.sessions[sesnames[-1]].volsize    ; aset = vol.archive
    manifest = merge_manifests(vol, msessions=sesnames, addcol=addcol)
    with open(manifest, "r") as mrgf:
        for addr in range(0, volsize, aset.chunksize):
            ln = mrgf.readline().strip()
            if not ln:   break
            ln1, ln2 = ln.split(maxsplit=1)
            if addcol:   ln2, ses = ln2.split()
            if ln1 != "0":
                assert len(ln1) == aset.mhash_sz    ; h1 = base64.urlsafe_b64decode(ln1)
            assert len(ln2) == aset.sesname_sz      ; a1 = int("0"+ln2, 16)
            if addr != a1:
                print(ln); raise ValueError("Manifest seq error. Expected %d got %d." % (addr, a1))

    if addr != vol.last_chunk_addr(volsize):
        raise ValueError("Manifest range stopped short at", addr)

    os.remove(manifest)


# Get configuration settings:

def get_configs(opts):
    load_children = 1 if opts.action in ("arch-delete","delete","add") else 2

    dest = Destination(opts.dest, opts.dest_name)

    # FIX: allow monitor to function offline
    # FIX: check dest names for meta path if 'local_actions'
    if True: ####(opts.action not in local_actions and dest.sys is not None) or opts.remap:
        dest.detect_state(opts.dedup)

    # Check online status for certain commands.
    if not dest.online and (opts.remap  \
    or opts.action not in local_actions) and not (opts.action == "delete" and opts.clean):
        x_it(5, "Destination not ready to receive commands.")
    if not dest.writable and opts.action in write_actions \
    and not (opts.action == "delete" and opts.clean):
        x_it(5, "Destination not writable.")
    if dest.archive_ini_hash == "none" and opts.action not in local_actions+("arch-init",):
        x_it(6,"Archive not found at '%s'" % dest.spec)

    if opts.action in ("arch-init",):
        if dest.archive_ini_hash != "none":
            x_it(1, "Archive already exists: "+dest.spec)
        arch_init(ArchiveSet(tmpdir, dest), opts)
        x_it(0, "Done.")

    aset = get_configs_remote(dest, cachedir)

    if aset.updated_at is None:
        x_it(6, "Archive not found.")
    elif opts.local:
        aset.set_local(opts.local)

    os.utime(aset.path)
    return aset


# Fetch copy of archive metadata from remote/dest

def get_configs_remote(dest, base_dir):

    recv_list = [(ArchiveSet.confname,  ArchiveSet.max_conf_sz),
                 (ArchiveSet.confname+".salt", DataCryptography.max_keyfile_sz)]
    fetch_file_blobs(recv_list, tmpdir, dest, skip0=True)

    # Instantiate ArchiveSet to authenticate archive.ini
    aset = ArchiveSet(tmpdir, dest, children=0, pass_agent=options.pass_agent)

    assert aset.updated_at is not None
    arch_dir = base_dir + aset.subdir;   ini = pjoin(arch_dir, ArchiveSet.confname)

    if exists(ini) and not hmac.compare_digest(aset.raw_hashval,
                                        hashlib.sha256(open(ini,"rb").read()).hexdigest()):
        try:
            cache_aset = ArchiveSet(arch_dir, dest, children=0, prior_auth=aset)
        except Exception as e:
            err_out(repr(e)+"\nCached archive.ini didn't load.")
            shutil.rmtree(arch_dir)
        else:
            if cache_aset.updated_at > aset.updated_at:
                # check if any non-crypto-counter vars differ
                if aset.mcrypto and cache_aset.header != aset.header \
                or set(aset.conf) != set(cache_aset.conf) \
                or any((x != y for s in aset.conf
                  for x, y in zipln(dict(cache_aset.conf[s]).items(), dict(aset.conf[s]).items())
                  if x[0] not in ("mci_count","dataci_count","updated_at"))):
                    raise ValueError(f"Cached metadata is newer, from {cache_aset.path}\n"
                                     f"{cache_aset.updated_at} vs. {aset.updated_at}\n")
                elif aset.mcrypto:
                    # difference was only in counters, so advance them and continue
                    aset.mcrypto.set_counter(cache_aset.mci_count)
                    aset.datacrypto.set_counter(cache_aset.dataci_count)

                # Enh: Test-load the cache fully and/or fetch remote, but use largest counter#s
                # Fix: Check for .tmp versions and use local copy if all other fields match;
                # this allows recovery from interruption during send (archive.ini cadence).
            elif cache_aset.updated_at == aset.updated_at:
                err_out("Cached archive.ini differs with dest but timestamp matches.")
            else:
                if verbose:   err_out("Updating metadata cache.")
                shutil.rmtree(arch_dir+"-old", ignore_errors=True)
                os.replace(arch_dir, arch_dir+"-old")

    if not exists(ini):
        os.makedirs(arch_dir, exist_ok=True)
        for f, _ in recv_list:
            if exists(pjoin(tmpdir,f)):
                shutil.copyfile(pjoin(tmpdir,f), pjoin(arch_dir,f))

    # Initial auth successful! Fetch + auth volume metadata...
    return ArchiveSet(arch_dir, dest, children=2, allvols=True, prior_auth=aset)


class Destination:

    url_types   = ("ssh", "file", "qubes-ssh", "qubes")

    ssh_opts    = ["-x", "-o", "ControlPath=~/.ssh/controlsocket-%r@%h-%p",
                         "-o", "ControlMaster=auto", "-o", "ControlPersist=60",
                         "-o", "ServerAliveInterval=30", "-o", "ConnectTimeout=30",
                         "-o", "Compression=no"]

    magic       = b"\xff\x11\x15"

    def __init__(self, dspec, dname):

        # fetch locations list, dest spec
        dest_url = dspec    ; locs = self.load_locations()
        if dname:
            if re.match(".*[,=\^]", dname) or any(map(str.isspace, dname)) \
            or not dname.isprintable():
                x_it(1,"Error: [^control], [space], and ',^=' not allowed in dest-name.")
            if not dest_url and dname in locs:
                dest_url = locs[dname]
        elif not dspec and "default" in locs and not options.metadir:  #### options
            dest_url = locs["default"]

        # parse and validate dest spec
        if not dest_url:                 x_it(1,"Error: Missing dest specification.")
        if not dest_url.isprintable():   x_it(1,"Error: [^control] not allowed in dest.")
        dparts      = urlparse(dest_url)    ; self.dtype = dtype = dparts.scheme
        if dtype not in self.url_types:
            x_it(1,"'%s' not an accepted type." % dtype)
        if (dtype == "file" and (dparts.netloc or not dparts.path)) \
        or (dtype in ("ssh","qubes","qubes-ssh") and not dparts.netloc) \
        or (dtype == "qubes-ssh" and not all(dparts.netloc.partition(":"))) :
            x_it(1,"Error: Malformed --dest specification.")

        self.spec   = dest_url
        self.sys    = dparts.netloc
        self.path   = os.path.normpath(dparts.path)
        self.cd     = " cd '"+self.path+"'"      ; self.archive_ini_hash = "none"
        self.free   = self.dtmp     =  None      ; self.dname  = dname
        self.online = self.writable =  False

        self.run_map = {"file":       [CP.sh],
                        "ssh":        [CP.ssh] + self.ssh_opts + [self.sys],
                        "qubes":      [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                      "-p", self.sys],
                        "qubes-ssh":  [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                      "-p", self.sys.split(":")[0]]
                        }

        # save locations change
        if dspec and dname:
            print(f"Naming this URL '{dname}'.")
            locs[dname] = dspec
            self.save_locations(locs)    ; del(locs)


    def remove_dtmp(self):
        if self.online:
            self.run([f"cd $(dirname {self.dtmp}) && rm -rf $(basename {self.dtmp})"],
                     timeout=10)

    def get_free(self, fpath): ## Enh: add sanitize
        for ln in open(fpath,"r"):
            if ln.startswith("wyng_check_free"):  self.free = int(ln.split()[1])

    def save_locations(self, locs):
        with open(vardir+"/dests", "w") as fl:
            for x, y in locs.items():   print(x, y, file=fl)

    def load_locations(self):
        if not exists(vardir+"/dests"):   return {}
        with open(vardir+"/dests", "r") as fl:
            loc = { x: y for x, y in (w.strip().split(" ",maxsplit=1) for w in fl) }

        return loc

    # Run system commands on destination

    def run(self, commands, direct=False, timeout=None,
            infile="", inlines=None, out="", check=True, trap=False):

        if direct:
            cmd = self.run_map[self.dtype] + commands
        else:
            cmd = self.run_args(commands, trap=trap)

        return do_exec([cmd], infile=infile, inlines=inlines, out=out,
                       check=check, timeout=timeout)

    # Build command lists that can be shunted to remote systems.
    # The input commands are stored in a temp file and a standard command that
    # runs the temp file is returned.

    def run_args(self, commands, trap=False, dest_type=None):

        dest_type = dest_type or self.dtype    ; tmpprefix = "/tmp/wyngrpc/"
        trapcmd   = "trap '' INT TERM QUIT ABRT ALRM TSTP USR1\n" if trap else ""
        # shunt commands to local tmp file
        with tempfile.NamedTemporaryFile(dir=tmpdir+"/rpc", delete=False) as tmpf:
            cmd = bytes(trapcmd + shell_prefix
                        + " ".join(commands) + "\n", encoding="UTF-8")
            tmpf.write(cmd)
            remotetmp = os.path.basename(tmpf.name)

        if dest_type in {"qubes","qubes-ssh"}:
            do_exec([[CP.qvm_run, "--no-color-stderr", "--no-color-output", "-p",
                      (self.sys if dest_type == "qubes" else self.sys.split(":")[0]),
                      (CP.mkdir+" -p "+tmpprefix+"; " if not self.dtmp else "")
                      +CP.cat+" >"+tmpprefix+remotetmp
                    ]], infile=pjoin(tmpdir,"rpc",remotetmp))
            if dest_type == "qubes":
                add_cmd = [CP.sh+" "+tmpprefix+remotetmp]
            else:
                add_cmd = [CP.ssh+" "+" ".join(self.ssh_opts)+" "+self.sys.split(":")[1]
                        +' "$('+CP.cat+' '+tmpprefix+remotetmp+')"']

        elif dest_type == "ssh":
            #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
            add_cmd = [cmd]

        elif dest_type == "file":
            add_cmd = [pjoin(tmpdir,"rpc",remotetmp)]

        return self.run_map[dest_type] + add_cmd


    def detect_state(self, dedup):

        if self.dtype == "qubes-ssh":
            # fix: possibly remove dargs and use dest.run()
            dargs = self.run_map["qubes"][:-1] + [self.sys.split(":")[0]]

            cmd = dargs + [shell_prefix + "mkdir -p /tmp/wyngrpc"]
            do_exec([cmd])

        tmpprefix = "/tmp/wyngrpc/"    ; tmpdigits = 12
        cmd  = [r"mkdir -p "+self.path+r" && "+self.cd

                # send helper program to remote dest
                +r"  && mkdir -p " +tmpprefix + r" && chmod 777 " + tmpprefix
                +r"  && tdir=$(mktemp -d " + tmpprefix + ("X"*tmpdigits) + r") && echo $tdir"
                +r"  && cat >$tdir/dest_helper.py"

                # check free space and archive.ini status on remote
                +r"  && echo -n 'wyng_check_free ' && stat -f -c '%a %S' ."
                +r"  && echo -n 'wyng_archive_ini '"
                +r"  && { if [ -e archive.ini ]; then sha256sum archive.ini; else echo none; fi }"

                # test write access and hardlinks
                +r"  && touch archive.dat && echo 'wyng_writable'"
                +(r" && ln -f archive.dat .hardlink" if dedup else "")
                ]
        try:
            online = \
                not do_exec([self.run_args(cmd),
                            # sanitize remote output:
                            [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]],
                            pipefail=True,
                            out=tmpdir+"/dest-state.log", infile=tmpdir+"/rpc/dest_helper.py")
        except SPr.CalledProcessError as e:
            online = False    ; err_out(repr(e))

        if online:
            for ln in open(tmpdir+"/dest-state.log","r"):
                if ln.startswith("wyng_archive_ini"):
                    self.archive_ini_hash = ln.split()[1]
                elif ln.startswith("wyng_check_free"):
                    parts = ln.split()    ; self.free = int(parts[1]) * int(parts[2])
                elif ln.startswith("wyng_writable"):
                    self.writable = True
                elif ln.startswith(tmpprefix):
                    self.dtmp = ln.strip()

            if not self.dtmp or len(self.dtmp) != len(tmpprefix)+tmpdigits \
            or not set(self.dtmp[5:]) <= set(string.ascii_letters + string.digits + "/"):
                raise ValueError("Missing or malformed tmp dir: "+repr(self.dtmp))
        else:
            for log in ("/dest-state.log", "/err.log"):
                if exists(tmpdir+log):
                    do_exec([ [CP.cat, "-v", tmpdir+log],  [CP.tail, "--bytes=2000"],
                              [CP.grep, "-v", "^--+--"],
                              [CP.tee, "--append", tmpdir+log+"-out" ] ])
                    err_out(open(tmpdir+log+"-out", "r").read())

        self.online = self.free is not None


    def write_helper_program(path):

        dest_program = r'''#  Copyright Christopher Laprise 2018-2023
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, time, signal, shutil, subprocess as SPr, gzip, tarfile

def fssync(path):
    if msync:   SPr.Popen(["sync","-f",path])

def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1"], iflag=False):
    for sval in (getattr(signal,"SIG"+x) for x in sel):
        signal.signal(sval, handle_signal) ; signal.siginterrupt(sval, iflag)

def handle_signal(sig, frame):
    if sig == signal.SIGALRM:   raise IOError("Timeout ALRM")

def helper_send():
    mkdirs = os.makedirs    ; hlink = os.link    ; dirname = os.path.dirname
    with tarfile.open(mode="r|", fileobj=sys.stdin.buffer) as tarf:
        extract = tarf.extract    ; substitutions = {}    ; dirlist = set()
        for member in tarf:   sdir = member.name    ; mkdirs(sdir)    ; print(sdir)    ; break
        for member in tarf:
            if not member.islnk():
                extract(member, set_attrs=False)
            else:
                source = src_orig = member.linkname   ; dest = member.name   ; ddir = dirname(dest)
                if source in substitutions:   source = substitutions[source]
                if ddir not in dirlist:   mkdirs(ddir, exist_ok=True)   ; dirlist.add(ddir)
                try:
                    hlink(source, dest)
                except OSError as err:
                    print(err)    ; print("Substitution:", source, dest)
                    shutil.copyfile(source, dest)    ; substitutions[src_orig] = dest

def helper_receive(lstf):
    stdout_write = sys.stdout.buffer.write   ; exists = os.path.exists   ; getsize= os.path.getsize
    stdout_flush = sys.stdout.buffer.flush   ; magicm = magic
    for line in lstf:
        fname = line.strip()
        if not fname:   break
        fsize = getsize(fname) if exists(fname) else 0
        stdout_write(magicm + fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:   stdout_write(dataf.read(fsize))
        stdout_flush()

def helper_merge():
    exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
    try:
        if resume:
            if exists("merge-init") or not exists("merge"):
                raise RuntimeError("Merge: Init could not complete; Aborting merge.")
        else:
            print("Merge: Initialization.")
            for f in (target+"/info", target+"/manifest.z", "volinfo", "archive.ini"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")    ; fssync(".")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):   replace(i.path, i.name)
        elif not exists("merge"):
            for f in (target+"/info.tmp", target+"/manifest.tmp", "volinfo.tmp", "merge.lst.gz"):
                if exists(f):   os.remove(f)
        fssync(".")    ; print(err)    ; sys.exit(50)
    try:
        os.chdir("merge")  #  CD
        if not resume or not exists("CHECK-mv-rm"):
            print("Merge: remove/replace files.")    ; subdirs = set()
            for src in src_list:  # Enh: replace os.scandir w manifest method
                for i in os.scandir(src):
                    if i.is_dir():   subdirs.add(i.name)
            for sdir in subdirs:   os.makedirs(merge_target+"/"+sdir, exist_ok=True)
            for line in lstf:
                ln = line.split() # default split() does strip()
                if ln[0] == "rename" and (not resume or exists(ln[1])):
                    replace(ln[1], ln[2])
                elif ln[0] == "-rm" and exists(ln[1]):
                    remove(ln[1])
            open("CHECK-mv-rm","w").close()
        os.chdir("..")     #  CD
    except Exception as err:
        print(err)    ; sys.exit(60)

def helper_merge_finalize():
    try:
        print("Merge: Finalize target")
        m = "merge/"    ; open("CHECK-start-finalize","w").close()
        for f in ("/info", "/manifest.z"):
            if not resume or exists(m+target+f+".tmp"): replace(m+target+f+".tmp", m+merge_target+f)
        if not resume or not exists(target):         replace("merge/"+merge_target, target)
        if not resume or exists("volinfo.tmp"):      replace("volinfo.tmp", "volinfo")
        if not resume or exists("archive.ini.tmp"):  replace("archive.ini.tmp", "../archive.ini")
        if not exists(target):   raise FileNotFoundError(target)
        fssync(".")
    except Exception as err:
        print(err)    ; sys.exit(70)
    shutil.rmtree("merge", ignore_errors=True)
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

## MAIN ##
cmd = sys.argv[1]   ; msync = "--sync" in sys.argv   ; magic  = b"\xff\x11\x15"
tmpdir = os.path.dirname(os.path.abspath(sys.argv[0]))  
exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
if cmd in ("receive","dedup") and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
else:
    lstf = None

if cmd == "merge":
    src_list = []    ; resume = "--resume" in sys.argv    ; mtpath = sys.argv[2]
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    if resume and not (exists("merge.lst.gz") and exists("merge")):
        print("Error: Remote dir not initialized.")
        sys.exit(50 if exists(mtpath) else 40)
    lstf = gzip.open("merge.lst.gz", "rt")
    merge_target, target = lstf.readline().split()
    while True:
        ln = lstf.readline().strip()
        if ln == "###":  break
        src_list.append(ln)
    if "--finalize" not in sys.argv:
        helper_merge()
    catch_signals()
    helper_merge_finalize()
elif cmd == "receive":
    helper_receive(lstf if lstf else sys.stdin)
elif cmd == "send":
    helper_send()
elif cmd == "dedup":
    ddcount = 0    ; substitutions = {}
    for line in lstf:
        source, dest = line.split()   ; src_orig = source    ; deststat = os.stat(dest)
        ddcount += deststat.st_size
        if source in substitutions:   source = substitutions[source]
        if os.stat(source).st_ino != deststat.st_ino:
            try:
                os.link(source, dest+"-lnk")    ; replace(dest+"-lnk", dest)
            except OSError as err:
                if err.errno == 31:
                    # source has too many links; substitute
                    substitutions[src_orig] = dest   ; ddcount -= deststat.st_size   ; continue
                else:
                    if exists(dest+"-lnk"):   remove(dest+"-lnk")
                    print(err)   ; raise err
    print(ddcount, "bytes reduced.")
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)
'''
        with open(path+"/dest_helper.py", "wb") as progf:
            progf.write(bytes(dest_program, encoding="UTF-8"))

    #####>  End dest_helper program  <#####


# Run system commands with pipes, without shell:
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'. 'inlines' can be a list-like collection of strings
# to be used as input instead of 'infile'.
# List of commands may include 'None' instead of a child list; these will be ignored.

def do_exec(commands, cwd=None, check=True, out="", infile="", inlines=[], text=False,
            pipefail=False, timeout=None, getctl=False):
    ftype   = "t" if text else "b"
    if issubclass(type(out), io.IOBase):
        outf = out
    else:
        outmode = "a" if out.startswith(">>") else "w"    ; out = out.lstrip(">>")
        if cwd and out and out[0] != "/":   out = pjoin(cwd,out)
        outfunc = gzip.open if out.endswith(".gz") else open
        outf    = outfunc(out, outmode+ftype) if out else SPr.DEVNULL

    if inlines:
        inf = SPr.PIPE  #io.StringIO("\n".join(inlines)+"\n")
    else:
        if cwd and infile and infile[0] != "/":   infile = pjoin(cwd,infile)
        infunc  = gzip.open if infile.endswith(".gz") else open
        inf     = infunc(infile, "r"+ftype) if infile else SPr.DEVNULL

    errf = open(tmpdir+"/err.log", "a")  ; print("--+--", file=errf)   ; err = None
    if debug:   print(commands, file=errf, flush=True)

    # Start each command, linking them via pipes
    commands = list(filter(None, commands))    ; procs = []    ; start_t = time.monotonic()
    for i, clist in enumerate(commands):
        p = SPr.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else SPr.PIPE,
                             stderr=errf)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)

    try:
        if inlines:   procs[0].communicate(("\n".join(inlines)+"\n").encode("UTF-8"),
                                            timeout=timeout)
    except SPr.TimeoutExpired:
        pass

    # Monitor and control processes
    def control_execs():
        while True:
            err = None    ; finish = to_flag = False
            for p1 in reversed(procs):
                retcode = p1.poll()
                if not finish and retcode is None:
                    try:
                        p1.communicate(timeout=2)
                    except SPr.TimeoutExpired:
                        to_flag = True
                        continue
                    retcode = p1.returncode   ; finish = True
                    if check and (retcode != 0):
                        err = p1              ; finish = True
                elif finish and retcode is None:
                    p1.terminate()
                    continue

            if err or not to_flag or (timeout and time.monotonic() - start_t > timeout):
                break
            else:
                yield err

    def close():
        rclist = [0] + list(filter(bool if pipefail else str, ( x.returncode for x in procs )))
        if check and any(rclist):
            raise SPr.CalledProcessError("Chain exited "+repr(rclist), commands)

        for f in [inf, outf, errf]:
            if type(f) is not int: f.close()

        return rclist

    if getctl:
        return control_execs, close, (inf, outf, errf)
    else:
        for ii in control_execs():   err = ii

    return close()[-1]


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Session objects.
# Returns False if local and dest hashes match.

def compare_files(arch, pathlist=[], volumes=[], sessions=[], clear=True, manifest=False):
    dest = arch.dest    ; cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions) and not x.meta_checked]
    realses   = [x for x in sessions if not x.meta_checked]
    if len(volumes)+len(sessions)+len(pathlist) == 0:   return False

    with open(cmp_list, "a") as flist:
        for pth in pathlist:
            #if not exists(pth):   raise FileNotFoundError(pth)
            print(pth, file=flist)
        for v in realvols:
            v.meta_checked = True    ; print(v.vid+"/volinfo", file=flist)
        for s in realses:
            s.meta_checked = True
            for sf in ["info"] + ["manifest.z"] if manifest else []:
                print(pjoin(s.volume.vid,s.name,sf), file=flist)

    do_exec([[CP.xargs, CP.sha256sum]], cwd=arch.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest.run([dest.cd + " && xargs sha256sum"],
             infile=cmp_list, out=tmpdir+"/compare-hashes.dest")
    # maybe switch cmp to diff/sha256sum if they are safe enough to read untrusted input
    files  = [tmpdir+"/compare-hashes.local", tmpdir+"/compare-hashes.dest"]
    result = do_exec([[CP.cmp] + files], check=False) > 0

    return result


## Enh: Create aset functions to supply meta filenames for objects

def update_dest(arch, pathlist=[], volumes=[], sessions=[], ext="", delete=False):
    lcd = arch.path    ; dest = arch.dest

    update_list = [x.volume.vid+"/"+x.name+"/manifest.z" for x in sessions] \
                + [x.volume.vid+"/"+x.name+"/info" for x in sessions] \
                + [x.vid+"/volinfo" for x in volumes] + pathlist

    if delete:
        assert not ext;  dest.run([dest.cd + " && xargs rm -f"], inlines=update_list)
        return

    do_exec([[CP.tar,"-cf","-","--no-recursion","--verbatim-files-from","--files-from", "-"],
             dest.run_args([dest.cd + "  && tar --no-same-owner -xf -"]
                                + [" && mv '"+x+ext+"' '"+x+"'" for x in update_list if ext])
            ], inlines=[x+ext for x in update_list], cwd=lcd)


def fetch_file_blobs(recv_list, recv_dir, dest, ext="", skip0=False, skip_exists=False,
                     verifier=None):

    magic = dest.magic    ; exists = os.path.exists
    recv_list = [(x,y) for x,y in recv_list if not skip_exists or not exists(recv_dir+"/"+x)]
    if not recv_list:   return []

    cmd = dest.run_args(
            [dest.cd
             +" && exec 2>>"+dest.dtmp+"/receive.log"
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ])
    recvp = SPr.Popen(cmd, stdout=SPr.PIPE, stdin=SPr.PIPE)
    recvp.stdin.write(("".join((x+"\n" for x,y in recv_list))).encode("UTF-8"))
    recvp.stdin.flush()    ; rc = recvp.poll()

    for fname, fsz in list(recv_list):
        if rc is not None:   raise RuntimeError("Process terminated early.")
        fpath = recv_dir+"/"+fname+ext
        if exists(fpath):   os.remove(fpath)
        assert recvp.stdout.read(3) == magic
        # Read chunk size
        untrusted_size = int.from_bytes(recvp.stdout.read(4),"big")
        if untrusted_size == 0 and skip0:   continue
        if not fsz >= untrusted_size > 0:
            raise BufferError("Bad file size "+str(untrusted_size))

        # Size is OK.
        size = untrusted_size
        # Read chunk buffer
        untrusted_buf = recvp.stdout.read(size)    ; rc  = recvp.poll()
        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))
        if verifier:   verifier.auth(untrusted_buf)

        os.makedirs(os.path.dirname(fpath), exist_ok=True)
        with open(fpath, "wb") as outf:   outf.write(untrusted_buf)

    return recv_list


# Prepare snapshots and check consistency with metadata:
# Normal use will have a snap1 snapshot of the volume already in place.  Here we
# create a fresh snap2 so 'update_delta_digest' can compare it to the older snap1
# and then rotate snap2 -> snap1.

def prepare_snapshots_lvm(storage, aset, datavols, monitor_only):

    incr_vols, complete_vols = [], []   ; lvols = storage.lvols

    for datavol in datavols:

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, options.voldesc)

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        vol   = aset.vols[datavol]
        l_vol = storage.new_vol_entry(datavol, vol.vid)   ; snap1, snap2 = l_vol.snap1, l_vol.snap2

        if not l_vol.exists():
            err_out("Warning: Local '%s' does not exist!" % datavol)
            continue

        if lvols[snap1].is_arch_member() == "false":
            if options.remap:
                l_vol.delete    ; print("  Removed mis-matched snapshot", datavol)
            else:
                print("  Skipping %s; snapshot is from a different archive." % datavol)
                continue

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing timestamps; a match means remap is unnecessary.
        if len(vol.sessions):
            mapfile = vol.mapfile()    ; s1tags = lvols[snap1].tags.split(",")

            if not lvols[snap1].exists() or not exists(mapfile)    \
            or os.stat(mapfile).st_mtime_ns != lvols[snap1].gettime() \
            or vol.last not in s1tags:

                # Handle inadvertant mapfile snapshot mis-match
                lvols[snap1].delete()
                print("  Removed mis-matched snapshot for", datavol)

            if not lvols[snap1].exists() and lvols[snap2].exists() \
            and vol.last in lvols[snap2].tags.split(",") and exists(mapfile)  \
            and os.stat(mapfile).st_mtime_ns == lvols[snap2].gettime():
                lvols[snap2].rename(snap1)
                print("  Recovered interrupted snapshot rotation:", datavol)
 
            #if lvols[snap1].exists() and "delta" in lvols[snap1].tags \
            #and (     vol.map_used() == 0 
                  #or (vol.map_used() > 0 and
                      #"delta-"+str(int(os.path.getmtime(mapfile))) not in lvols[snap1].tags
                      #)
                 #):
                ## Handle inadvertant mapfile snapshot mis-match
                #vol.init_deltamap()
                #lvols[snap1].delete()    ; print("  Removed mis-matched delta", snap1vol)

            if not exists(mapfile) and lvols[snap1].exists() \
            and vol.last in s1tags \
            and "delta" not in lvols[snap1].tags \
            and os.stat(mapfile).st_mtime_ns == lvols[snap1].gettime():
                # Latest session matches current snapshot; OK to make blank map.
                vol.init_deltamap()

        elif monitor_only:
            print("  Skipping %s; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            incr_vols.append(datavol)
        elif not monitor_only:
            print("  Re-mapping", datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping %s; No paired snapshot." % datavol)    ; continue

        # Make fresh snap2vol
        lvols[snap2].delete()
        tagopts = ["--addtag=wyng", "--addtag=arch-"+aset.uuid]
        if monitor_only:   tagopts.append("--addtag=delta")
        lvols[snap2].create(snapshotfrom=datavol, ro=True, addtags=tagopts)

    return incr_vols, complete_vols


def prepare_snapshots_reflink(storage, aset, datavols, monitor_only):

    incr_vols, complete_vols = [], []   ; lvols = storage.lvols

    for datavol in datavols:

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, options.voldesc)

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        vol   = aset.vols[datavol]
        l_vol = storage.new_vol_entry(datavol, vol.vid)   ; snap1, snap2 = l_vol.snap1, l_vol.snap2

        if not l_vol.exists():
            err_out("Warning: Local '%s' does not exist!" % datavol)
            continue

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing timestamps; a match means remap is unnecessary.
        if len(vol.sessions):
            mapfile = vol.mapfile()    ; lastses = vol.sessions[vol.last]

            if not exists(mapfile) and lvols[snap1].exists() \
            and lastses.gettime() == lvols[snap1].gettime()  \
            and lastses.volsize == lvols[snap1].getsize():
                # Create deltamap if its missing but datavol and lastses timestamps align
                vol.init_deltamap(timestamp=lvols[snap1].gettime())

            if exists(mapfile) and not lvols[snap1].exists() \
            and os.stat(mapfile).st_mtime_ns == l_vol.gettime() == lastses.gettime() \
            and lastses.volsize == l_vol.getsize():
                # Create snap1 from datavol if its missing but other timestamps align
                lvols[snap1].create(snapshotfrom=datavol, ro=True)
                # Avoid race condition
                if lvols[snap1].gettime() != lastses.gettime():   lvols[snap1].delete()

            if not lvols[snap1].exists() or not exists(mapfile) \
            or (os.stat(mapfile).st_mtime_ns != lvols[snap1].gettime()
                and os.stat(mapfile).st_blocks == 0):
                # Handle inadvertant mapfile snapshot mis-match
                lvols[snap1].delete()
                print("  Removed mis-matched snapshot for", datavol)

        elif monitor_only:
            print("  Skipping %s; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            incr_vols.append(datavol)
        elif not monitor_only:
            print("  Re-mapping", datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping %s; No paired snapshot." % datavol)    ; continue

        # Make fresh snap2vol
        lvols[snap2].delete()
        lvols[snap2].create(snapshotfrom=datavol, ro=True)

    return incr_vols, complete_vols


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(storage, aset, datavols):

    #Enh: construct 'poolset' as vg,pool pairs; change vgname handling to track 'apool'
    vgname   = storage.vgname.replace("-","--")    ; lvols = storage.lvols
    poolset  = set(lvols[x].pool_lv for x in datavols)
    assert aset.chunksize % storage.block_size == 0

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    catch_signals()
    for apool in sorted(poolset):
        poolname = apool.replace("-","--")
        try:
            storage.metadata_lock(apool)
            for datavol in datavols:
                lv = lvols[datavol]
                if lv.pool_lv != apool:   continue
                cmds = [[CP.thin_delta, "--metadata-snap",
                                        "--thin1=" + lvols[lv.snap1].thin_id,
                                        "--thin2=" + lvols[lv.snap2].thin_id,
                                        "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", r"^\s*<same .*\/>$"]
                        ]
                do_exec(cmds,  out=tmpdir+"/delta."+datavol)
        except Exception as e:
            err_out("ERROR running thin_delta process.")
            raise e
        finally:
            storage.metadata_unlock(apool)

    catch_signals(None)


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest_lvm(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = lvols[vol.name].snap1      ; snap2vol   = lvols[vol.name].snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    assert len(vol.sessions) and exists(vol.mapfile())
    assert storage.block_size == 512

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(tmpdir+"/delta."+datavol).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    dnewchunks  = isnew  = anynew = dfreedblocks = 0

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile(), "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // storage.block_size
        chunkblocks  = chunksize // storage.block_size
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)
        tally=0 ####

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                isnew = anynew = 1
                if delta.tag == "right_only":   tally -= blocklen
            elif delta.tag == "left_only":
                isnew = 0    ; dfreedblocks += blockend - blockbegin
                tally += blocklen
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm)
        if dnewchunks+dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())

    catch_signals()
    map_updated = dnewchunks+dfreedblocks+anynew+vol.map_used() > 0
    if monitor_only:
        t1   = lvols[snap2vol].gettime()
        tags = ["--addtag="+vol.last, "--addtag=delta-"+str(t1)]
        t2   = lvols[datavol].rotate_snapshots(rotate=map_updated, timestamp_path=vol.mapfile(),
                                               addtags=tags)
        assert t1 == t2
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if map_updated else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    catch_signals(None)
    return map_updated


def get_reflink_deltas(storage, aset, datavols):

    assert aset.chunksize % storage.block_size == 0
    blksz = str(storage.block_size)
    generation, stpath = storage.metadata_lock()    ; lvols = storage.lvols    ; procs=[]

    for vol in (aset.vols[x] for x in datavols):
        lv    = lvols[vol.name]
        dpath = tmpdir+"/delta."+vol.vid    ; cmds, devs = [], set()

        for side, snapname, outp in (("11", lv.snap1, ' > "'+dpath+'-p"'), ("22", lv.snap2, "")):
            if not exists(stpath+snapname):
                raise RuntimeError("Volume path '%s' does not exist." % fpath)
            # filefrag output: file extent ranges, their 'physical' block ranges, extent length
            # sort -m will merge this output from two vols (file extent #s are pre-sorted)
            # uniq -u will filter-out exact extent/phys/len matches between the two vols
            # The result will be a list of extent ranges that have changed.

            devs.add(os.stat(stpath+snapname).st_dev)
            cmds.append([
              [CP.env, "LC_ALL=''", CP.filefrag, "-vs", "-b"+blksz, stpath+snapname],
              # awk checks the expected filefrag layout and removes punctuation;
              # fields are arranged to complement sort & uniq field requirements.
              [CP.awk, r'BEGIN {PAT="^\\s*ext:\\s+logical_offset:\\s+physical_offset:\\s+length:"}'
                       r' NR==1, $0 ~ PAT {if ($0 ~ /\s0 extents found$/) {ZXT="Y";'
                       r' print "00 0 1 0 0 0 wyng_input"' +outp+ r'} HEADER=$0; next}'
                       r' END {if (!(ZXT=="Y" || HEADER ~ PAT && $0 ~ /extents? found$/)) exit 2}'
                       r' /^\s*[0-9]+:/ {gsub(/\.|:/, " ", $0);'
                                        r' print '+side+r', $2, $6, $3, $4, $5, $NF'+outp+r'}']
              ]
            )
        if debug:   print(devs, "\n", cmds)
        assert len(devs) == 1

        # Save output from first volume to a named pipe
        os.mkfifo(dpath+"-p")
        p11ctl, p11close, _na = do_exec(cmds[0], timeout=60, getctl=True)
        #Enh: maybe use dev+inode as fname

        # Pipe output from second volume to 'sort' and include pipe from first
        p22ctl, p22close, _na = do_exec(cmds[1] +
                             [ [CP.sort, "-s", "-k2n,2", dpath+"-p", "-"],
                               [CP.uniq, "-u", "-f1"],
                               [CP.gzip, "-2"]
                              ],
                                 out=dpath, timeout=60, getctl=True
                            )
        for ii in zipln(p11ctl(), p22ctl()):   errs = ii
        retcode = p22close() + p11close()
        os.remove(dpath+"-p")

    # Check here that Btrfs snapshot gen#s did not change.
    # If changed: retry, x_it(7), use remap mode or pause/cancel the Btrfs op.
    # This could be extended with a fallback mode that checks _get_btrfs_generation() directly
    # while doing one snap pair at a time to increase chances of success.
    if storage.metadata_unlock()[0] != generation:
        x_it(7, "fs gen ids differ.")


def update_delta_digest_reflink(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols              ; l_vol      = lvols[datavol]
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = l_vol.snap1                ; snap2vol   = l_vol.snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    assert len(vol.sessions) and exists(vol.mapfile())

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile(), "r+b") as bmapf, gzip.open(tmpdir+"/delta."+vol.vid, "r") as deltaf:
        snap_ceiling = max(snap1size, snap2size) // storage.block_size
        chunkblocks  = chunksize // storage.block_size
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)
        dnewchunks   = isnew  = anynew = dfreedblocks = highwater = tally = 0

        # Cycle through the merged 'filefrag' metadata, marking bits in bmap_mm as needed.
        # Entries carry a beginning block position and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for dln in deltaf:
            side, blockbegin, blocklen = map(int, dln.split()[:3])

            tally += blocklen if side == 11 else -blocklen
            blockend     = min(blockbegin + blocklen, snap_ceiling)
            if blockend <= highwater:   continue
            highwater    = blockend

            #if delta.tag in ("different", "right_only"):
            isnew = anynew = 1 #### FIX: reconcile with tally

            # Fix: compare '11' output with final output to find freed blocks

            #elif delta.tag == "left_only":
            #    isnew = 0    ; dfreedblocks += blockend - blockbegin
            #else: # superfluous tag
            #    continue

            # blockpos iterates over disk blocks.
            # block_size (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm)
        if dnewchunks+dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())

    catch_signals()
    map_updated = dnewchunks+dfreedblocks+anynew+vol.map_used() > 0
    if monitor_only:
        t = lvols[datavol].rotate_snapshots(rotate=map_updated, timestamp_path=vol.mapfile())
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if map_updated else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    catch_signals(None)
    return map_updated


# Reads addresses from manifest and marks corresponding chunks in a volume's deltamap.

def manifest_to_deltamap(volume, manifest, mapsize):
    aset = volume.archive
    with open(manifest, "r") as mf, \
         open(volume.mapfile(), "r+b") as bmapf:

        bmapf.truncate(mapsize)    ; bmapf.flush()
        bmap_mm = mmap.mmap(bmapf.fileno(), 0)       ; chunksize  = aset.chunksize
        for ln in mf:
            addr = int(ln.split()[1][1:], 16)        ; volsegment = addr // chunksize
            bmap_pos = volsegment // 8               ; bmap_mm[bmap_pos] |= 1 << (volsegment % 8)


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a blank deltamap and .tick snapshot to
# track changes. After an incremental send, snapshots are rotated and the deltamap is reset.
#
# Returns (int, int) representing an estimate of bytes sent and chunks freed.
# Bytes sent will always be >0 if nonzero chunks were added to the manifest.
# A result of (0, 0) means no change was detected or sent.

def send_volume(storage, vol, curtime, ses_tags, send_all, benchmark=False):

    aset        = vol.archive
    datavol     = vol.name                      ; dedup       = options.dedup
    snap2vol    = storage.lvols[storage.lvols[vol.name].snap2]
    snap2size   = snap2vol.getsize()
    bmap_size   = vol.mapsize(snap2size)        ; chunksize   = aset.chunksize
    chdigits    = max_address.bit_length()//4   ; chformat    = "x%0"+str(chdigits)+"x"
    bksession   = "S_"+curtime                  ; sdir        = pjoin(vol.vid, bksession)
    prior_size  = vol.volsize()                 ; prior_ses   = vol.last
    verbose     = not options.quiet             ; zeros       = bytes(chunksize)
    addrsplit   = -address_split[1]             ; lchunk_addr = vol.last_chunk_addr(snap2size)
    compress = compressors[aset.compression][2] ; compresslevel = int(aset.compr_level)

    if benchmark:   testtime = time.monotonic()
    def tar_add_pass(*args, **kwargs):   pass

    if len(vol.sessions):
        # Our chunks are usually smaller than LVM's, so generate a full manifest to detect
        # significant amount of unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(vol), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None
    fman_hash     = fman_fname = ""

    old_mapfile     = vol.mapfile()
    ses = vol.new_session(bksession, addtags=ses_tags)
    ses.localtime   = str(snap2vol.gettime())
    ses.volsize     = snap2size
    ses.permissions = storage.lvols[vol.name].getperms()
    ses.path        = vol.path+"/"+bksession+"-tmp"

    # Code from init_dedup_indexN() localized here for efficiency.
    dedup_idx     = dedup_db = None
    if dedup:
        hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w \
                  = aset.dedupindex
        chtree_max= 2**(chtree[0].itemsize*8)
        idxcount  = dataf.seek(0,2) // (ch_w+ses_w)    ; ddblank_ch = bytes(hash_w)
        ddsessions = aset.dedupsessions                ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(aset.path)    ; os.makedirs(sdir+"-tmp")
    do_exec([[CP.chattr, "+c", sdir+"-tmp"]], check=False)


    if aset.datacrypto:
        crypto   = True    ; crypto_cadence = aset.datacrypto.ctcadence
        encrypt  = aset.datacrypto.encrypt
    else:
        crypto   = False    ; etag     = b''     ; crypto_cadence = 0


    # Use tar to stream files to destination
    stream_started = False
    untar_cmd = [dest.cd + " && mkdir -p ./"+sdir
                 + " && exec >>"+dest.dtmp+"/send.log 2>&1"
                 + " && echo "+vol.vid
                 + " && python3 "+dest.dtmp+"/dest_helper.py send"
                 + (" --sync" if options.maxsync else "")]

    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(snap2vol.path,"rb", buffering=chunksize) as vf,    \
         open(sdir+"-tmp/manifest.tmp", "wt") as hashf,            \
         open("/dev/zero" if send_all else old_mapfile,"rb") as bmapf:

        vf_seek = vf.seek; vf_read = vf.read   ; BytesIO = io.BytesIO
        gethash = aset.getdatahash             ; b64enc  = base64.urlsafe_b64encode
        b2int   = int.from_bytes               ; islice  = itertools.islice
        compare_digest = hmac.compare_digest

        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        zdelim  = bytes(64)    ; zdlen = len(zdelim)*8      ; minibmap = None   ; bmap_list = []
        addr    = counter = percent = bcount = ddbytes = 0  ; checkpt = 128     ; bmsz = zdlen*50

        while addr < snap2size:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  list(islice(fullmanifest, zdlen))
                addr += chunksize*zdlen
                minibmap = bmap_list.pop(0)
            elif not send_all:
                # Get more: split(zdelim) shows where large unmodified zones exist.
                bmap_list.extend(bmapf.read(bmsz).split(zdelim))
                minibmap = bmap_list.pop(0)

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, snap2size if send_all
                        else min(snap2size, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = chformat % addr

                if fullmanifest:
                    try:
                        fman_hash, fman_fname = fullmanifest_readline().split()
                    except ValueError: # EOF
                        fullmanifest.close()   ; os.remove(fullmanifest)
                        fullmanifest = None    ; fman_hash = ""
                    else:
                        if fman_fname != destfile:
                            raise ValueError("expected manifest addr %s, got %s"
                                            % (destfile, fman_fname))

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                # Fetch chunk as buf
                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Process checkpoint
                if counter > checkpt:
                    # Keep updating aset counters if key has a low cadence
                    if 0 < crypto_cadence < 101 and not benchmark:
                        aset.save_conf()    ; tarf_add(aset.confname)
                    # Show progress.
                    if verbose:
                        percent = int(addr/snap2size*1000)
                        print("\x0d  %4.1f%% %7dM" % (percent/10, bcount//1000000),
                              end="", flush=True)
                    counter = 0

                # Compress & write only non-empty chunks
                if buf == zeros:
                    if fman_hash == "0":   continue
                    b64hash = "0"
                else:
                    # Compress chunk and hash it
                    buf    = compress(buf, compresslevel)
                    bhashb = gethash(buf)   ; b64hash = b64enc(bhashb).decode("ascii")
                    # Skip when current and prior chunks are the same
                    if compare_digest(fman_hash, b64hash):  continue

                # Start tar stream
                if not stream_started:
                    untar = SPr.Popen(dest.run_args(untar_cmd),
                            stdin =SPr.PIPE,    stdout=SPr.DEVNULL,
                            stderr=SPr.DEVNULL)
                    tarf = tarfile.open(mode="w|", fileobj=untar.stdin)
                    tarf_addfile = tar_add_pass if benchmark else tarf.addfile
                    tarf_add     = tar_add_pass if benchmark else tarf.add
                    TarInfo  = tarfile.TarInfo         ; LNKTYPE = tarfile.LNKTYPE
                    tar_info = TarInfo(sdir+"-tmp")    ; tar_info.type  = tarfile.DIRTYPE
                    tarf_addfile(tarinfo=tar_info)     ; stream_started = True

                # Add entry to new manifest
                print(b64hash, destfile, file=hashf)
                if b64hash == "0":   continue

                # Add buffer to stream
                tar_info = TarInfo("%s-tmp/%s/%s" % (sdir, destfile[1:addrsplit], destfile))

                # If chunk already in archive, link to it
                if dedup:
                    i      = b2int(bhashb[:ht_ksize], "big")
                    pos    = hashtree[i].find(bhashb)     ; ddses = None
                    if pos % hash_w == 0:
                        data_i = chtree[i][pos//hash_w]   ; dataf.seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[b2int(dataf.read(ses_w),"big")]
                        if ddses is None:
                            hashtree[i][pos:pos+hash_w] = ddblank_ch # zero-out obsolete entry
                        else:
                            ddchx = dataf.read(ch_w).hex().zfill(chdigits)
                            tar_info.type = LNKTYPE
                        dataf.seek(0,2)
                    if ddses is None and bhashb != ddblank_ch and idxcount < chtree_max:
                        hashtree[i].extend(bhashb)   ; chtree[i].append(idxcount)   ; idxcount += 1
                        dataf.write(ses_index.to_bytes(ses_w,"big") + addr.to_bytes(ch_w,"big"))

                if tar_info.type == LNKTYPE:
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.vid,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)

                else:
                    # Encrypt the data chunk
                    if crypto:
                        etag, buf = encrypt(buf, bhashb)

                    # Send data chunk to the archive
                    fileobj = BytesIO()
                    tar_info.size = fileobj.write(etag) + fileobj.write(buf)   ; fileobj.seek(0)
                    tarf_addfile(tarinfo=tar_info, fileobj=fileobj)
                    bcount += len(buf)    ; counter += 1

            # Advance addr, except when minibmap is zero len.
            if minibmap or send_all:  addr += chunksize

    print("\r  100% ", ("%8.1fM  |  %s" % (bcount/1000000, datavol)),
          ("\n  (reduced %0.1fM)" % (ddbytes/1000000)) if ddbytes and options.verbose else "",
          end="")

    if benchmark:   print("\nTime:", time.monotonic() - testtime)

    # Send session info, end stream and cleanup
    if fullmanifest:   fullmanifest.close()   ; os.remove(fullmanifest.name)
    if stream_started and not benchmark:
        # Save session info
        if crypto:   aset.datacrypto.save_counter()
        ses.save_info(ext=".tmp")
        for f in ("manifest.z","info"):
            fpath = sdir+"-tmp/"+f         ; tarf_add(fpath+".tmp", arcname=fpath)
        tarf_add(vol.vid+"/volinfo.tmp")   ; tarf_add(aset.confname+".tmp")

        tarf.close()    ; untar.stdin.close()
        try:
            untar.wait(timeout=60)
        except SPr.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 99
            untar.kill()
        else:
            retcode = untar.poll()

        if retcode != 0:
            raise RuntimeError("tar transport failure code %d" % retcode)

        if ses.volsize != prior_size and len(vol.sessions) > 1:
            os.link(ses.path+"/manifest.tmp", ses.path+"/manifest")
            check_manifest_sequence(vol, vol.sesnames)

        # Finalize on VM/remote
        catch_signals()
        dest.run([ dest.cd
                 +" && mv -T "+sdir+"-tmp "+sdir
                 +" && mv "+vol.vid+"/volinfo.tmp "+vol.vid+"/volinfo"
                 +" && mv archive.ini.tmp archive.ini"
                 +(" && ( nohup sync -f . 2&>/dev/null & )" if options.maxsync else "")
                 ], trap=True)

        # Local finalize
        ses.path = ses.path.rsplit("-tmp",maxsplit=1)[0]   ; os.replace(ses.path+"-tmp", ses.path)
        ses.rename_saved(ext=".tmp")
        if old_mapfile and exists(old_mapfile):   os.remove(old_mapfile)
        fssync(vol.path)

    else:
        if stream_started:   tarf.close(); untar.kill()
        catch_signals()
        vol.delete_session(bksession)    ; shutil.rmtree(aset.path+"/"+sdir+"-tmp")

    if not benchmark:
        vol.init_deltamap()
        storage.lvols[vol.name].rotate_snapshots(rotate=True, timestamp_path=vol.mapfile(),
                                                 addtags=["--addtag="+vol.last])
    catch_signals(None)
    if dedup and debug:   show_mem_stats()

    return stream_started + bcount


# Build deduplication hash index and list

def init_dedup_index(aset, listfile=""):

    ctime      = time.perf_counter()    ; makelist = bool(listfile)
    addrsplit  = -address_split[1]
    # Define arrays and element widths
    hash_w     = hash_bits // 8
    ht_ksize   = 2 # binary digits for tree key
    hashtree   = [bytearray() for x in range(2**(ht_ksize*8))]
    chtree     = [array("I") for x in range(2**(ht_ksize*8))]
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2

    # Create master session list, limit to ses_w range
    for vol in aset.vols.values():   aset.dedupsessions += vol.sessions.values()
    aset.dedupsessions.sort(key=lambda x: x.name, reverse=True)
    ddsessions = aset.dedupsessions[:2**(ses_w*8)-(len(aset.vols))-1]

    dataf  = open(tmpdir+"/hashindex.dat","w+b")
    dataf_read  = dataf.read    ; dataf_seek = dataf.seek      ; int_frbytes = int.from_bytes
    dataf_write = dataf.write   ; bfromhex = bytes().fromhex   ; b64dec = base64.urlsafe_b64decode
    if makelist:   dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = match = 0
    for sesnum, ses in enumerate(ddsessions):
        vol = ses.volume    ; vid = vol.vid    ; sesname = ses.name
        vol.decode_one_manifest(ses)
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":   continue
                bhashb = b64dec(ln1)
                i      = int_frbytes(bhashb[:ht_ksize], "big")
                pos    = hashtree[i].find(bhashb)
                if pos % hash_w == 0:
                    match += 1
                    if makelist:
                        data_i = chtree[i][pos//hash_w]
                        dataf_seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[int_frbytes(
                                 dataf_read(ses_w),"big")]
                        ddchx  = dataf_read(ch_w).hex().zfill(chdigits)
                        print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                            (ddses.volume.vid, ddses.name, ddchx[:addrsplit], ddchx,
                            vid, sesname, ln2[1:addrsplit], ln2),
                            file=dedupf)
                        dataf_seek(0,2)
                elif count < chtree_max:
                    hashtree[i].extend(bhashb)
                    chtree[i].append(count)
                    dataf_write(sesnum.to_bytes(ses_w,"big"))
                    dataf_write(bfromhex(ln2[1:]))  # Enh: scale no. digits to match vol size
                    count += 1

    if listfile:   dedupf.close()

    aset.dedupindex    = (hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w)
    aset.dedupsessions = ddsessions

    if not debug:  return
    print("\nIndexed in %.1f seconds." % (time.perf_counter()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d, matches: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count, match)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


# Deduplicate data already in archive

def dedup_existing(aset):

    print("Building deduplication index...", end="")
    init_dedup_index(aset, "dedup.lst.gz")
    dest = aset.dest

    print(" linking...", end="", flush=True)
    do_exec( [dest.run_args([dest.cd
               +" && /bin/cat >"+dest.dtmp+"/dest.lst.gz"
               +" && /usr/bin/python3 "+dest.dtmp+"/dest_helper.py dedup"
               ]),
              [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]
            ], infile=tmpdir+"/dedup.lst.gz", out=tmpdir+"/arch-dedup.log")
    print(" done.")
    if options.verbose:   print("".join(open(tmpdir+"/arch-dedup.log","r")))


# Controls flow of monitor and send_volume procedures:

def monitor_send(storage, aset, datavols, monitor_only):

    dest = aset.dest    ; storage.check_support()

    if options.autoprune.lower() == "full":
        for vol in aset.vols:   autoprune(vol, apmode="full")

    curtime = time.strftime("%Y%m%d-%H%M%S", time.localtime(time_start))

    print("\nPreparing snapshots...")
    incrementals, send_alls \
        = storage.prep_snapshots(storage, aset, datavols, monitor_only)
    storage.update_vol_list([(x.name, x.vid) for x in aset.vols.values()])

    if monitor_only:   send_alls.clear()

    if len(incrementals)+len(send_alls) == 0:
        x_it(0, "No new data.")

    # Process session tags
    ses_tags = []
    if options.tag and options.tag == [""] and not options.unattended and not monitor_only:
        print("Enter tag info as 'tagID[, tag description]'. Blank to end input.")
        while ans := ask_input("[%d]: " % (len(ses_tags)+1)).strip():
            tag = ArchiveSession.tag_parse(ans, delim=",")
            if not tag:   continue
            ses_tags.append(tag)
            if len(ses_tags) == ArchiveSet.max_tags:   break
        print(len(ses_tags), "tags total.")
    elif options.tag and not monitor_only:
        for tag_opt in options.tag:
            tag = ArchiveSession.tag_parse(tag_opt, delim=",")
            if not tag:   raise ValueError("Invalid tag "+tag_opt)
            ses_tags.append(tag)

    if len(incrementals) > 0:
        if verbose:   print("Acquiring deltas.")
        storage.acquire_deltas(storage, aset, incrementals)

    if not monitor_only:
        cmpvols = [x for x in aset.vols.values()
                         if x.name in incrementals + send_alls and x.sessions]
        cmpses  = [v.sessions[v.sesnames[-1]] for v in cmpvols]
        if compare_files(aset, volumes=cmpvols, sessions=cmpses):
            x_it(1, "Error: Local and archive metadata differ.")

        print("\nSending backup session %s to '%s'." % (curtime, dest.spec))

    for datavol in incrementals + send_alls:
        vol = aset.vols[datavol]    ; updated = False
        print(" ", "Scan" if monitor_only else "Send", "volume      | ", datavol, flush=True,end="")

        if datavol in incrementals:
            updated = storage.process_deltas(storage, aset, datavol, monitor_only)

        if monitor_only:   continue

        if datavol in send_alls or updated:
            if options.dedup and not aset.dedupindex:
                init_dedup_index(aset)

            if vol.changed_bytes > dest.free:
                # Enh: add loop here for all volumes, implement a 'forced' mode
                autoprune(vol, needed_space=vol.changed_bytes, apmode=options.autoprune)
                if vol.changed_bytes > dest.free:
                    print(" %d additional bytes needed." % (vol.changed_bytes-dest.free))
                    print("Insufficient space on destination %d; Skipping." % dest.free)
                    error_cache.append(datavol)
                    continue

            dnew   = send_volume(storage, vol, curtime, ses_tags, send_all=datavol in send_alls)
            dest.free -= int(dnew + (dnew * 0.05))

        else:
            storage.lvols[vol.name].rotate_snapshots(rotate=False)    ; dnew = 0

        print("\r" if dnew else "\r  No changes   ", flush=True)


# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS or ^tagname format.

def prune_sessions(volume, times):

    sessions = volume.sesnames
    t1, t2   = "", ""                ; to_prune  = []

    if len(sessions) < 2:    print("  No extra sessions to prune.")    ; return

    # Validate date-time params
    for pos, dt in enumerate(times[:]):
        if not dt[0].startswith("^"):
            if not dt.startswith("S_"):   times[pos] = "S_"+dt.strip()
            datetime.datetime.strptime(times[pos][2:], "%Y%m%d-%H%M%S")
        elif dt[1:] not in volume.tags:
            print(" No match for", dt)    ; return
        elif pos == 0:
            for sesname in sessions:
                if dt[1:] in volume.sessions[sesname].tags:
                    if len(times) == 1 and not options.allbefore:
                        to_prune.append(sesname)
                    else:
                        t1 = sesname   ; break
        elif pos == 1:
            for sesname in reversed(sessions):
                if dt[1:] in volume.sessions[sesname].tags:   t2 = sesname   ; break

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    if options.allbefore:   t1 = sessions[0]    ; t2 = times[0]
    if not t1:   t1 = times[0]
    if not t2 and len(times) > 1:
        t2 = times[1]
        if t2 <= t1:  x_it(1, "Error: Second date-time must be later than first.")

    # Find specific sessions to prune in contiguous range
    if to_prune:
        pass

    elif t2 == "":
        # find single session
        if t1 in sessions:   to_prune.append(t1)

    else:
        # find sessions in a date-time range
        start = len(sessions)   ; end = 0
        if t1 in sessions:
            start = sessions.index(t1)
        else:
            for ses in sessions:
                if ses > t1:   start = sessions.index(ses)    ; break
        if t2 in sessions:
            end = sessions.index(t2)+1
        else:
            for ses in reversed(sessions):
                if ses < t2:   end = sessions.index(ses)+1    ; break
        to_prune = sessions[start:end]

    if len(to_prune) and to_prune[-1] == sessions[-1]:
        print("  Preserving latest session.")
        del(to_prune[-1])
    if len(to_prune) == 0:
        print("  No selections in this date-time range.")
        return

    autoprune(volume, apmode=options.autoprune, include=set(to_prune))


# Parameters / vars for autoprune:
# oldest (date): date before which all sessions are pruning candidates
# thin_days (int): number of days ago before which the thinning params are applied
# ndays & nsessions: a days/sessions ratio for amount of sessions left after thinning
# nthresh: min number of sessions to prune this time (0 = prune all candidates)
# target_size (0 or MB int): User-selected size cap for archive (future)

def autoprune(vol, needed_space=0, apmode="off", include=set()):

    dtdate = datetime.date
    def to_date(sesdate):
        return dtdate(int(sesdate[2:6]),int(sesdate[6:8]),int(sesdate[8:10]))

    if len(vol.sesnames) < 2:   return False
    datavol   = vol.name                      ; dest    = vol.archive.dest
    apmode    = apmode.lower()                ; exclude = set()
    sessions  = vol.sesnames[:-1]             ; marked  = 0
    today     = dtdate.today()                ; oldest  = today-datetime.timedelta(days=366)
    thin_days = datetime.timedelta(days=32)   ; ndays   = 7      ; nsessions = 2
    startdate = to_date(sessions[0])          ; nthresh = 3 if apmode == "on" else 0
    enddate   = min(to_date(sessions[-2]), today-thin_days) if len(sessions) > 2 else None

    # Make a 2d array of ordinal dates and populate with session id + flag
    apcal = { day: [] for day in range(dtdate(startdate.year,1,1).toordinal(),
                                       to_date(sessions[-1]).toordinal()+ndays) }
    for ses in sessions:  apcal[to_date(ses).toordinal()].append([ses, True])

    # Build set of excluded sessions
    for sx in options.keep:
        if not sx.startswith("^"):
            datetime.datetime.strptime(sx, "%Y%m%d-%H%M%S")    ; exclude.add("S_"+sx)
        else:
            if sx[1:] == "all":
                exclude += {x.name for x in vol.sessions if x.tags}
            else:
                exclude += {x.name for x in vol.sessions if sx[1:] in x.tags}
    include -= exclude

    # Mark all sessions prior to oldest date setting, plus include list
    for ses in sessions:
        sdate = to_date(ses)
        if (apmode != "off" and sdate <= oldest) or ses in include:
            for dses in apcal[sdate.toordinal()]:
                if dses[0] == ses and ses not in exclude:
                    dses[1] = False    ; vol.sessions[ses].toggle = False

    # Mark sessions for thinning-out according to ndays + nsessions
    if apmode != "off" and (needed_space or apmode != "min") and enddate:
        for year in range(startdate.year, enddate.year+1):
            for span in range(dtdate(year,1,1).toordinal(),
                            min(dtdate(year,12,31), enddate).toordinal(), ndays):
                dlist = []    ; offset = 0
                for day in range(span, min(span+ndays, enddate.toordinal())):
                    dlist.append(sum( x[1] for x in apcal[day] ))
                while sum(dlist) > nsessions: ## Enh: Make even distribution
                    bigday = dlist.index(max(dlist[offset:]), offset)
                    offset += (ndays//nsessions)+1    ; offset %= min(ndays, len(dlist))
                    for dses in apcal[span+bigday]:
                        if dses[1]:
                            # always decr bigday, but don't toggle if session is excluded
                            dlist[bigday] -= 1
                            dses[1] = vol.sessions[dses[0]].toggle = dses[0] in exclude
                            break

    # Find contiguous marked ranges and merge/prune them. Repeat until free >= needed space.
    factor = 1    ; sessions.append("End")
    while True:
        to_prune = []    ; removed_ct = 0    ; skipped = False
        for ses in sessions:
            if ses is None:   continue
            if ses == "End" or vol.sessions[ses].toggle :
                if to_prune:
                    # prioritize ranges that overlap with requested includes
                    if include and not (set(to_prune) & include):
                        to_prune.clear()    ; skipped = True    ; continue

                    target_s = vol.sesnames[vol.sesnames.index(to_prune[-1]) + 1]
                    merge_sessions(vol, to_prune, target_s, clear_sources=True)

                    for i in to_prune:   sessions[sessions.index(i)] = None
                    include -= set(to_prune)    ; removed_ct += len(to_prune);   to_prune.clear()
                    if not include and nthresh and removed_ct >= nthresh*factor:  break # for ses
            else:
                to_prune.append(ses)

        if removed_ct:
            if options.verbose:   print(datavol+": Removed", removed_ct)
            dest.get_free(tmpdir+"/merge.log")    ; os.remove(tmpdir+"/merge.log")

        if skipped and apmode != "off":   continue # while
        if removed_ct == 0 or nthresh == 0 or needed_space <= dest.free:
            break # while
        elif factor > 4:
            nthresh = 0
        else:
            factor += 2

    return True


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session name.

def merge_manifests(volume, msessions=None, mtarget=None, addcol=False, rdiff=[]):
    # Enh: implement mtarget to support merge_sessions()
    aset      = volume.archive                    ; slist  = []
    msessions = msessions or volume.sesnames
    sespaths  = [ os.path.basename(volume.sessions[x].path) for x in msessions ]
    tmp       = aset.big_tmpdir if volume.volsize() > 128000000000 else tmpdir
    outfile   = tempfile.NamedTemporaryFile(dir=tmp, prefix="mout_", delete=False)

    if not aset.dedupsessions:   volume.decode_manifests(msessions)
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, prefix="sl_", delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(sespaths)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir_obj = tempfile.TemporaryDirectory(dir=tmp, prefix="m_")
        cdir     = cdir_obj.name    ; slsort  = slist[1]

        # fix: extrapolate path with filename
        do_exec([[CP.xargs, "-0", "-a", slist[0],
                  CP.awk, r'{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                            +cdir+r'/"FILENAME}']], cwd=volume.path)
    else:
        cdir  = volume.path     ; slsort  = slist[0]

    cmds = [[CP.sort, "-umsd", "-k2,2", "--batch-size=16", "--files0-from="+slsort]]

    if rdiff:
        # merge a different set of manifests, use their address column to filter the main merge:
        mfilter = merge_manifests(volume, msessions=list(reversed(rdiff)), addcol=False)
        cmds.append([CP.sort, "-msd", "-k2,2", mfilter, "-"])
        cmds.append([CP.awk, r'{if (PREV==$2) print $0} {PREV=$2}'])

    do_exec(cmds, out=outfile.file, cwd=cdir)

    for f in slist:  os.remove(f)
    if rdiff:        os.remove(mfilter)
    if addcol:       cdir_obj.cleanup()
    return outfile.name


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume, source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(volume, sources, target, clear_sources=False):

    aset       = volume.archive    ; dest = aset.dest    ; resume = bool(aset.in_process)
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    chformat   = "x%0"+str(chdigits)+"x"
    m_tmp      = tmpdir if volume.volsize() < 128000000000 else aset.big_tmpdir

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)         ; destvol       = "/"+volume.vid

    if not resume:
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max(x.volsize for x in volume.sessions.values()
                                    if x.name in sources)
        last_chunk = chformat % volume.last_chunk_addr(volsize)
        lc_filter  = '"'+last_chunk+'"'

        with open("merge.lst", "wt") as lstf:
            print(merge_target, target, file=lstf)
            volume.decode_one_manifest(volume.sessions[merge_target])

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                volume.decode_one_manifest(volume.sessions[ses])
                do_exec([[CP.sed, "-E", r"s|$| "+ses+r"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umsd", "-k2,2", "--batch-size=16"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umsd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]],
                out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, r"$2<="+lc_filter+r" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, r"$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                r"s|^0 x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"-rm " +merge_target+ r"/\1/x\1\2|; t; "

                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"rename \3/\1/x\1\2 " +merge_target+ r"/\1/x\1\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.lst")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, r"$2>"+lc_filter, m_tmp+"/manifest.two"],
                     [CP.sed, "-E", r"s|^\S+\s+x(\S{" + str(address_split[0]) + r"})(\S+)|"
                                    r"-rm " + merge_target + r"/\1/x\1\2|"]
                    ], out=">>merge.lst")

        do_exec([[CP.gzip, "-f", "merge.lst"]])

    if not resume:
        # Set archive in_process state to "merge"
        aset.set_in_process(["merge", volume.name, str(clear_sources), target, sources],
                            tmp=False, todest=False)

    # Update & send new metadata and process lists to dest
    if clear_sources:
        for ses in sources:
            if ses in volume.sessions:   volume.delete_session(ses, remove=False)

    cmds = []    ; dest_cmds = dest.cd + destvol
    if not resume:
        aset.set_in_process(None, tmp=True, todest=False)
        volume.sessions[target].save_info(".tmp")
        cmds += [CP.tar, "-cf", "-", "../archive.ini", "../archive.ini.tmp", "merge.lst.gz",
                            target+"/manifest.z.tmp", target+"/info.tmp", "volinfo.tmp"]
        dest_cmds += " && tar --no-same-owner -xmf -"

    # Start merge operation on dest
    retcode = do_exec([cmds, dest.run_args([dest_cmds + " && python3 "
                        + dest.dtmp+"/dest_helper.py merge "+merge_target
                        + (" --resume" if resume else "") + (" --sync" if options.maxsync else "")
                         ])
                        ], cwd=volume.path, check=False, out=tmpdir+"/merge.log"
                      )

    catch_signals()
    if retcode == 50:
        # Initialization didn't complete, so reload aset and abort
        aset = ArchiveSet(aset.path, aset.dest, prior_auth=aset)
        aset.set_in_process(None)
        for f in ("merge.lst.gz", "volinfo.tmp", "vi.dat.tmp", target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest.run([dest.cd + destvol + " && rm -rf merge merge.lst.gz"], check=False)
        x_it(1, "Error: Merge could not initialize!")
    elif retcode != 0:
        x_it(retcode, "Error: Remote exited!")

    # Local finalize
    volume.sessions[target].rename_saved(ext=".tmp")
    aset.set_in_process(None, save=False)
    catch_signals(None)

    # Check consistency after resuming merge
    if resume and compare_files(aset, volumes=[volume], sessions=[volume.sessions[target]],
                                manifest=True):
        x_it(1, "Error: Local and dest metadata differ.")

    os.remove("merge.lst.gz")
    for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(storage, vol, select_ses="", save_path="", diff=False, verify_only=0):

    def diff_compare(dbuf,z):
        if dbuf != volf.read(chunksize):
            if remap:
                volsegment = addr // chunksize 
                bmap_pos = volsegment // 8
                bmap_mm[bmap_pos] |= 1 << (volsegment % 8)
            return len(dbuf)
        else:
            return 0

    dest        = (aset := vol.archive).dest      ; magic        = dest.magic
    verbose     = not options.quiet and verify_only != 2 ; debug = options.debug
    attended    = not options.unattended          ; remap        = options.remap
    sparse      = options.sparse                  ; sparse_write = options.sparse_write or sparse
    chunksize   = aset.chunksize                  ; use_snapshot = False
    sessions    = vol.sesnames                    ; zeros        = bytes(chunksize)
    # functions
    compress    = compressors[aset.compression][2]; compresslevel = int(aset.compr_level)
    decompress  = compressors[aset.compression][0].decompress
    decrypt     = aset.datacrypto.decrypt if aset.datacrypto else None
    compare_digest = hmac.compare_digest          ; b64enc    = base64.urlsafe_b64encode
    gethash     = aset.getdatahash

    if diff or verify_only:
        save_path = ""

    # Set the session to retrieve
    if select_ses:
        if select_ses[0] == "^":
            # match tag to session id
            tag = select_ses[1:]
            if tag in vol.tags:
                select_ses = sorted(vol.tags[tag])[-1]
                print("Matched tag to", select_ses)
        else:
            # validate date-time input
            datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
            select_ses = "S_"+select_ses

        if select_ses not in sessions:
            err_out(f"No volume {vol.name} in session {select_ses}.")
            return None

    elif len(sessions) > 0:
        # default to last session
        select_ses = sessions[-1]
    else:
        err_out("No sessions available.")
        return None

    if diff and remap and select_ses != sessions[-1]:
        err_out("Cannot use prior session for remap.")
        return None

    ses_obj     = vol.sessions[select_ses]           ; volsize     = ses_obj.volsize
    chdigits    = max_address.bit_length() // 4      ; chformat    = "x%0"+str(chdigits)+"x"
    lchunk_addr = vol.last_chunk_addr(volsize)       ; last_chunkx = chformat % lchunk_addr
    addrsplit   = -address_split[1]                  ; rc = l_vol  = volf = None

    # Prepare save volume
    if not (diff or verify_only):

        # Decode save path semantics
        if save_path:
            save_storage = None       ; returned_home = False
            save_type, lvpool, lpath = LocalStorage.parse_local_path(os.path.dirname(save_path))
            if save_type not in (None,"block device"):
                save_storage = LocalStorage((lvpool, lpath), auuid=aset.uuid,
                                            arch_vols=storage.arch_vols)
        else:
            save_storage = storage    ; returned_home = storage.online
            save_type = "logical volume" if storage.pooltype=="tlvm" else "file"
            l_vol = save_storage.new_vol_entry(vol.name, vol.vid)    ; save_path = l_vol.path

        if exists(save_path) and attended:
            print("\n!! This will", "overwrite" if sparse_write else "erase all",
                "existing data in",save_path,"!!")
            ans = ask_input("   Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:
                return None

        # possibly use snapshot as baseline for receive
        if returned_home and options.use_snapshot \
        and (snap_lv := save_storage.lvols[l_vol.snap1]).is_paired(vol.mapfile(), vol.last):
            print("Using snapshot as baseline.")   ; assert l_vol.path == save_path
            sparse_write = use_snapshot = True    ; sparse = False
            l_vol.delete(force=True)
            l_vol.create(snapshotfrom=snap_lv.name, ro=False)

        elif save_type == "logical volume":
            if not l_vol.exists():
                print("Creating '%s' in thin pool %s[%s]." %
                      (l_vol.name, save_storage.path, save_storage.lvpool))
                # Fix:  translate to safe lvm name
                l_vol.create(volsize, ro=False)    ; sparse_write = sparse = False

            elif l_vol.getsize() != volsize:
                if debug:   print("Re-sizing LV to %d bytes." % volsize)
                l_vol.resize(volsize)

        if save_type in("logical volume","block device") and exists(save_path):
            punch_hole = save_storage.block_discard_chunk
            if not sparse_write:   do_exec([[CP.blkdiscard, save_path]])
            volf = open(save_path, "r+b")

        elif os.path.abspath(save_path).startswith("/dev/"):
            x_it(1, "Cannot create new volume from ambiguous /dev path.\n"
                    " Please create the volume before using 'receive', or specify"
                    " --save-to=volgroup/pool/volname in case of a thin LVM volume.")
        else:
            save_type = "file"   ; punch_hole = save_storage.file_punch_hole
            if not exists(save_path) or not sparse_write:
                open(save_path, "wb").close()    ; sparse_write = sparse = False

            volf = open(save_path, "r+b")    ; volf.truncate(volsize)

    elif diff:
        l_vol = storage.lvols[vol.name]     ; snap1vol = l_vol.snap1
        if not l_vol.exists():
            err_out("Local volume must exist for diff.")
            return None
        if remap:
            raise NotImplementedError()
            #lv_remove(vgname, snap1vol)
            #do_exec([[CP.lvm,"lvcreate", "-pr", "-kn", "-ay", "--addtag=wyng",
                    #"--addtag="+vol.last, "--addtag=delta",
                    #"--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n", snap1vol]])
            #print("  Initial snapshot created for", datavol)
            #get_lvm_vgs(aset.vgname)
            #if not exists(vol.mapfile()):
                #vol.init_deltamap()
            #bmapf = open(vol.mapfile(), "r+b")
            #bmapf.truncate(vol.mapsize())    ; bmapf.flush()
            #bmap_mm = mmap.mmap(bmapf.fileno(), 0)
        else:
            if storage.lvols[snap1vol].exists():
                l_vol = storage.lvols[snap1vol]
            else:
                print("Snapshot not available; Comparing with source volume instead.")

            if volsize != l_vol.getsize():
                err_out("Volume sizes differ:\n  Archive = %d \n  Local   = %d"
                        % (volsize, l_vol.getsize()))   ; return None

        volf  = open(l_vol.path, "r+b")

    if volf:
        volf_read = volf.read     ; volf_write = volf.write    ; volf_seek = volf.seek
        volfno    = volf.fileno() ; fcntl.lockf(volf, fcntl.LOCK_EX|fcntl.LOCK_NB)

    if verbose:
        print("\nReceiving" if save_path else "\nVerifying", "volume:", vol.name, select_ses[2:])
        if save_path:    print("Saving to %s '%s'" % (save_type, save_path))

    # Collect session manifests
    diff_ses = []
    if use_snapshot:
        # sessions after selected, except if last ses (then we're already done)
        if select_ses == vol.last:
            print("Snapshot retrieved...Done.")
            return volsize
        incl_ses = sessions[:sessions.index(select_ses)+1]
        diff_ses = sessions[sessions.index(select_ses)+1:]
    elif verify_only == 2:
        # only selected session
        incl_ses = [select_ses]
    else:
        # everything upto & including selected
        incl_ses = sessions[:sessions.index(select_ses)+1]

    # Merge manifests and send to archive system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    vol.decode_manifests(incl_ses+diff_ses, force=not aset.just_fetched)
    manifest = merge_manifests(vol, msessions=incl_ses, addcol=True, rdiff=diff_ses)

    if not sparse:
        cmds = [[CP.sed, "-E", r"/" +last_chunkx+ r"/q", manifest],  ## Enh: detect vol_shrank
                [CP.sed, "-E", r"/^0\s/ d; "
                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|\3/\1/x\1\2|;"],
                [CP.gzip, "-c", "-4"
                ],
                dest.run_args(["cat >"+dest.dtmp+"/dest.lst.gz"]),
            ]   # Enh: replace list with range
        do_exec(cmds)

    # Create retriever process using py program
    cmd = dest.run_args(
            [dest.cd + "/"+vol.vid
             +" && exec 2>>"+dest.dtmp+"/receive.log"
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ])
    getvol   = SPr.Popen(cmd, stdout=SPr.PIPE,
                                     stdin =SPr.PIPE if sparse else SPr.DEVNULL)
    gv_stdin = io.TextIOWrapper(getvol.stdin, encoding="utf-8") if sparse else None

    # Open manifest then receive, check and save data
    addr = bcount = diff_count = 0    ; buf = b''
    for mfline in open(manifest, "r"):
        if addr >= lchunk_addr:   break
        cksum, faddr, ses = mfline.split()    ; addr = int(faddr[1:], 16)

        if verbose:   print("%.2f%%" % (addr/volsize*100), end="\x0d")

        # Process zeros quickly
        if cksum == "0":
            if save_path:
                volf_seek(addr)
                if sparse_write and volf_read(chunksize) != zeros:
                    # Note: punch_hole() might not work, write zeros first anyway
                    volf_seek(addr)    ; volf_write(zeros)    ; diff_count += chunksize
                    punch_hole(volfno, addr, chunksize)
            elif diff:
                volf_seek(addr)    ; diff_count += diff_compare(zeros,True)
                if diff_count and not remap:   break

            continue

        # Request chunks on-demand if local chunk doesn't match cksum
        if sparse and save_path:
            volf_seek(addr)
            if b64enc(gethash(compress(volf_read(chunksize),
                                       compresslevel))).decode("ascii") == cksum:
                continue
            else:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)

        # Read chunk size
        assert getvol.stdout.read(3) == magic
        untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

        # allow for slight expansion from compression algo
        if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
            raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

        ##  Size is OK  ##
        size = untrusted_size

        # Read chunk buffer
        untrusted_buf = getvol.stdout.read(size)
        rc  = getvol.poll()
        if rc is not None and len(untrusted_buf) == 0:
            break

        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))


        # Decrypt the data chunk
        # Validation MUST be next step!
        if decrypt:
            untrusted_buf = decrypt(untrusted_buf)

        # Validate data chunk
        if not compare_digest(cksum, b64enc(gethash(untrusted_buf)).decode("ascii")):
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            print(size, mfline)
            raise ValueError("Bad hash "+faddr+" :: "+str(b64enc(gethash(untrusted_buf))))

        ##  Buffer is OK  ##
        buf = untrusted_buf   ; bcount += len(buf)

        if verify_only:   continue

        # Proceed with decompress.
        decomp = decompress(buf)
        if len(decomp) != chunksize and addr < lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        if addr == lchunk_addr and len(decomp) != volsize - lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        buf = decomp

        # Write data to volume (receive)
        if save_path:
            volf_seek(addr)
            # Don't re-check buffer for sparse mode, check for sparse_write:
            if sparse:
                volf_write(buf)    ; diff_count += len(buf)
            elif sparse_write:
                if use_snapshot or volf_read(chunksize) != buf:
                    volf_seek(addr)    ; volf_write(buf)    ; diff_count += len(buf)
            else:
                volf_write(buf)
        elif diff:
            volf_seek(addr)    ; diff_count += diff_compare(buf,False)
            if diff_count and not remap:   break


    if rc is not None and rc != 0:
        raise RuntimeError("Error code from getvol process: "+str(rc))

    if ((not sparse and verify_only != 2 and not use_snapshot and not diff) \
    or (diff and remap)) and addr+len(decompress(untrusted_buf)) != volsize:
        err_out("Received range %d does not match volume size %d."
                % (addr+len(decompress(untrusted_buf)), volsize))
        return None

    print("OK" if not diff_count else f"Diff bytes: {diff_count}", end="")
    print(f" Data bytes: {bcount}", f"/ {addr+len(buf)}" if verify_only != 2 else "")

    if save_path:
        volf.flush()    ; os.fsync(volf.fileno())    ; volf.close()
        storage.setperms(save_path, ses_obj.permissions)
        if is_num(ses_obj.localtime):   storage.settime(save_path, int(ses_obj.localtime))
        if returned_home and select_ses == sessions[-1] and storage.pooltype == "rlnk":
            vol.init_deltamap()
            tags = ["--addtag=wyng", "--addtag=arch-"+aset.uuid, "--addtag="+sessions[-1]]
            save_storage.lvols[l_vol.snap2].delete()
            save_storage.lvols[l_vol.snap2].create(snapshotfrom=vol.name, addtags=tags)
            l_vol.rotate_snapshots(timestamp_path=vol.mapfile())
            print("  Snapshot paired for", vol.name)
    if remap:
        bmapf.close()
        if diff_count > 0 and options.action != "send":
            print("\nNext 'send' will bring this volume into sync.")
    elif diff and diff_count:
        return None

    os.remove(manifest)
    return bcount


# Rename a volume in the archive

def rename_volume(storage, aset, oldname, newname):

    os.chdir(aset.path)    ; vol = aset.vols[oldname]
    if not aset.rename_volume_meta(oldname, newname, ext=".tmp"):
        x_it(1, "Error: Cannot rename '%s' to '%s'." % (oldname,newname))

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    vol.rename_saved(ext=".tmp")
    catch_signals(None)

    # move snapshots to new pathname
    new_lvol = storage.new_vol_entry(newname, vol.vid)    ; old_lvol = storage.lvols[oldname]
    for atr in ("snap1","snap2"):
        oldsnap, newsnap = storage.lvols[getattr(old_lvol, atr)], \
                           storage.lvols[getattr(new_lvol, atr)]
        newsnap.delete()
        if new_lvol.exists():
            if oldsnap.exists():   oldsnap.rename(newsnap.name)
        else:
            oldsnap.delete()


def add_volume(aset, datavol, desc):
    #if not storage.online or not exists(storage.lvols[datavol].path):
        #print("Warning:", datavol, "not found.")
    vol = aset.add_volume_meta(datavol, desc=desc, ext=".tmp")
    if not vol:   return

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    vol.rename_saved(ext=".tmp")
    catch_signals(None)

    #if lv_exists(aset.vgname, datavol+".tick") \
    #and "arch-"+aset.uuid not in volgroups[aset.vgname].lvs[datavol+".tick"].tags:
        #sys.stderr.write("Warning: Volume '%s' is already tracked"
                            #"by a Wyng snapshot from a different archive!\n" % datavol)


# Remove a volume from the archive

def delete_volume(storage, aset, dv):

    if not options.unattended and not aset.in_process:
        print("\nWarning! Delete will remove ALL metadata AND archived data",
              "for volume", dv)

        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:   x_it(0,"")
        print()

    print("Deleting volume", dv, "from archive.")
    catch_signals()
    if not aset.in_process:       aset.set_in_process(["delete", dv])
    dvid = aset.delete_volume_meta(dv)
    update_dest(aset, pathlist=[aset.confname])
    aset.set_in_process(None)

    if dvid:
        aset.dest.run([aset.dest.cd + "  && rm -rf '%s'" % dvid])
    catch_signals(None)

    # remove snapshots
    for lvol_name in (storage.lvols[dv].snap1, storage.lvols[dv].snap2):
        storage.lvols[lvol_name].delete()
    return


# Remove Wyng snapshots and metadata from local system
#   archive == None: purge all wyng snapshots
#   archive != None and purge_other: purge snapshots except those linked to archive
#   archive != None and not purge other: purge only snapshots linked to archive

def remove_local_metadata(storage, archive, purge_other=False):
    assert not purge_other or (archive and purge_other)

    # Remove LVM snapshots
    for lv in (x for vg in storage.vgs_all.values() for x in vg.values()):
        if lv.name.endswith(lv.snap_ext) and "wyng" in lv.tags.split(","):

            if not archive or (lv.is_arch_member() == "false" and purge_other) \
            or (archive and lv.is_arch_member() == "true" and not purge_other):
                lv.delete()
                if verbose:   print("Removed", fpath)

    # Remove reflink snapshots by scanning local dir
    if storage.pooltype == "rlnk":
        exts = ReflinkVolume.snap_ext    ; lvols = storage.lvols
        if archive:
            archpaths = [lvols[getattr(lvols[x], y)].path for x in archive.vols.keys()
                                                          for y in ("snap1","snap2")]
        for ldir, _, files in os.walk(storage.path):
            for fpath in (ldir+"/"+x for x in files if x.endswith(exts)):
                if not archive or (purge_other and fpath not in archpaths):
                    os.remove(fpath)
                    if verbose:   print("Removed", fpath)

    # Remove metadata dir(s)
    if not archive:
        shutil.rmtree(cachedir, ignore_errors=True)
    #elif purge_other:


def show_list(aset, selected_vols):

    if aset.dest.archive_ini_hash == "none":   print("(CACHED)")
    if options.verbose:
        print("\nArchive Settings:")
        for key, val in aset.conf["var"].items():
            print(" %-15s = %s" % (key, val))

    # Print list of sessions grouped by tag. First, organize by tag:
    if options.tag:
        print("\nTag Assignments")    ; ltags = {}
        for dv in selected_vols or datavols:
            vol = aset.vols[dv]
            for tag, tses in vol.tags.items():
                #tset = ( dv+" / "+x[2:] for x in tses )
                tset = ( (dv, x[2:], vol.sessions[x].tags[tag] ) for x in tses )
                if tag not in ltags:
                    ltags[tag] = list(tset)
                else:
                    ltags[tag] += tset
        # Print result:
        for tag in sorted(ltags):
            print("\n"+tag+":")
            for ses in sorted(ltags[tag]):   print(" ", ses)
        return

    # Print list of volumes if no volume is selected.
    if not aset.vols:
        print("\nNo volumes.")    ; return
    elif not selected_vols and not len(options.volumes):
        print("\nVolumes:")
        maxwidth = max(len(x.name) for x in aset.vols.values())
        fmt    = "%7.2f GB  %s  %-" + str(maxwidth+2) + "s  %s (%3d)"
        for vname in sorted(x.name for x in aset.vols.values()):
            vol = aset.vols[vname]   ; sname = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            if options.verbose:
                print(fmt % ((vol.volsize() / 1024**3), vol.vid, vname, sname, len(vol.sessions)))
                if vol.desc:   print("  desc:", vol.desc)
            else:
                print("",vname)
        return

    # Print list of sessions grouped by volume.
    # Get the terminal column width and set cols to number of list columns.
    ttycols = os.popen('stty size', 'r').read().split()[1]
    cols = max(4, min(10, int(ttycols)//17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        if not aset.vols[dv].sessions:   print("None.")    ; continue
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for sname in vol.sesnames + [""]:
            month = sname[2:8]    ; ses = vol.sessions[sname] if sname else None
            if options.unattended or options.verbose:
                # plain listing
                print(sname[2:])
                if ses and options.verbose:
                    for tag in sorted(ses.tags.items()):
                        print(" tag:", tag[0]+(", "+tag[1] if tag[1] else ""))
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(sname)
            else:
                # print the month listing: 'rows' is adjusted to carry the remainder on
                # additional lines. 'extra' and 'steps' are used to eliminate ragged
                # column on the right (a ragged row is more pleasing to the eye).
                size  = len(slist)    ; rows = size//cols
                rows += (size%rows)//cols if rows else 0     ; extra = size - cols*rows
                heights = [rows+(x<extra) for x in range(cols) ]

                # output one row at a time
                for ii in range(max(heights)):
                    print("  ".join(  slist[ii+sum(heights[:c])][2:] for c in range(cols)
                                       if ii < heights[c]  ))
                print()

                # start new month list
                lmonth = month    ; slist = [sname]


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def is_num(val):
    try:
        float(val)
    except:
        return False
    else:
        return True


def os_kill(pid, sig=signal.SIGTERM):
    try:
        os.kill(pid, sig)
    except ProcessLookupError:
        pass


def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1","USR2"], iflag=False):
    # Remove existing handlers
    for sig in list(signal_handlers):   signal.signal(sig, signal_handlers.pop(sig))
    # Set handler on requested signals
    for sig in (getattr(signal,"SIG"+x) for x in (sel or [])):
        signal_handlers[sig] = signal.getsignal(sig)   ; signal.signal(sig, handle_signal)
        signal.siginterrupt(sig, iflag)

    while not sel and signals_caught:   os.kill(os.getpid(), signals_caught.pop(0))


def handle_signal(signum, frame):
    sys.stderr.write(" *** Caught signal "+str(signum))
    if signum == signal.SIGALRM:  raise TimeoutError("SIGALRM")
    if signum not in signals_caught:   signals_caught.append(signum)


def ask_input(text):
    sys.stderr.write(text)
    return input()


def show_dest_free(dest):
    print("\n%dMB free on destination." % (dest.free//1024//1024))


# Exit with simple message
def x_it(code, text):
    err_out(text)
    sys.exit(code)


def err_out(text):
    sys.stdout.write(text+"\n")


@atexit.register
def cleanup():
    if debug:
        shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
        shutil.move(tmpdir, "/tmp/"+prog_name+"-debug")
    try:
        if not debug:
            if dest:   dest.remove_dtmp()
            if aset:   shutil.rmtree(aset.big_tmpdir, ignore_errors=True)
            if tmpdir and exists(tmpdir):   shutil.rmtree(tmpdir, ignore_errors=True)
    except:
        pass
    if error_cache:
        err_out("Error on volume(s): " + ", ".join(error_cache))
        sys.exit(2)




##  MAIN  #########################################################################################

# Constants / Globals
prog_name             = "wyng"
prog_version          = "0.4alpha3"     ; prog_date = "20230612"
format_version        = 3               ; debug     = False       ; tmpdir = None
admin_permission      = os.getuid() == 0
aset = dest = storage = None
time_start, monotonic_start = time.time(), time.monotonic()


if sys.hexversion < 0x3080000:
    x_it(1, "Python ver. 3.8 or greater required.")

# Allow only one instance at a time
lockpath = "/var/lock/"+prog_name
try:
    lockf = open(lockpath, "w")
    fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
except PermissionError:
    x_it(1, "ERROR: No writing permission on %s.  Run %s as root." % (lockpath,prog_name))
except IOError:
    x_it(1, "ERROR: "+prog_name+" is already running.")

cpu_flags = [x for x in open("/proc/cpuinfo") if x.startswith("flags")] [0].split()[1:]

max_address           = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split         = [len(hex(max_address))-2-7, 7]
hash_bits             = 256    ; hash_bytes   = hash_bits // 8

os.environ["LC_ALL"]  = "C"    ; shell_prefix = "set -e && export LC_ALL=C\n"

pjoin       = os.path.join     ; exists = os.path.exists   ; zipln = itertools.zip_longest


## Parse Arguments ##

local_actions         = ("monitor","version")
write_actions         = ("add","send","prune","delete","rename","arch-delete","arch-deduplicate")
dest_actions          = ("list","receive","verify","diff","arch-check","arch-init") + write_actions

parser = argparse.ArgumentParser(description="")
parser.add_argument("action", choices=local_actions+dest_actions, help="Action to take")
parser.add_argument("-u", "--unattended", action="store_true", default=False,
                    help="Non-interactive, supress prompts")
parser.add_argument("--pass-agent", type=int, default=0,
                    help="Minutes to remember passphrase")
parser.add_argument("--all-before", dest="allbefore", action="store_true", default=False,
                    help="Select all sessions before --session date-time.")
parser.add_argument("--all", action="store_true", default=False)
parser.add_argument("--session", help="YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS] or ^tag_id"
                                 " select session date|tag, singular or range.")
parser.add_argument("--tag", action="append",
                    help="tag_id[,description] Add tags when sending")
parser.add_argument("--keep", action="append", default=[],
                    help="YYYYMMDD-HHMMSS or ^tag_id exclude session by date|tag (prune)") 
parser.add_argument("--volex", action="append", help="Exclude volume")
parser.add_argument("--autoprune", default="off", help="Automatic pruning")
parser.add_argument("--save-to", dest="saveto", default="",help="Path to store volume for receive")
parser.add_argument("--sparse", action="store_true", default=False, help="Retrieve differences only")
parser.add_argument("--sparse-write", action="store_true", default=False, help="Save differences only")
parser.add_argument("--use-snapshot", action="store_true", default=False,
                    help="Use local snapshot as baseline for receive")
parser.add_argument("--remap", action="store_true", default=False, help="Remap snapshots")
parser.add_argument("--dest", default="",     help="URL to archive")
parser.add_argument("--dest-name", "-n", default="", help="Nickname for dest location")
parser.add_argument("--local", default="",    help="Init: LVM vg/pool containing source volumes")
parser.add_argument("--encrypt", default=None, help="Encryption mode")
parser.add_argument("--compression", default="", help="Init: compression type:level")
parser.add_argument("--hashtype", default="", help="Init: hash function type")
parser.add_argument("--chunk-factor", dest="chfactor", type=int,
                    help="Init: set chunk size to N*64kB")
parser.add_argument("--meta-dir", dest="metadir", default="", help="Use alternate metadata path")
parser.add_argument("--force", action="store_true", default=False, help="For arch-delete")
parser.add_argument("--clean", action="store_true", default=False, help="Clean snapshots, metadata")
parser.add_argument("--maxsync", action="store_true", default=False, help="Use more fs sync")
parser.add_argument("--debug", action="store_true", default=False, help="Debug mode")
parser.add_argument("--quiet", action="store_true", default=False)
parser.add_argument("--verbose", action="store_true", default=False)
parser.add_argument("--dedup", "-d", action="store_true", default=False,
                    help="Data deduplication (send)")
parser.add_argument("--volume-desc", dest="voldesc", default="",
                    help="Set volume description (add, rename, send)")
parser.add_argument("volumes", nargs="*")
options = parser.parse_args()    ; options.action = options.action.lower()

# Set stdout to devnull if --quiet is specified
debug = options.debug    #; assert not (options.quiet and (options.verbose or debug))
verbose = options.verbose or debug
if debug:   options.verbose = True
if options.quiet and options.action not in ("list","version"):
    options.unattended = True
    sys.stdout = open(os.devnull, "w")

print("%s %s release %s" % (prog_name.capitalize(), prog_version, prog_date), flush=True)

if options.action in write_actions+("arch-init","monitor") and not admin_permission:
    x_it(1, "Must be root/admin user for %s." % options.action)
if options.action == "version":   x_it(0,"")
if options.action not in ("send", "arch-deduplicate"):   options.dedup = False
if options.action == "arch-deduplicate":   options.dedup = True
options.unattended = options.unattended or not sys.stdin.isatty()


## General Configuration ##

signal_handlers  = {}    ; signals_caught = []    ; error_cache = []

# vardir  : holds data not directly related to an archive, such as nicknames for archive URLs.
# cachedir: holds cached archive metadata.
vardir      = "/var/lib/"+prog_name
cachedir    = options.metadir if options.metadir else "/var/cache/"+prog_name
tmpdir      = tempfile.mkdtemp(prefix=prog_name, dir="/tmp")

os.makedirs(vardir, exist_ok=True)
shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
os.makedirs(tmpdir+"/rpc")
Destination.write_helper_program(tmpdir+"/rpc")
agent_helper_write(tmpdir)

# Dict of compressors in the form: (library, default_compress_level, compress_func)
compressors      =    {"zlib":   (zlib, 4, zlib.compress),
                       "bz2" :   (bz2,  9, bz2.compress)}
if zstd:   compressors["zstd"] = (zstd, 3, lambda data, lvl: zstd.compress(data, lvl, 3))

hash_funcs       = {"hmac-sha256": None,
                    "sha256"     : hashlib.sha256,
                    "blake2b"    : lambda x: hashlib.blake2b(x, digest_size=hash_bytes).digest()}


# Create ArchiveSet and Destination objects with get_configs().
# Passphrase input happens here; no stdin before this point!
aset    = get_configs(options)    ; dest = aset.dest
require_online = options.action in ("monitor","send","receive","diff") and not options.saveto
storage = LocalStorage((None, options.local) if options.local else (aset.vgname, aset.poolname),
                       require_online=require_online,
                       arch_vols=[(x.name, x.vid) for x in aset.vols.values()],
                       auuid=aset.uuid)


# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.
if aset.in_process and dest.online  \
    and options.action not in ("arch-init","arch-delete","list"):

    if (options.action == "delete" and aset.in_process[1] == options.volumes[0]) \
        or (options.clean and options.force):
        # user is currently deleting the in_process volume or using --clean --force
        aset.set_in_process(None)
    elif aset.in_process:
        open(aset.path+"/in_process_retry","w").close()
    elif exists(aset.path+"/in_process_retry"):
        x_it(1, "Interrupted process already retried; Exiting.")

    if aset.in_process and aset.in_process[0] in ("delete","merge"):
        print("Completing prior operation in progress:", " ".join(aset.in_process[0:2]))

        if aset.in_process[0] == "delete":
            vname = aset.in_process[1]
            delete_volume(storage, aset, vname)
        elif aset.in_process[0] == "merge":
            vname = aset.in_process[1]
            merge_sessions(aset.vols[vname], aset.in_process[4].split(":|"),
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))
    else:
        raise ValueError("Bad in_process descriptor: "+repr(aset.in_process[0]))

        aset = ArchiveSet(aset.path, aset.dest, allvols=True, prior_auth=aset)
        if compare_files(aset, pathlist=[aset.confname], volumes=[aset.vols[vname]]):
            x_it(1, "Error: Local and archive metadata differ.")

# Display archive update time as local time
arch_dt_raw = datetime.datetime.fromtimestamp(float(aset.updated_at), datetime.timezone.utc)
arch_dt     = arch_dt_raw.astimezone().isoformat(sep=u" ")

print("Encrypted" if (aset.mcrypto and aset.datacrypto) else "Un-encrypted",
      "archive:", dest.spec,
      "\nLast updated %s (%s)" % (arch_dt[:-6], arch_dt[-6:]), flush=True)

# Check volume args against config
exclude_vols  = set(options.volex or [])
datavols      = sorted(set(aset.vols.keys())  - exclude_vols)
selected_vols = options.volumes[:]
for vol in selected_vols[:]:
    if selected_vols.count(vol) > 1:
        selected_vols.remove(vol)
    if vol not in aset.vols and options.action not in {"add","rename","send"}:
        print("Volume "+vol+" not configured; Skipping.")
        selected_vols.remove(vol)


## Process Commands ##

if options.action   == "monitor":
    monitor_send(storage, aset, selected_vols or datavols, monitor_only=True)


elif options.action == "send":
    monitor_send(storage, aset, selected_vols or datavols, monitor_only=False)


elif options.action == "prune":
    if options.autoprune.lower() == "off" and not options.session:
        x_it(1, "Must specify --autoprune or --session for prune.")
    dvs = selected_vols or datavols

    if not options.unattended and len(dvs):
        print("This operation will delete session(s) from the archive;")
        ans = ask_input("Are you sure? [y/N]: ").strip()
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    for dv in dvs:
        if dv in datavols:
            if options.session:
                prune_sessions(aset.vols[dv], options.session.split(","))
            elif options.autoprune.lower() in ("on","min","full"):
                autoprune(aset.vols[dv], apmode=options.autoprune)


elif options.action == "receive":
    if len(selected_vols) != 1 and options.saveto:
        x_it(1, "Specify one volume for receive --save-to.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify only one session for receive.")

    for dv in selected_vols:
        count = receive_volume(storage, aset.vols[dv], select_ses=options.session or "",
                                        save_path=options.saveto or "")
        if count is None:   error_cache.append(dv)


elif options.action == "verify":
    if not (loopvols := datavols if options.all else selected_vols):
        x_it(1, "Volume name(s) required for verify.")

    for dv in loopvols:
        count = receive_volume(storage, aset.vols[dv],
                    select_ses="" if not options.session else options.session.split(",")[0],
                    verify_only=1, save_path="")
        if count is None:   error_cache.append(dv)


elif options.action == "diff":
    if not (loopvols := datavols if options.all else selected_vols):
        x_it(1, "Volume name(s) required for diff.")

    for dv in loopvols:
        count = receive_volume(storage, aset.vols[dv], save_path="", diff=True)
        if count is None:   error_cache.append(dv)


elif options.action == "list":
    show_list(aset, datavols if options.all else selected_vols)


elif options.action == "add":
    if len(options.volumes) < 1:
        x_it(1, "Volume name(s) required for add.")

    for dv in options.volumes:
        if dv not in aset.vols:   add_volume(aset, dv, options.voldesc)


elif options.action == "rename":
    if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
    rename_volume(storage, aset, options.volumes[0], options.volumes[1])


elif options.action == "delete":
    if options.clean:
        print("Remove local Wyng metadata from system for path",
              aset.path if options.all else aset.path)
        if not options.unattended:
            ans = ask_input("Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:
                x_it(0,"")
        elif not options.force:
            x_it(1, "Ignoring --clean without --force.")
        remove_local_metadata(storage, None if options.all else aset)

    elif len(options.volumes) != 1:
        x_it(1, "Specify one volume to delete.")

    elif options.volumes[0] not in aset.vols:
        x_it(1, "Volume not found.")

    else:
        delete_volume(storage, aset, selected_vols[0])


elif options.action == "arch-init":
    pass


elif options.action == "arch-check":
    arch_check(storage, aset, selected_vols or datavols)


elif options.action == "arch-delete":
    raise NotImplementedError()


elif options.action == "arch-deduplicate":
    show_dest_free(aset.dest)
    dedup_existing(aset)


## END ##
