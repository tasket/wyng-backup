#!/usr/bin/env python3
# editor width: 100   -----------------------------------------------------------------------------


###  Wyng â€“ The Logical Volume Backup Tool
###  Copyright Christopher Laprise 2018-2024 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.
###  Permission to redistribute under similar naming "Wyng", "wyng backup", etc.
###  is granted only for un-modified code (GPLv3 S.7-c).


# Return codes:
# 1 - Fatal error
# 2 - Error on specific volume(s)
# 3 - Auth error
# 4 - Timeout
# 5 - Destination not ready
# 6 - Archive not found
# 7 - Local storage not ready
# 8 - Signal


import sys, signal, os, stat, shutil, subprocess as SPr, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, hashlib, hmac, functools, uuid
import getpass, base64, platform, resource, itertools, string, struct
import xml.etree.ElementTree, json, ctypes, ctypes.util, atexit
from array import array   ; from urllib.parse import urlparse   ; from math import ceil

try:
    import zstd, warnings
    # Required for crippled zstd library
    warnings.filterwarnings("ignore", category=DeprecationWarning)
except:
    zstd = None

try:
    from Cryptodome.Cipher import ChaCha20 as Cipher_ChaCha20
    from Cryptodome.Cipher import ChaCha20_Poly1305 as Cipher_ChaCha20_Poly1305
    from Cryptodome.Random import get_random_bytes
    import Cryptodome, Cryptodome.Protocol.KDF, Cryptodome.Hash.SHA512
except:
    Cryptodome = Cipher_ChaCha20 = Cipher_ChaCha20_Poly1305 = None


# ArchiveSet manages archive metadata incl. volumes and sessions

class ArchiveSet:

    confname      = "archive.ini"   ; saltname    = "archive.salt"
    max_volumes   = 1024            ; vid_len     = 10
    min_chunksize = 0x10000         ; mhash_sz    = 44            ; comment_len = 100
    max_conf_sz   = (vid_len + comment_len + mhash_sz + 25) * max_volumes + 1000

    attr_ints   = {"format_ver","chunksize","mci_count","dataci_count"}

    def __init__(self, top, dest, opts, ext="", allvols=False, children=2,
                 pass_agent=0, passphrase=None, prior_auth=None):
        self.time_start      = time_start
        self.monotonic_start = monotonic_start
        self.dest        = dest
        self.opts        = opts
        self.path        = top
        self.confpath    = pjoin(self.path, self.confname)
        self.saltpath    = pjoin(self.path, self.saltname)
        self.confprefix  = b"[WYNG%02d]\n" % format_version
        self.modeprefix  = b"ci = "
        self.mcrypto     = mcrypto = None
        self.datacrypto  = datacrypto = None
        self.vols        = {}
        self.dedupindex  = {}
        self.dedupsessions = []
        self.raw_hashval = None
        self.apdays      = None
        self.big_tmpdir  = self.path+"/.tmp"
        self.subdir      = None
        self.gethash     = hash_funcs["blake2b"]
        self.autoreduce  = children > 1

        # persisted:
        self.format_ver  = 0
        self.chunksize   = self.min_chunksize * 2
        self.compression = "zstd" if zstd else "zlib"
        self.compr_level = str(compressors[self.compression][1])
        self.hashtype    = "hmac-sha256"
        self.uuid        = None
        self.updated_at  = None
        self.ci_mode     = None
        self.mci_count   = self.dataci_count  =  0
        self.in_process  = []

        shutil.rmtree(self.big_tmpdir, ignore_errors=True)
        os.makedirs(self.big_tmpdir, exist_ok=True)

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"], cp["volumes"], cp["in_process"]  =  {},{},{}

        # halt configuration if this is a new or temp config
        os.makedirs(self.path, exist_ok=True)
        if not exists(self.confpath+ext):
            self.uuid = str(uuid.uuid4())
            return

        # decode archive.ini file
        # Format "magic" is "[WYNGvv]\n" where vv = format version: 9 bytes
        # Format mode is next as "ci = m0\n" where m = cipher mode: 8 bytes
        with open(self.confpath+ext, "rb") as f:
            self.header = (header0 := f.read(9)) + (header1 := f.read(8))
            buf = f.read(self.max_conf_sz)
        self.raw_hashval = hashlib.sha256(header0 + header1 + buf).hexdigest()
        if not (header0 == self.confprefix and header1.startswith(self.modeprefix)):
            if header0.startswith(b"[var]"):
                self.format_ver = 2
                return
            else:
                raise ValueError("Not a Wyng format.")

        # use existing auth if specified
        if isinstance(prior_auth, ArchiveSet):
            ci = prior_auth.ci_mode
            self.mcrypto    = mcrypto = prior_auth.mcrypto
            self.datacrypto = datacrypto = prior_auth.datacrypto
        else:
            ci = header1[-3:-1].decode("ASCII")

        # parse metadata crypto mode and instantiate it
        if ci != "00" and not mcrypto:
            ci_types = DataCryptography.crypto_codes[ci]
            if debug:   print("metadata cipher =", ci_types[1])

            # access a key agent, if available for this UUID and archive URL
            agent_name = "wyng-agent-" + str(os.getuid()) + hashlib.blake2b(
                           bytes(self.dest.spec, encoding="UTF-8"), digest_size=20).hexdigest()
            agentkeys  = agent_get(agent_name, pass_agent)

            # get passphrase if we got no agentkeys
            if not passphrase and not agentkeys:   passphrase = get_passphrase(cmd=opts.passcmd)
            if passphrase:
                assert passphrase != bytes(len(passphrase))
            elif not agentkeys:
                raise ValueError("No authentication input.")
            passphrase_s1 = passphrase[:] if passphrase else None

            # create both crypto instances and load metadata key
            self.datacrypto = datacrypto = DataCryptography()
            self.mcrypto    = mcrypto    = DataCryptography()
            mcrypto.load(ci_types[1], self.saltpath, slot=1,
                         passphrase=passphrase_s1, agentkeys=agentkeys)

        # initial decryption + auth of archive.ini (root body)
        try:
            if mcrypto:   buf = mcrypto.decrypt(buf)
        except ValueError as e:
            if "MAC check failed" in e.args:
                x_it(3, "Error: Could not decrypt/authenticate archive.\n")
            raise e

        # read attributes from archive.ini
        if debug:   print(gzip.decompress(buf).decode("UTF-8"))
        cp.read_string(gzip.decompress(buf).decode("UTF-8"))
        for key, value in cp["var"].items():
            setattr(self, key, int(value) if key in self.attr_ints else value)
        self.in_process  = [ ln for ln in cp["in_process"].values() ]

        if self.ci_mode and self.ci_mode != ci:
            raise ValueError("Header ci_mode mismatch: %s != %s" % (ci, self.ci_mode))
        else:
            self.ci_mode = ci

        if self.format_ver > format_version or not self.format_ver:
            x_it(1,"Archive format ver = "+str(self.format_ver)+
                                ". Expected = "+str(format_version))

        # set final arch_init properties
        # use blake2b for metadata if hmac-sha256 is used for data:
        self.gethash     = hash_funcs["blake2b"] if self.hashtype == "hmac-sha256" \
                           else hash_funcs[self.hashtype]
        if self.compression == "zstd" and "zstd" not in compressors:
            x_it(1, "This archive requires the 'python3-zstd' module, which is not installed.")
        self.compress    = compressors[self.compression][2]
        self.decompress  = compressors[self.compression][0].decompress
        self.subdir      = "/a_"+hashlib.blake2b(
                           bytes(str(os.getuid())+self.uuid+self.dest.spec, encoding="UTF-8"),
                           digest_size=20).hexdigest()

        # init data crypto
        if mcrypto and datacrypto.key is None:
            if not float(self.updated_at) < self.time_start:
                raise ValueError("Current time is less than archive timestamp!")

            data_ci = ci_types[0]
            cadence = 20 + (data_ci.endswith(("-dgr","-msr")) * 100)
            datacrypto.load(data_ci, mcrypto.keyfile, slot=0, cadence=cadence,
                            passphrase=passphrase, agentkeys=agentkeys)

            # persist keys to allow future invocations w/o passphrase input
            if pass_agent > 0 and not agentkeys:
                agent_make(agent_name, pass_agent, [datacrypto.key, mcrypto.key])

        if mcrypto:
            # sync counters and set working salt file
            self.mci_count    = mcrypto.set_counter(self.mci_count, saltfile=self.saltpath)
            self.dataci_count = datacrypto.set_counter(self.dataci_count, saltfile=mcrypto.keyfile)

        # Enh: make 'hashtype' selectable for cipher modes that support hmac
        if datacrypto and datacrypto.mhashkey:
            self.getdatahash = datacrypto.getmhash_hmac
        else:
            self.getdatahash = hash_funcs[self.hashtype]

        # Ensure fully encrypted or unencrypted
        assert int(bool(self.mcrypto)) + int(bool(self.datacrypto)) != 1

        # load volume metadata objects
        if children:
            # initial load, depth 1
            self.load_volumes(1)

            if children > 1:
                # load volume objects fully
                self.load_volumes(children)

    def load_volumes(self, children, vids=[], handle=False):

        errors = []    ; vids = vids or list(self.conf["volumes"].keys())
        recv_list = [(x+"/volinfo", ArchiveVolume.max_volinfosz)
                       for x in self.conf["volumes"].keys() if x in vids]
        fetch_file_blobs(recv_list, self.path, self.dest, skip_exists=True)
        do_exec([[CP.chattr, "+c"] + list(self.conf["volumes"].keys())],
                cwd=self.path, check=False)

        for key, value in self.conf["volumes"].items():
            # conditions when a volume must be loaded:
            # - allvols flag is set
            # - in_process flag from an unfinished action
            # - volume specified on command line
            # - no volumes specified (hence all)
            # - deduplication is in effect
            #if children: ## and (allvols or self.in_process or options.from_arch or \
            ##(len(options.volumes)==0 or key in options.volumes or options.dedup)):
                # instantiate:

            if vids and key not in vids:   continue
            vid = key; hashval = value; vname = None

            #loadses = allvols or self.in_process or options.from_arch or options.dedup or
            try:
                vol  = ArchiveVolume(self, vid, hashval, pjoin(self.path,vid),
                                name=vname, children=children)
            except Exception as e:
                errors.append((vid, e))
                if not handle:   raise e

            self.vols[vol.name] = vol

        return errors


    def save_conf(self, ext=""):
        c = self.conf['var']    ; c.clear()    ; mcrypto = self.mcrypto
        c['uuid']        = self.uuid or str(uuid.uuid4())
        c['updated_at']  = self.updated_at = \
                            str(self.time_start - self.monotonic_start + time.monotonic())
        c['format_ver']  = str(format_version)
        c['chunksize']   = str(self.chunksize)
        c['compression'] = self.compression
        c['compr_level'] = self.compr_level
        c['hashtype']    = self.hashtype
        assert bool(self.ci_mode)
        c['ci_mode']     = self.ci_mode

        if mcrypto:
            if self.datacrypto.counter > self.datacrypto.ctstart:
                self.dataci_count = self.datacrypto.counter
            self.mci_count = mcrypto.counter
            c['dataci_count']= str(self.dataci_count)
            c['mci_count']   = str(self.mci_count)
        self.conf['in_process'] = { x: ln if type(ln) is str else ":|".join(ln)
                                          for x,ln in enumerate(self.in_process) }

        os.makedirs(self.path, exist_ok=True)    ; etag = b''
        with io.StringIO() as fs:
            self.conf.write(fs)    ; fs.flush()
            buf = gzip.compress(fs.getvalue().encode("UTF-8"), 4)
        if mcrypto:   etag, buf = mcrypto.encrypt(buf)
        with open(self.confpath+ext, "wb") as f:
            f.write(b''.join((self.confprefix, self.modeprefix,
                              self.ci_mode.encode("ASCII"), b"\n")))
            f.write(etag)    ; f.write(buf)

    def rename_saved(self, ext=".tmp"):
        os.replace(self.confpath+ext, self.confpath)

    # Set or clear state for the archive as 'in_process' in case of interruption during write.
    # Format is list containing strings or list of strings. For latter, ':|' is the delimiter.
    def set_in_process(self, outer_list, tmp=False, save=True, todest=True):
        fssync(aset.path)
        self.in_process = [] if outer_list is None else outer_list
        if not save:   return
        self.save_conf(".tmp" if tmp else "")

        #if not tmp:   os.replace(self.confpath+".tmp", self.confpath)
        if todest:      update_dest(self, pathlist=[self.confname], ext=".tmp" if tmp else "")

    def stop(self):
        self.autoreduce = False

    def add_volume_meta(self, datavol, desc="", ext=""):
        errs = []
        if len(self.conf["volumes"]) >= self.max_volumes:   x_it(1, "Too many volumes")
        if datavol in self.vols:
            print(datavol+" is already configured.")    ; return None

        namecheck = ArchiveVolume.volname_check(datavol)
        if namecheck:
            errs.append(namecheck+"\n")
        if len(desc.encode("UTF-8")) > self.comment_len:
            errs.append("Error: Max "+self.comment_len+" size for volume desc.\n")
        if not desc.isprintable():
            errs.append("Error: [^control] not allowed in volume desc.\n")
        if errs:
            sys.stderr.write("".join(errs))    ; error_cache.append(datavol)    ; return None

        while (vid := "Vol_"+os.urandom(3).hex()) in self.conf["volumes"]:   pass

        self.vols[datavol] = vol = ArchiveVolume(self, vid, None, pjoin(self.path,vid),
                                               name=datavol, children=0)
        vol.save_volinfo(ext)
        return vol

    def delete_volume_meta(self, vname=None, vid=None):
        # Enh: add delete-by-vid
        vid = self.vols[vname].vid if vname else vid
        assert vid.startswith("Vol_")
        vpath = self.path+"/"+vid
        del(self.conf["volumes"][vid])
        if vname in self.vols: del(self.vols[datavol])
        self.save_conf()

        if exists(vpath):    shutil.rmtree(vpath)
        return vid

    def rename_volume_meta(self, datavol, newname, ext=""):
        vol = self.vols[datavol]
        if newname in self.vols or ArchiveVolume.volname_check(newname):   return False

        vol.name = newname
        self.vols[newname] = vol    ; del(self.vols[datavol])
        vol.save_volinfo(ext)
        return True

    def b64hash(self, buf):
        return base64.urlsafe_b64encode(self.gethash(buf)).decode("ascii")

    def encode_file(self, fname, fdest=None, get_digest=True, compress=True):
        # Enh: optimize memory use
        mcrypto = self.mcrypto    ; digest = None    ; etag = b''
        destname= fdest or fname+(".z" if compress else "")

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            inbuf = bytes(inmap) if self.compression == "zstd" else inmap
            mbuf  = self.compress(inbuf, int(self.compr_level)) if compress else inbuf
            if get_digest:   digest = self.b64hash(mbuf)
            if mcrypto:      etag, mbuf = mcrypto.encrypt(mbuf)
            with open(destname,"wb") as f:   f.write(etag); f.write(mbuf)

        return digest

    def decode_file(self, fname, fdest=None, digest=None, max_sz=16000000):
        # Enh: optimize memory use
        destname= fdest or (fname[:-2] if fname.endswith(".z") else fname)
        mcrypto = self.mcrypto         ; buf_start = mcrypto.buf_start if mcrypto else 0

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            assert buf_start < len(inmap) <= max_sz + buf_start
            mbuf = mcrypto.decrypt(inmap) if mcrypto else bytes(inmap)
            assert hmac.compare_digest(digest, self.b64hash(mbuf))
            with open(destname,"wb") as f:   f.write(self.decompress(mbuf))


class ArchiveVolume:

    max_sessions  = 16384    ; volname_len = 4000    ; sesname_sz = 17
    max_volinfosz = (ArchiveSet.mhash_sz + sesname_sz + 10) * max_sessions \
                  + volname_len * 4 + ArchiveSet.comment_len * 4 + 10

    __slots__ = ("vid","name","hashval","archive","path","alias","aliastype","sessions","sesnames",
                 "_seslist","last","meta_checked","tags","desc","changed_bytes")

    def __init__(self, archive, vid, hashval, path, name=None, children=2):
        self.vid       = vid                       ; self.tags    = {}
        self.archive   = archive                   ; self.path    = path
        self.hashval   = hashval                   ; self.changed_bytes = 0
        self.last      = "None"                    ; self.meta_checked  = False
        self.alias     = name                      ; self.aliastype     = None
        # persisted here:
        self.name      = name                      ; self.desc      = ""
        self.sessions  = {}                        ; self.sesnames= []

        Ses = ArchiveSession
        if debug:  print("\n", vid, "-", name)
        if path and not exists(path):   os.makedirs(path)
        if path and hashval:
            with open(pjoin(path,"volinfo"), "rb") as f:
                fsize = os.fstat(f.fileno()).st_size
                if fsize > self.max_volinfosz:   raise ValueError("volinfo too large")
                buf = f.read(fsize)
            if archive.mcrypto:   buf = archive.mcrypto.decrypt(buf)
            if not hmac.compare_digest(hashval, archive.b64hash(buf)):
                raise ValueError("Volume %s hash %s, expected %s" \
                                 % (vid, archive.b64hash(buf), hashval))
            if debug:   print(archive.decompress(buf).decode("UTF-8"))
            with io.StringIO(archive.decompress(buf).decode("UTF-8")) as f:
                for ln in f:
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if vname.startswith("S_"):
                        self.sessions[vname] = Ses(self, vname, value, "")
                    else:
                        setattr(self, vname, value)
        if not self.name: raise ValueError("Vol name missing")
        self.alias = self.name

        if children > 1:
            self.load_sessions()

        if exists(pjoin(path,"volchanged")):
            self.changed_bytes = int(open(pjoin(path,"volchanged"),"r").readlines()[0].strip())

    def load_sessions(self, sesids=[], handle=False, mfdecode=False, force=False):

        sessions, path, vid, vname = self.sessions, self.path, self.vid, self.name
        Ses = ArchiveSession    ; sesids = sesids or list(sessions.keys())    ; errors = []

        # fetch session metadata 'info' from remote
        ses_list =[(vid+"/"+x, x, vname) for x in sesids]
        fetch_file_blobs([(x+"/info", ArchiveSession.max_infosz) for x,_,_ in ses_list],
                         self.archive.path, self.archive.dest, skip_exists=not force, skip0=True)

        for sname in sesids or sessions:
            if not (s := sessions[sname]).loaded:
                try:
                    s = sessions[sname] = Ses(self, sname, s.hashval, path+"/"+sname)
                except Exception as e:
                    #err_out(f"{self.vid} - {sname}\n{repr(e)}")
                    errors.append((sname, e))
                    if not handle:   raise e


        # session name list sorted by sequence field
        if not errors:
            self._seslist = list(self.sessions.values())
            self._seslist.sort(key=lambda x: x.sequence)
            self.sesnames = [y.name for y in self._seslist]
        if self.sesnames:   self.last = self.sesnames[-1]

        if mfdecode:
            errors.extend(self.decode_manifests(sesids, force=True, handle=handle))

        return errors

    def save_volinfo(self, ext=""):
        os.makedirs(self.path, exist_ok=True)
        arch = self.archive    ; fname = "volinfo"   ; etag = b''
        with io.StringIO() as f:
            print("name =", self.name, file=f)
            print("desc =", self.desc, file=f)
            for ses in self.sessions.values():
                if ses.saved:   print(ses.name, "=", ses.hashval, file=f)

            buf = arch.compress(f.getvalue().encode("UTF-8"), int(arch.compr_level))
            assert len(buf) <= self.max_volinfosz
            arch.conf["volumes"][self.vid] = arch.b64hash(buf)

        if arch.mcrypto:
            etag, buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
        arch.save_conf(ext)

    def rename_saved(self, ext=".tmp"):
        assert ext and exists(pjoin(self.path,"volinfo")+ext)
        for rpath in (pjoin(self.path,"volinfo"), pjoin(self.path,"vi.dat")):
            if exists(rpath+ext):   os.replace(rpath+ext, rpath)
        self.archive.rename_saved(ext)

    def volname_check(vname): # Fix: move to LocalStorage class
        if not 0 < len(vname) <= ArchiveVolume.volname_len:
            return f"Volume path/name length must be 1 - {ArchiveVolume.volname_len}."
        if not vname.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in vname.strip().split("/")]):
            return "Bad volume name."
        return ""

    def volsize(self):
        return self.sessions[self.last].volsize if self.sessions else 0

    def last_chunk_addr(self, vsize=None):
        if vsize is None:  vsize = self.volsize()
        chdigits    = max_address.bit_length() // 4
        lchunk_addr = max(0, (vsize-1) - ((vsize-1) % self.archive.chunksize))
        return lchunk_addr, ("x%0"+str(chdigits)+"x") % lchunk_addr

    def mapfile(self, pos=None):
        if not self.sessions:   return None
        s = self.sessions[self.sesnames[pos] if pos else self.last]
        return f"{self.path}/{s.name}_{s.sequence}.deltamap"

    # Based on last session size unless volume_size is specified.
    def mapsize(self, volume_size=None):
        vs = volume_size or self.volsize()
        return (vs // self.archive.chunksize // 8) + 1

    def map_used(self, ext=""):
        return os.stat(self.mapfile()+ext).st_blocks if exists(self.mapfile()+ext) else 0

    def changed_bytes_add(self, amount, reset=False, save=False):
        if reset:
            if exists(self.path+"/volchanged"):   os.remove(self.path+"/volchanged")
            self.changed_bytes = 0  ; return

        self.changed_bytes += amount
        if save:
            with open(self.path+"/volchanged", "w") as f:
                print(self.changed_bytes, file=f)
                f.flush()    ; os.fsync(f.fileno())

    def init_deltamap(self, timestamp=None):
        self.changed_bytes_add(0, reset=True)    ; bmfile = self.mapfile()
        if exists(bmfile):
            os.remove(bmfile)
        with open(bmfile, "wb") as bmapf:
            bmapf.truncate(self.mapsize())    ; bmapf.flush()
        if timestamp:   os.utime(bmfile, ns=(timestamp,)*2)

    def new_session(self, sname, addtags=[]):
        ns = ArchiveSession(self, sname, None, addtags=addtags)
        ns.path = pjoin(self.path, sname)
        ns.sequence = self.sessions[self.last].sequence + 1 if self.sessions else 0
        ns.previous = self.last

        self.last = sname
        self.sesnames.append(sname)
        self.sessions[sname] = ns
        if self.archive.dedupindex:    self.archive.dedupsessions.append(ns)
        return ns

    def delete_session(self, sname, remove=True, force=False):
        ses     = self.sessions[sname]
        index   = self.sesnames.index(sname)    ; affected = None
        if sname == self.last and ses.saved and not force:
            raise ValueError("Cannot delete last session")

        for tag in list(ses.tags.keys()):   self.sessions[sname].tag_del(tag)
        del(self.sesnames[index], self.sessions[sname])

        if self.archive.dedupsessions:
            indexdd = self.archive.dedupsessions.index(ses)
            self.archive.dedupsessions[indexdd] = None

        # Following condition means:
        #   * sesnames cannot be empty
        #   * ses wasn't deleted from end of list

        if len(self.sesnames) > index:
            affected = self.sesnames[index]
            self.sessions[affected].previous = ses.previous

        self.last  = self.sesnames[-1] if len(self.sesnames) else "None"

        if remove and exists(ses.path):   shutil.rmtree(ses.path)
        if not remove and exists(ses.path+"/manifest"):   os.remove(ses.path+"/manifest")
        return affected

    # Decodes a 'manifest.z' file
    def decode_one_manifest(self, ses, force=False):
        if not ses.path:   return
        path = ses.path+"/manifest"
        if not exists(path) or (force and not ses.just_fetched):
            os.makedirs(ses.path, exist_ok=True)
            do_exec([[CP.chattr, "+c", ses.path]], check=False)
            if ses.manifesthash == "0":   open(path,"wb").close()   ; return

            self.archive.decode_file(path+".z", fdest=path+".tmp",
                                        digest=ses.manifesthash, max_sz=ses.manifest_max())
            os.replace(path+".tmp", path)
            ses.just_fetched = True
        else:
            os.utime(path)

        if exists(path+".z"):   os.utime(path+".z")

    # Fetches files and passes them to decode_one_manifest()
    def decode_manifests(self, sesnames, force=False, handle=False):
        apath    = self.archive.path    ; dest = self.archive.dest    ; errors = []
        ses_list = [self.sessions[ses] for ses in sesnames]

        ff = fetch_file_blobs([(pjoin(self.vid, ses.name, "manifest.z"), ses.manifest_max())
                                 for ses in ses_list if ses.path and not ses.path.endswith("-tmp")
                                 and ses.manifesthash != "0"],
                              apath, dest, skip_exists=not force)

        for ses in ses_list:
            try:
                self.decode_one_manifest(ses, force=force)
            except Exception as e:
                errors.append((ses.name, e))
                if not handle:   raise e

        return errors


class ArchiveSession:

    attr_str  = ("localtime","previous","permissions","manifesthash")
    attr_int  = ("volsize","sequence")
    attr_misc = ("tags","volume","archive","name","path","saved","loaded","toggle",
                 "uuid","hashval","meta_checked","just_fetched")
    sesname_sz= ArchiveVolume.sesname_sz    ; tag_len = 25    ; max_tags = 5    ; max_field = 64
    max_permsz= 32 * 4 * 2 + 5  # user & group names in utf-8 plus mode
    max_infosz= (tag_len * 4 + ArchiveSet.comment_len * 4 + 10) * max_tags + max_permsz \
              + sum(map(len, attr_str+attr_int)) + len(attr_str+attr_int)*(max_field+10)
    __slots__ = attr_str + attr_int + attr_misc

    def __init__(self, volume, name, hashval, path="", addtags=[]):
        self.volume   = volume;    self.archive = arch = volume.archive
        self.name     = name
        self.path     = path
        self.saved    = self.loaded = False
        self.toggle   = True
        self.hashval  = hashval
        self.meta_checked = False
        self.just_fetched = False
        # persisted:
        self.localtime= 0
        self.volsize  = None
        self.sequence = None
        self.previous = "None"
        self.tags     = {}
        self.permissions  = ""
        self.manifesthash = None

        if path and hashval:
            with open(pjoin(path,"info"), "rb") as sf:
                fsize = os.fstat(sf.fileno()).st_size
                if fsize > self.max_infosz:   raise ValueError("info too large")
                buf = sf.read(fsize)
            if arch.mcrypto:   buf = arch.mcrypto.decrypt(buf)
            if debug:   print(self.volume.vid, self.name); print(arch.decompress(buf).decode("UTF-8"))
            if not hmac.compare_digest(hashval, arch.b64hash(buf)):
                raise ValueError("%s Session %s hash %s, expected %s" \
                                 % (volume.vid, name, arch.b64hash(buf), hashval))
            with io.StringIO(arch.decompress(buf).decode("UTF-8")) as sf:
                for ln in sf:
                    vname, value = map(str.strip, ln.split("=", maxsplit=1))
                    if vname == "tag":
                        self.tag_add(ArchiveSession.tag_parse(value))
                        continue

                    setattr(self, vname, 
                        int(value) if vname in self.attr_int else value)

            self.saved = self.loaded = True

        for tag in addtags:   self.tag_add(tag)


    def manifest_max(self):
        return (self.volsize // self.archive.chunksize + 1) \
               * (self.archive.mhash_sz + len(hex(max_address)) + 10)

    def tag_parse(tag, delim=" "):
        errs = []      ; parts   = tag.strip().split(delim, maxsplit=1)
        tag_id = parts[0].strip().lower()[:ArchiveSession.tag_len]
        comment = parts[1].strip() if len(parts)>1 else ""

        if re.match(r".*[,=\^]", tag_id) or any(map(str.isspace, tag_id)) \
        or not tag_id.isprintable():
            errs.append("Error: [^control], [space], and ',^=' not allowed in tag ID.\n")
        if not comment.isprintable():
            errs.append("Error: [^control] not allowed in tag comment.\n")
        if tag_id == "all":   errs.append("Error: tag 'all' is reserved.\n")
        sys.stderr.write("".join(errs))

        return tuple() if errs else (tag_id, comment)

    def tag_add(self, tag):
        if len(self.tags) >= ArchiveSession.max_tags:
            x_it(1, ArchiveSession.max_tags+" maximum tags.")
        self.saved = False    ; voltags = self.volume.tags    ; tid = tag[0]
        if tid not in self.tags:   self.tags[tid] = tag[1]
        if tid in voltags:
            voltags[tid].add(self.name)
        else:
            voltags[tid] = {self.name}
        return True

    def tag_del(self, tag):
        del(self.tags[tag])   ; voltags = self.volume.tags
        if tag in voltags:
            if self.name in voltags[tag]:   voltags[tag].remove(self.name)
            if len(voltags[tag]) == 0:    del(voltags[tag])

    def gettime(self):
        if is_num(self.localtime):
            return int(self.localtime)
        else:
            return int(time.mktime(time.strptime(self.localtime, "%Y%m%d-%H%M%S"))*1000000000)

    def save_info(self, ext=""):
        assert self.path   ; saved_files = []
        etag = b''   ; fname = "info"   ; arch = self.volume.archive

        if os.path.getsize(self.path+"/manifest"+ext) == 0:
            self.manifesthash = "0"
        else:
            saved_files.append(mfz := "manifest.z"+ext)
            self.manifesthash = arch.encode_file(self.path+"/manifest"+ext, fdest=self.path+"/"+mfz)
        with io.StringIO() as f:
            for attr in self.attr_str+self.attr_int:
                print(attr, "=", field := getattr(self, attr), file=f)
                assert len(str(field)) <= self.max_field
            for tkey, tdesc in self.tags.items():
                print("tag =",   tkey, tdesc, file=f)
            buf = arch.compress(f.getvalue().encode("UTF-8"),
                                int(arch.compr_level))
            self.hashval = arch.b64hash(buf)
            assert len(buf) <= self.max_infosz

        if arch.mcrypto:   etag, buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
        saved_files.append(fname+ext)    ; self.saved = self.loaded = True
        self.volume.save_volinfo(ext)
        return saved_files

    def rename_saved(self, ext=".tmp"):
        assert ext and exists(pjoin(self.path,"info")+ext)
        for rpath in (pjoin(self.path,x) for x in ("info", "manifest", "manifest.z")):
            if exists(rpath+ext):   os.replace(rpath+ext, rpath)
        self.volume.rename_saved(ext)

# END class ArchiveSet, ArchiveVolume, ArchiveSession


class OldArchiveSet_V2:
    confname = "archive.ini"  ; format_ver = 2   ; a_ints = {"chunksize"}

    def __init__(self, top, dest):
        self.path  = top      ; self.dest = dest          ; self.vols  = {}
        if exists(self.path+"/in_process"):
            raise ValueError("in_process must be resolved before upgrade.")

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()    ; cp.optionxform = lambda option: option
        cp["var"], cp["volumes"] = {}, {}                 ; cp.read(pjoin(top, self.confname))
        for name in cp["var"].keys():
            setattr(self, name, int(cp["var"][name]) \
                                if name in self.a_ints else cp["var"][name])

        # load volume metadata objects
        for key in cp["volumes"]:
            fetch_file_blobs([(pjoin(key,"volinfo"), 100000000)], top, dest)
            self.vols[key] = self.Volume(self, key, pjoin(top,key), self.vgname)

    class Volume:
        def __init__(self, archive, name, path, vgname):
            self.archive = archive    ; self.name = name    ; self.path = path
            self.format_ver = "0"     ; self.uuid = None    ; self.desc = ""
            self.sessions, self.sesnames = {}, []
            self.first = self.last = "None"

            # load volume info
            with open(pjoin(path,"volinfo"), "r") as f:
                for ln in f:
                    vname, value = ln.strip().split("=", maxsplit=1)
                    setattr(self, vname.strip(), value.strip())

            if self.format_ver != "2": raise ValueError("Format ver = "+self.format_ver)

            # load sessions as reverse-linked list, starting with the last
            if debug:  print("\nV2 VOLUME", self.name, self.first, self.last)
            sprev = self.last
            while sprev != "None":
                if debug:  print(sprev, end="  ")
                fetch_file_blobs([(f"{name}/{sprev}/info", 100000000),
                                  (f"{name}/{sprev}/manifest", 100000000)], archive.path, archive.dest)
                s = self.sessions[sprev] = self.Ses(self, sprev, path+"/"+sprev)
                sprev = s.previous    ; self.sesnames.insert(0, s.name)
                if sprev == "None" and self.first != s.name:
                    raise ValueError("PREVIOUS MISMATCH: %s/%s, EXPECTED %s" 
                                     % (self.name, s.name, self.first))

        class Ses:
            def __init__(self, volume, name, path="", addtags={}):
                self.name     = name     ; self.path = path    ; self.volume = volume
                self.previous = "None"   ; self.tags = {}
                self.localtime= self.volsize  = self.format = self.sequence = None

                with open(pjoin(path,"info"), "r") as sf:
                    for ln in sf:
                        if ln.strip() == "uuid =":  continue
                        vname, value = ln.split("=", maxsplit=1)
                        vname = vname.strip()    ; value = value.strip()
                        if value == "none":   value = "None"
                        if vname == "tag":
                            self.tag_add(value.split(maxsplit=1))
                            continue

                        setattr(self, vname, 
                                int(value) if vname in ("volsize","sequence") else value)

                    if not exists(pjoin(path,"manifest")):
                        raise FileNotFoundError("ERROR: Manifest does not exist for "+name)

                for tag in addtags:   self.tag_add(tag)

            def tag_add(self, tag):
                tid = tag[0]
                if tid not in self.tags:   self.tags[tid] = tag[1]

# END OldArchiveSet_V2


def upgrade_from_v2(aset):
    print("Creating metadata backup file 'wyng_metadata_bak.tbz'.")
    do_exec([[CP.tar, "-cjf", "wyng_metadata_bak.tbz", aset.path]])
    print("Upgrading archive format...", end="")   ; dest = aset.dest   ; sessions, moves = [],[]

    os.mkdir(tmpdir+"/:upgrade:")
    new_aset = ArchiveSet(tmpdir+"/:upgrade:", aset.dest, aset.opts)
    for attr in ("uuid","chunksize","compression","compr_level","hashtype"):
        setattr(new_aset, attr, getattr(aset, attr))
    new_aset.ci_mode = "00"    ; new_aset.save_conf()
    new_aset = ArchiveSet(new_aset.path, aset.dest, aset.opts)

    for vname, vol in aset.vols.items():
        newvol = new_aset.add_volume_meta(vname, desc=vol.desc)
        assert newvol.vid not in aset.vols    ; moves.append(f"mv -T {vname} {newvol.vid}\n")
        for sname in vol.sesnames:
            ses = vol.sessions[sname]    ; newses = newvol.new_session(sname)
            for attr in ("tags","volsize","localtime"):
                setattr(newses, attr, getattr(ses, attr))
            os.makedirs(newses.path, exist_ok=True)
            with open(ses.path+"/manifest", "r") as mreadf, \
                    open(newses.path+"/manifest", "w") as mwf:
                for hexhash, addr in (ln.split() for ln in mreadf):
                    print("0" if hexhash == "0" else
                            base64.urlsafe_b64encode(bytes.fromhex(hexhash)).decode("ASCII"),
                            addr, file=mwf)
            newses.permissions = "w"    ; newses.save_info()
            sessions.append(newses)     ; os.remove(ses.path+"/manifest")
    catch_signals()
    dest.run(["".join(moves)], destcd=dest.path, trap=True)
    update_dest(new_aset, pathlist=[new_aset.confname],
                volumes=new_aset.vols.values(), sessions=sessions)
    catch_signals(**signormal)    ; print("Done.", end="")


def agent_helper_write(path):
    agent_program = r'''#  Copyright Christopher Laprise 2018-2024
#  Licensed under GNU General Public License v3. See wyng-backup/LICENSE file.
import os, sys, signal, time

def gettimes():    return time.clock_gettime(time.CLOCK_BOOTTIME), time.monotonic()

def sighandler(s,f):
    if s == ALRM:   raise TimeoutError("SIGALRM")

def do_mkpipe():
    if exists("/tmp/"+agent_name):   os.remove("/tmp/"+agent_name)
    os.mkfifo("/tmp/"+agent_name, mode=0o600)

class KeyHandler:
    def __init__(self):
        self.keys = []

    def __del__(self):
        for key in self.keys:
            for ii in range(len(key)):   key[ii] = 0
        os.remove("/tmp/"+agent_name)

## MAIN ##
cmd    = sys.argv[1]      ; agent_name = sys.argv[2]     ; inread = sys.stdin.buffer.read
KH     = KeyHandler()     ; magic  = b"\xff\x11\x15"     ; exists = os.path.exists
SIGINT = signal.SIGINT    ; USR1   = signal.SIGUSR1      ; ALRM   = signal.SIGALRM
for s in (SIGINT,USR1,ALRM):   signal.signal(s, sighandler)   ; signal.siginterrupt(s, True)

signal.alarm(10)    ; do_mkpipe()    ; duration = min(inread(1)[0], 60) * 60
bstart, mstart = gettimes()
for slot in range(2):
    KH.keys.append(bytearray(inread(inread(1)[0])))   ; assert inread(3) == magic

while True:
    try:
        signal.alarm(0)   ; res = signal.sigtimedwait({USR1}, 60)   ; signal.alarm(10)
        btime, mtime = gettimes()
        if (res is None and btime - bstart >= duration) \
        or abs(btime - mtime) > abs(bstart - mstart) + 5:
            break
        elif res is None:
            continue
        with open("/tmp/"+agent_name,"wb") as npipe:
            for key in KH.keys:
                npipe.write(magic + len(key).to_bytes(1,"big"))   ; npipe.write(key)
        bstart, mstart = gettimes()
    except TimeoutError:
        print("timeout")    ; do_mkpipe()    ; continue
'''
    with open(f"{path}/{prog_name}_agent.py", "wb") as progf:
        progf.write(bytes(agent_program, encoding="UTF-8"))


def agent_get(agname, duration):
    ps = SPr.check_output([CP.ps, "-u"+str(os.getuid()), "-o", "pid,command"], text=True)
    findp  = [x for x in ps.splitlines() if (agname in x and prog_name+"_agent.py store" in x)]
    if not findp:   return None
    pid    = int(findp[0].split()[0])   ;  os_kill(pid, signal.SIGUSR1)
    result = []   ; catch_signals(["ALRM"], iflag=True) ;   signal.alarm(10)
    try:
        with open("/tmp/"+agname, "rb") as npipe:
            for slot in range(2):
                if m := npipe.read(3) != Destination.magic:
                    raise ValueError("*Magic not found, got "+repr(m))
                result.append(key := bytearray(npipe.read(ksz := npipe.read(1)[0])))
                if len(key) != ksz:   raise ValueError("Key length")
    except (TimeoutError, ValueError) as e:
        for k in result:   clear_array(k)
        result = None    ; os_kill(pid)
    finally:
        if duration == -1:   os_kill(pid)
        signal.alarm(0)  ; catch_signals(**signormal)
        return result


def agent_make(agname, duration, keys):
    p = SPr.Popen([CP.python, f"{tmpdir}/{prog_name}_agent.py","store",agname], text=False,
                  shell=False, stdin=SPr.PIPE, stdout=SPr.DEVNULL, stderr=SPr.STDOUT)
    pwrite = p.stdin.write   ; pwrite(min(duration,60).to_bytes(1,"big"))
    for k in keys:   pwrite(len(k).to_bytes(1,"big"))   ; pwrite(k)   ; pwrite(Destination.magic)
    p.poll(); p.stdin.flush(); p.stdin.close()
    return p


# DataCryptography(): Handle crypto functions and state for volume data

class DataCryptography:

    crypto_key_bits = 256      ; max_ct_bits = 128        ; salt_sz   = 64
    nonce_sz        = 24       ; tag_sz      = 16         ; saltchksz = salt_sz+nonce_sz+tag_sz
    max_keyfile_sz  = (salt_sz + (max_ct_bits // 8)) * 4 + saltchksz # max 4 slots
    time_headroom   = int(60*60*24*365.25*50)             ; timesz    = 32 // 8

    # Matrix of recommended mode pairs = 'formatcode: (data, metadata, selectable)'
    # User selects a data cipher which is automatically paired w a metadata authentication cipher.
    crypto_codes    = {"00":  ("off",                "off", 1),
                       "10":  ("n/a",                "n/a", 0),
                       "20":  ("n/a",                "n/a", 0),
                       "30":  ("xchacha20",          "xchacha20-poly1305",     0),
                       "31":  ("n/a",                "n/a", 0),
                       "32":  ("xchacha20-ct",       "xchacha20-poly1305-ct",  1),
                       "33":  ("n/a",                "n/a", 0),
                       "34":  ("xchacha20-msr",      "xchacha20-poly1305-msr", 1),
                       "35":  ("xchacha20-dgr",      "xchacha20-poly1305-msr", 1),
                       "40":  ("n/a",                "n/a", 0)}

    __slots__ = ("key","keyfile","ci_type","counter","ctstart","ctcadence","countsz","max_count",
                 "slot","slot_offset","key_sz","randomsz","buf_start","mode",
                 "encrypt","decrypt","auth","ChaCha20_new","ChaCha20_Poly1305_new",
                 "time_start","monotonic_start","get_rnd","noncekey","mhashkey")

    def __init__(self):
        self.key = self.noncekey = self.keyfile = self.counter = self.ctstart = self.countsz \
                 = self.mhashkey = self.slot_offset = None

    def load(self, ci_type, keyfile, slot, passphrase, agentkeys=None, cadence=1, init=False):

        if tuple(time.gmtime(0))[:6] != (1970, 1, 1, 0, 0, 0):
            x_it(1, "System time epoch is not 1970-01-01.")
        assert passphrase is None or type(passphrase) == bytearray
        assert type(cadence) is int and cadence > 0

        if not Cryptodome \
        or (ci_type.startswith("xchacha20") and Cryptodome.version_info[0:2] < (3,9)):
            raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")

        self.time_start = time_start       ; self.monotonic_start = monotonic_start
        self.slot       = slot             ; self.slot_offset = self.get_slot_offset(slot)
        self.keyfile    = keyfile          ; self.ci_type     = ci_type
        self.ctcadence  = cadence          ; mknoncekey       = mkmhashkey = False
        self.get_rnd    = get_random_bytes ; self.auth        = False

        # xchacha20 common
        self.key_sz  = self.crypto_key_bits//8 ; self.max_count = 2**80-64
        self.countsz = 10                      ; self.buf_start = self.nonce_sz
        self.randomsz=self.nonce_sz - self.countsz - self.timesz
        self.ChaCha20_new = Cipher_ChaCha20.new
        self.ChaCha20_Poly1305_new = Cipher_ChaCha20_Poly1305.new
        self.decrypt = self._dec_chacha20

        if ci_type == "xchacha20":
            self.encrypt = self._enc_chacha20_ct

        elif ci_type in ("xchacha20-poly1305", "xchacha20-poly1305-ct"):
            self.max_count = 2**61-64
            self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305_ct

        elif ci_type == "xchacha20-ct":
            self.encrypt = self._enc_chacha20_ct
            mkmhashkey   = True

        elif ci_type == "xchacha20-msr":
            self.encrypt = self._enc_chacha20_msr
            mknoncekey   = mkmhashkey = True

        elif ci_type == "xchacha20-poly1305-msr":
            self.max_count = 2**63-64
            self.buf_start = self.nonce_sz + self.tag_sz
            self.decrypt = self.auth = self._dec_chacha20_poly1305
            self.encrypt = self._enc_chacha20_poly1305_msr
            mknoncekey   = True

        elif ci_type == "xchacha20-dgr":
            self.encrypt = self._enc_chacha20_dgr
            mknoncekey   = mkmhashkey = True

        else:
            raise ValueError("Invalid cipher spec "+ci_type)


        # Load counter and salt from salt file
        if not issubclass(type(keyfile), io.IOBase):
            if init:
                # initialize new salt file
                assert slot == 1 and not exists(keyfile)
                open(keyfile, "wb").close()    ; sf = self.keyfile = self.open_saltfile(keyfile)
                for ii in range(4):
                    sf.seek(self.get_slot_offset(ii))
                    sf.write(bytes(self.countsz) + self.get_rnd(self.salt_sz))
            else:
                sf = self.keyfile = self.open_saltfile(keyfile)

        ct, salt     = self.load_slot(self.slot, self.keyfile)
        # Compatibility kludge for original counter mode salt
        if ci_type in ("xchacha20","xchacha20-poly1305"):   salt = salt[:32]

        # Advance counter with a safe margin 'cadence X2' if cadence is quick (<101)
        self.counter = self.ctstart = ct + (cadence * 2 * int(cadence < 101))

        # Import or derive encryption key
        if agentkeys:
            self.key = agentkeys[slot]
        else:
            self.key = self.derive_key(salt, passphrase, self.key_sz)
        assert len(self.key) == self.key_sz and type(self.key) is bytearray

        if mknoncekey:
            # Derive subkey for generating nonces
            _na, nk_salt  = self.load_slot(2, self.keyfile)
            self.noncekey = self.derive_subkey(self.key, 64, 0, nk_salt,
                                               b"Wyng_Nonces" + str(self.slot).encode())

        if mkmhashkey:
            # Derive subkey for data hashing
            _na, mhk_salt = self.load_slot(3, self.keyfile)
            self.mhashkey = self.derive_subkey(self.key, 64, 1, mhk_salt,
                                               b"Wyng-Manifest-Hash" + str(self.slot).encode())

        if init and slot == 1:
            sbuf = b"".join(self.encrypt(self.hash_salts()[0]))
            sf.seek(self.get_slot_offset(4))    ; sf.write(sbuf)    ; sf.flush()
            # Write a copy of salt file, inverted so it isn't deduped.
            with open(os.path.dirname(sf.name)+"/salt.bak","wb") as outf:
                sf.seek(0)    ; outf.write(bytes(x ^ 0xff for x in sf.read()))

        return self.counter


    def __del__(self):
        if self.counter and self.counter > self.ctstart:   self.save_counter()
        if self.key:        clear_array(self.key)
        if self.noncekey:   clear_array(self.noncekey)
        if self.mhashkey:   clear_array(self.mhashkey)


    def open_saltfile(self, fpath, verify=False):
        assert self.slot == 1
        sf = open(fpath, "r+b", buffering=0)

        if verify:
            h, enc_h  = self.hash_salts(sf)
            if len(enc_h) < self.saltchksz:   raise ValueError("Wrong salt hash size "+str(len(enc_h)))
            if not hmac.compare_digest(h, self.decrypt(enc_h)):   raise ValueError("Bad salt hash.")

        return sf

    def get_slot_offset(self, slot):
        return (self.salt_sz + (self.max_ct_bits//8)) * slot

    def load_slot(self, slot, saltf=None):
        saltf = saltf or self.keyfile
        saltf.seek(self.get_slot_offset(slot))
        counter = int.from_bytes(saltf.read(self.countsz), "big")
        salt    = saltf.read(self.salt_sz)
        saltf.seek(self.get_slot_offset(slot+1))
        return counter, salt

    def hash_salts(self, saltf=None):
        saltf = saltf or self.keyfile    ; slots = (0,1,2,3)
        h = hashlib.blake2b(b"".join((self.load_slot(x, saltf)[1] for x in slots)),
                            digest_size=self.salt_sz).digest()
        enc_h = saltf.read()
        return h, enc_h

    def derive_key(self, salt, passphrase, size):
        key = bytearray(hashlib.scrypt(passphrase, salt=salt, n=2**19, r=8, p=1,
                                        maxmem=640*1024*1024, dklen=size))
        clear_array(passphrase)
        return key

    def derive_subkey(self, key, size, subslot, salt, context):
        skeys = Cryptodome.Protocol.KDF.HKDF(key, size, salt, Cryptodome.Hash.SHA512,
                                             max(subslot+1, 2), context)
        return bytearray(skeys[subslot])

    # Provide keyed hash func for manifests (send/receive)
    def getmhash_hmac(self, buf):
        return hmac.digest(self.mhashkey, buf, "sha256")

    # Update key counter on disk; call directly at end of transaction if cadence > 1
    def save_counter(self):
        self.keyfile.seek(self.slot_offset)
        self.keyfile.write(self.counter.to_bytes(self.countsz, "big"))
        self.keyfile.flush()

    # Update counter with a new value, if greater.  Update salt file location.
    def set_counter(self, ct, saltfile=None):
        if issubclass(type(saltfile), io.IOBase):
            self.keyfile = saltfile
        elif saltfile and saltfile != self.keyfile.name:
            self.keyfile = self.open_saltfile(saltfile, verify=True)

        self.counter = max(ct, self.load_slot(self.slot)[0])    ; self.save_counter()
        return self.counter

    # Decrypt [X]ChaCha20:
    def _dec_chacha20(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        return cipher.decrypt(untrusted_buf[self.nonce_sz:])

    # Decrypt [X]ChaCha20-Poly1305:
    def _dec_chacha20_poly1305(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)

    # Encrypt [X]ChaCha20 (protected counter nonce):
    def _enc_chacha20_ct(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.time_start - self.monotonic_start + time.monotonic())
                               - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        buf    = cipher.encrypt(buf)
        return  nonce, buf

    # Encrypt [X]ChaCha20-Poly1305 (protected counter nonce):
    def _enc_chacha20_poly1305_ct(self, buf):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.time_start - self.monotonic_start + time.monotonic())
                               - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (HMAC nonce: message & random inputs)
    def _enc_chacha20_msr(self, buf, _na):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || buf
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(buf)
        nonce   = nonce_h.digest()[:24]

        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)

    # Encrypt [X]ChaCha20-Poly1305 (HMAC nonce: message & random inputs)
    def _enc_chacha20_poly1305_msr(self, buf):
        self.counter += ceil(len(buf) // 16384)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || buf
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(buf)
        nonce   = nonce_h.digest()[:24]

        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  b''.join((nonce, ci_tag)), buf

    # Encrypt [X]ChaCha20 (HMAC nonce: digest & random inputs)
    def _enc_chacha20_dgr(self, buf, mhash):
        self.counter += 1
        if self.counter % self.ctcadence == 0:   self.save_counter()
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")

        # Nonce from HMAC of rnd || Hm (manifest hash)
        nonce_h = hmac.new(self.noncekey, msg=self.get_rnd(24), digestmod="sha256")
        nonce_h.update(mhash)
        nonce   = nonce_h.digest()[:24]

        cipher  = self.ChaCha20_new(key=self.key, nonce=nonce)
        return  nonce, cipher.encrypt(buf)


# Define absolute paths of commands

class CP:
    awk    = "/usr/bin/gawk"    ; sed   = "/usr/bin/sed"     ; sort     = "/usr/bin/sort"
    cat    = "/usr/bin/cat"     ; mkdir = "/usr/bin/mkdir"   ; python   = "/usr/bin/python3"
    mv     = "/usr/bin/mv"      ; grep  = "/usr/bin/grep"    ; ssh      = "/usr/bin/ssh"
    sh     = "/usr/bin/sh"      ; tar   = "/usr/bin/tar"     ; tail     = "/usr/bin/tail"
    tee    = "/usr/bin/tee"     ; sync  = "/usr/bin/sync"    ; cp       = "/usr/bin/cp"
    chattr = "/usr/bin/chattr"  ; xargs = "/usr/bin/xargs"   ; sha256sum= "/usr/bin/sha256sum"
    cmp    = "/usr/bin/cmp"     ; gzip  = "/usr/bin/gzip"    ; env      = "/usr/bin/env"
    ps     = "/usr/bin/ps"      ; uniq  = "/usr/bin/uniq"    ; ionice   = "/usr/bin/ionice"
    qvm_run= "/usr/bin/qvm-run" ; rm    = "/usr/bin/rm"      ; find     = "/usr/bin/find"
    thin_delta = "/usr/sbin/thin_delta" ; filefrag = "/usr/sbin/filefrag"  ; lvm = "/usr/sbin/lvm"
    blkdiscard = "/sbin/blkdiscard"     ; dmsetup  = "/sbin/dmsetup"
    btrfs      = "/usr/sbin/btrfs" if os.path.exists("/usr/sbin/btrfs") else "/bin/btrfs"


# Manage local (source) data volumes, thin lvm and reflink image files and snapshots.
# Volume names are mapped into the 'lvols' cache based on whether they are found
# under the current --local path or if a volume name exists in the archive.
# lvols enties may point to local volumes that are non-existant, so use v.exists().

class LocalStorage:

    BLKDISCARD               = 0x1277           ; BLKDISCARDZEROES        = 0x127c
    FALLOC_FL_KEEP_SIZE      = 0x01             ; FALLOC_FL_PUNCH_HOLE    = 0x02
    FALLOC_FL_COLLAPSE_RANGE = 0x08             ; FALLOC_FL_ZERO_RANGE    = 0x10
    FALLOC_FL_INSERT_RANGE   = 0x20             ; FALLOC_FL_UNSHARE_RANGE = 0x40
    FALLOC_FL_PUNCH_FULL     = FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE

    try:
        fallocate = ctypes.CDLL(ctypes.util.find_library("c")).fallocate
        fallocate.restype = ctypes.c_int
        fallocate.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int64, ctypes.c_int64]
    except:
        fallocate = None    ; print("fallocate() function not available.")

    def __init__(self, localpath, auuid=None, arch_vols={}, clean=False, sync=False,
                 require_online=False):

        self.stypes  = { "tlvm":  LvmVolume, "rlnk": ReflinkVolume, "file": FileVolume }
        self.rltypes = { "btrfs", "xfs" }

        assert len(auuid) > 8
        self.gc_procs  = []          ; self.locked  = False      ; self.auuid     = auuid
        self.clean     = clean       ; self.sync    = sync       ; self.arch_vols = arch_vols
        self.lvols, self.vgs_all = {}, {}
        self.users, self.groups  = {}, {}
        self.path = self.pooltype = self.fstype = self.lvpool = None
        self.online = self.can_snapshot = False

        loctype, locvol, locpool, pathxvg = LocalStorage.parse_local_path(localpath)
        ol_reason = ""

        if loctype is None:
            self.path           = None

        elif loctype in ("lvm volgroup", "tlvm pool"):
            self.pooltype       = "tlvm"          ; self.path    = "/dev/"+pathxvg+"/"
            self.block_size     = 512
            self.vgname         = pathxvg         ; self.lvpool  = locpool
            self.online  = self.can_snapshot = exists(self.path)

            self.acquire_deltas = get_lvm_deltas
            self.process_deltas = update_delta_digest_lvm
            self.prep_snapshots = prepare_snapshots_lvm

        elif loctype == "file" and exists(pathxvg):
            self.path       = pathxvg.rstrip("/")+"/"; self.online, self.can_snapshot = True, False
            self.fstype     = fs = LocalStorage.get_fs_type(self.path)
            self.pooltype   = "rlnk" if fs in self.rltypes else "file"

            if fs == "btrfs":
                if os.stat(self.path).st_ino == 256:
                    self.snappath = self.path+"wyng_snapshot_tmp/"   ; self.can_snapshot = True
                self.block_size = 4096
            elif fs == "xfs":
                self.snappath = ""   ; self.can_snapshot = True    ; self.block_size = 4096

            if self.can_snapshot:
                self.acquire_deltas = get_reflink_deltas
            self.process_deltas = update_delta_digest_reflink
            self.prep_snapshots = prepare_snapshots_reflink

        self.users  = {x[0]: int(x[2]) for x in
                        (ln.split(":") for ln in open("/etc/passwd","r") if ln.strip())}
        self.groups = {x[0]: int(x[2]) for x in
                        (ln.split(":") for ln in open("/etc/group","r") if ln.strip())}

        if require_online and not self.online:
            err_out("Local storage is offline: "+repr(localpath)+ol_reason)

        if self.online:
            self.LVolClass    = self.stypes[self.pooltype]
            self.metadata_unlock()
            self.update_vol_list(arch_vols)

        if debug:
            print("**fstype is", self.fstype)
            print("**pooltype", self.pooltype, "not" if not self.online else "", "online")
            print(self.path, self.lvpool)

    def __del__(self):
        if self.clean:
            for p in self.gc_procs:   p.wait()


    # Note: file_punch_hole() and block_discard_chunk() have the same arg signature...
    def file_punch_hole(self, fn, start, length):
        if self.fallocate(fn, self.FALLOC_FL_PUNCH_FULL, start, length) == 0:
            return True
        else:
            return False

    def block_discard_chunk(self, fn, start, length):
        try:
            return fcntl.ioctl(fn, self.BLKDISCARD, struct.pack("LL", start, length))
        except OSError:
            return False
        else:
            return True

    def exists(self, volname):
        return exists(self.path + volname)

    def setperms(self, path, perms):
        if not perms:   return
        p_t = perms.split(":")    ; user = group = None

        if len(p_t) == 1:
            # set lvm type perm
            assert p_t[0] in ("r","w")
            pbits = 0o600 if p_t[0]=="w" else 0o400
        else:
            pbits, user, group = p_t    ; pbits = int(pbits)

        if os.path.isfile(path):
            os.chmod(path, stat.S_IMODE(pbits))
            if admin_permission and user and user in self.users:
                shutil.chown(path, user, group if group in self.groups else None)
        elif self.pooltype == "tlvm" and path.startswith(self.path):
            p  = "w" if pbits & stat.S_IWUSR else ""
            old= SPr.check_output([CP.lvm, "lvs", "--noheadings", "-o","lv_attr", path], text=True)
            if p != old.strip()[1]:
                do_exec([[CP.lvm, "lvchange", path, "-p", "r"+p]])

    def getperms(self, path):
        vstat = os.stat(os.path.realpath(path))    ; uid, gid = vstat.st_uid, vstat.st_gid
        # use names in place of uid/gid numbers:
        user  = [x for x, num in self.users.items()  if num == uid][0]
        group = [x for x, num in self.groups.items() if num == gid][0]
        return f"{stat.S_IMODE(vstat.st_mode)}:{user}:{group}"

    def settime(self, path, t):
        if t > 0 and os.path.isfile(path):
            os.utime(path, ns=(t,t))
            return True
        else:
            return False

    def metadata_lock(self, lvpool=None):
        mark = 0    ; spath = self.path
        if self.pooltype == "tlvm":
            self._lvm_meta_snapshot("reserve", pool=lvpool)
        elif self.pooltype == "rlnk":
            if not self.fstype or self.fstype not in ("btrfs","xfs"):
                raise ValueError("Bad fstype "+repr(self.fstype))
            if self.fstype == "btrfs":
                mark, spath = self._btrfs_subvol_snapshot()
            elif self.fstype == "xfs":
                pass # possibly file-lock and chmod -r rlnk snapshots
        else:
            raise ValueError(f"'{self.pooltype}' pooltype, no metadata.")

        self.locked = True
        return mark, spath

    def metadata_unlock(self, lvpool=None):
        mark_t = (0, self.path)
        if self.pooltype == "tlvm":
            for pl in {lvpool} if lvpool else {getattr(x, "pool_lv", self.lvpool)
                                               for x in self.lvols.values()}:
                if pl:   self._lvm_meta_snapshot("release", pool=pl)

        elif self.pooltype == "rlnk":
            if self.fstype == "btrfs":
                mark_t = self._btrfs_subvol_snapshot(delete=True)
        else:
            if debug:   print(f"'{self.pooltype}' pooltype, no metadata.")

        self.locked = False
        return mark_t

    def check_support(self):
        if self.pooltype == "tlvm":
            for prg in (CP.lvm, CP.dmsetup, CP.thin_delta ):
                if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)
            try:
                p = SPr.check_output([CP.thin_delta, "-V"])
            except:
                p = b""
            ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
            if p and ver < target_ver:
                raise RuntimeError("Thin provisioning tools version >= "+target_ver+" required.")
        elif self.pooltype == "rlnk" and self.fstype == "btrfs":
            if not shutil.which(CP.btrfs):  raise RuntimeError("Required command not found: btrfs")
            # maybe also check kernel version and xfs reflink support...

    def _btrfs_subvol_snapshot(self, delete=False):
        if delete and not self.can_snapshot:   return None, None

        svpath = self.path    ; dest = self.snappath    ; gen = 0    ; sv_start = time.monotonic()

        # remove snapshot at wrong path
        if exists(badpath := self.path.rstrip("/")+os.path.basename(dest.rstrip("/"))):
            do_exec([[CP.env, "-u", "LC_ALL",
                      CP.btrfs, "subvolume", "delete", "-c", badpath]], check=False)

        if debug:   print(f"\nSubvol snapshot: '{dest}' {exists(dest)} del={delete}", flush=True)
        if exists(dest):
            if delete:   gen = self._get_btrfs_generation(dest)
            do_exec([[CP.env, "-u", "LC_ALL",
                      CP.btrfs, "subvolume", "delete", "-c", dest]])
        if not delete:
            do_exec([[CP.env, "-u", "LC_ALL",
                      CP.btrfs, "subvolume", "snapshot", "-r", svpath, dest]])
            gen = self._get_btrfs_generation(dest)
            # possibly check /sys/fs/btrfs/uuid#/exclusive_operation

            if debug:   print("Created subvol (sec):", time.monotonic() - sv_start, flush=True)
        return gen, dest

    def _get_btrfs_generation(self, path):
        base = "/"+os.path.basename(path.rstrip("/"))
        res = SPr.check_output([CP.env, "-u", "LC_ALL", CP.btrfs, "subvolume", "list", path], text=True)
        res = [x for x in res.splitlines() if x.strip().endswith(base)]
        if debug:   print("*generation", res)
        rln = res[0].split()    ; assert len(res) == 1 and rln[2] == "gen"
        return int(rln[3])

    # Reserve or release lvm thinpool metadata snapshot.
    # action must be "reserve" or "release".
    def _lvm_meta_snapshot(self, action, pool=None):
        vgname   = self.vgname.replace("-","--")
        poolname = (pool or self.lvpool).replace("-","--")

        if debug:   print(f"TLVM meta snapshot: {vgname}/{poolname} {action}", flush=True)
        do_exec([[CP.dmsetup,"message", vgname+"-"+poolname+"-tpool",
                "0", action+"_metadata_snap"]], check= action=="reserve")

    def new_vol_entry(self, newname, vid, replace=False):
        if newname not in (lvols := self.lvols) or replace:
            lvols[newname] = vol = self.LVolClass(self, newname, vid=vid)
        else:
            vol = lvols[newname]    ; vol.vid = vid
        self.arch_vols[newname] = vid

        for sv in (vol.snap1, vol.snap2) if vol.snap1 else []:
            if sv and sv not in lvols:   lvols[sv] = self.LVolClass(self, sv, parent=newname)
        return vol

    # Create survey of all interesting volumes
    def update_vol_list(self, arch_vols):
        self.lvols.clear()
        if self.pooltype == "tlvm":   self.update_lvm_list()
        for vname, vid in arch_vols.items():
            self.new_vol_entry(vname, vid)

    # Retrieves survey of all LVM VGs/LVs
    def update_lvm_list(self, vol=None):
        if not shutil.which(CP.lvm):   sys.stderr.write("LVM not available.\n"); return

        delim   = ":::"            ; colnames = LvmVolume.colnames
        vgs_all = self.vgs_all     ; LvmVol   = LvmVolume

        cmd = [CP.lvm, "lvs", "--units=b", "--noheadings", "--separator="+delim,
                    "--options=" + ",".join(colnames)]
        if vol:   cmd.append(vol.path)
        do_exec([cmd], out=tmpdir+"/volumes.lst")

        for ln in open(tmpdir+"/volumes.lst", "r"):
            lv = LvmVol(self, "", members=zip(colnames, ln.strip().split(delim)))
            vgs_all.setdefault(lv.vg_name, {})[lv.name] = lv

        self.lvols = vgs_all[self.vgname]

    # static
    def vg_exists(vgname):
        try:
            do_exec([[CP.lvm, "vgdisplay", vgname]])
        except (SPr.CalledProcessError, FileNotFoundError):
            return False
        else:
            return True

    def get_fs_type(path):
        mtab = {x[1]: x[2] for x in map(str.split, open("/etc/mtab","r"))}
        while path not in mtab and path != "/":   path = os.path.dirname(path)
        return mtab[path]

    # accepts either a volgroup/pool or directory path and returns:
    # type, tvol, tpool, vg or abs path
    def parse_local_path(localpath):
        if type(localpath) is not str or not localpath.isprintable():
            return (None, None, None, None)

        abspath = os.path.abspath(os.path.realpath(localpath))
        if LocalStorage.vg_exists(localpath):
            return ("lvm volgroup", None, None, os.path.basename(localpath))

        lvname, lvpool, vg, lvattr = LocalStorage.get_lv_path_pool(localpath)

        if lvname and not lvpool and lvattr.startswith("t"):
            return ("tlvm pool", None, lvname, vg)
        elif lvname and lvpool and lvattr.startswith("V"):
            return ("tlvm volume", lvpool, lvname, vg)
        elif abspath.startswith("/dev"):
        #and stat.S_ISBLK(os.stat(localpath).st_mode):
            return ("block device", None, None, abspath)
        elif abspath.startswith("/") and exists(abspath) and os.path.isdir(abspath):
            return ("file", None, None, abspath)
        else:
            return (None, None, None, None)

    # Converts a non-cannonical LV path to LV name plus pool and vg names.
    def get_lv_path_pool(path):
        try:
            p = SPr.run([CP.lvm, "lvs", "--separator=:::", "--noheadings",
                                "--options=lv_name,pool_lv,vg_name,attr", path], check=True,
                                stdout=SPr.PIPE, stderr=SPr.DEVNULL)
        except:
            return "", "", "", ""
        else:
            return p.stdout.decode("utf-8").strip().split(":::")


# Base class for local volumes; do not instantiate.

class LocalVolume:
    maxname   = 255    ; maxpath = 4096 - maxname
    __slots__ = ("storage","lockfile","pdir","path","name","vid","snap1","snap2","parent")

    def _my_init(self, storage, name, vid=None, parent=None):
        assert storage.online

        self.storage   = storage                        ; self.lockfile = None
        self.pdir      = storage.path.rstrip("/")+"/"   ; self.path     = self.pdir+name
        self.name      = name                           ; self.vid      = vid
        self.snap1 = self.snap2 = None                  ; self.parent   = parent

    def lock(self, mode="r+b"):
        self.lockfile = lf = open(self.path, mode)
        fcntl.lockf(lf, fcntl.LOCK_EX|fcntl.LOCK_NB)
        return lf

    def unlock(self):
        if self.lockfile:   self.lockfile.close()
        self.lockfile = None
        return True

    def rotate_snapshots(self, rotate=True, timestamp_path=None, addtags=[]):
        assert not self.name.endswith(self.snap_ext)
        if debug:
            print("Rotate Snapshots", self.name, rotate, timestamp_path, addtags, flush=True)

        lvols = self.storage.lvols
        if rotate:
            if lvols[self.snap2].exists():
                t = lvols[self.snap2].gettime()    ; os.utime(timestamp_path, ns=(t,t))
                lvols[self.snap2].rename(self.snap1, addtags=addtags)
                with open(self.pdir+self.snap1,"rb") as vf:   os.fsync(vf)
                return t
        else:
            lvols[self.snap2].delete(sync=False)
        return None

    def setperms(self, perms):
        self.storage.setperms(self.path, perms)

    def settime(self, t):
        self.storage.settime(self.path, t)

    def exists(self):
        return exists(self.path)

    def check_pathname(path):
        if not 0 < len(path) <= LocalVolume.maxpath:
            return f"Volume path/name length must be 1 - {LocalVolume.maxpath}."
        if not path.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in path.strip().split("/")]):
            return "Bad volume name."


class FileVolume(LocalVolume):

    def __init__(self, storage, name, vid=None, parent=None):
        super()._my_init(storage, name, vid, parent=parent)


class ReflinkVolume(LocalVolume):

    snap_ext = (".wyng1",".wyng2")

    def __init__(self, storage, name, vid=None, parent=None):
        super()._my_init(storage, name, vid, parent=parent)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            fdir, fname  = os.path.split(name)    ; subdir = fdir+"/" if fdir else ""
            self.snap1 = "".join((subdir,"sn",self.storage.auuid,"_",self.vid,self.snap_ext[0]))
            self.snap2 = "".join((subdir,"sn",self.storage.auuid,"_",self.vid,self.snap_ext[1]))

    def rename(self, new_name, addtags=[]):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        assert self.exists()

        newvol = self.storage.lvols[new_name]
        if debug:   print("*rename", self.path, newvol.path)
        if not newvol.exists():   newvol.create(size=0, ro=False)
        self.unlock()    ; os.replace(self.path, newvol.path)

    def delete(self, sync=True, check=False, force=False):
        assert force or self.name.endswith(self.snap_ext)
        self.unlock()
        if self.exists():
            if debug:   print("Del vol", self.name, flush=True)
            os.remove(self.path)

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        assert self.storage.online and self.storage.fstype in self.storage.rltypes
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        if debug:   print("Create vol", self.name, "from", snapshotfrom, flush=True)

        subdir, fname  = os.path.split(self.name)
        os.makedirs(self.pdir+subdir, exist_ok=True)
        if snapshotfrom:
            snap_path = self.storage.lvols[snapshotfrom].path
            do_exec([[CP.env, "-u", "LC_ALL", CP.cp, "-p", "--reflink=always",
                      snap_path, self.path]])
        else:
            assert size is not None
            with open(self.path, "wb") as vf:  vf.truncate(size); vf.flush()

        if ro:   rel_chmod(self.path, "-", 0o222)

        self.storage.lvols[self.name] = self

    def update(self):
        return self

    def gettime(self):
        return os.stat(self.path).st_mtime_ns

    def resize(self, size):
        if not self.exists():   raise FileNotFoundError(self.path)
        with open(self.path, "r+b") as vf:   vf.truncate(size); vf.flush()

    def getsize(self):
        return os.path.getsize(self.path)

    def getperms(self):
        return self.storage.getperms(self.path)

    def is_arch_member(self):
        if not self.name.startswith("sn") or not self.name.endswith(self.snap_ext) \
        or not self.exists():
            return "na"
        elif self.parent and self.name.startswith("".join((subdir,"sn",self.storage.auuid,"_"))):
            return "true"
        else:
            return "false"

    def paired_state(self, mapfile, sestag):
        if not self.name.endswith(self.snap_ext):   raise ValueError("Not a snapshot.")
        if not self.exists() or not exists(mapfile) or self.gettime() != os.stat(mapfile).st_mtime_ns:
            return "N"
        elif os.stat(mapfile).st_blocks != 0:
            return "D"
        else:
            return "Y"
        #Enh: also evaluate sestag

    def convert_pathname(self, path):
        raise NotImplementedError()


class LvmVolume(LocalVolume):

    snap_ext  = (".tick",".tock")
    maxname   = 112    ; maxpath = maxname    ; alphanumsym = r"^[a-zA-Z0-9\+\._-]+$"
    colnames  = ("vg_name","lv_name","lv_attr","lv_size",
                 "lv_time","pool_lv","thin_id","tags")
    __slots__ = colnames

    def __init__(self, storage, name=None, vid=None, members=[], parent=None):

        self.tags = ""
        for attr, val in members:   setattr(self, attr, val)

        super()._my_init(storage, name or self.lv_name, vid, parent=parent)
        name = self.name    ; assert bool(name)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            self.snap1 = name + self.snap_ext[0]
            self.snap2 = name + self.snap_ext[1]

    def rename(self, new_name, addtags=[]):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        assert self.exists()
        storage = self.storage

        if addtags:   do_exec([[CP.lvm, "lvchange", self.path] + addtags])

        m = ((x, getattr(self,x)) for x in set(self.colnames) - {"lv_name","lv_path","thin_id"})
        nv = storage.lvols[new_name] = LvmVolume(storage, name=new_name, vid=self.vid, members=m)
        storage.new_vol_entry(self.name, self.vid, replace=True)

        nv.delete()
        self.unlock()
        do_exec([[CP.lvm, "lvrename", self.path, new_name]])

    def delete(self, sync=True, check=False, force=False):
        # Enh: re-write with asyncio
        sync = (optsync := options.maxsync) or sync    ; clean = options.clean # using global
        assert not (sync == False and check)
        assert force or self.name.endswith(self.snap_ext)

        if self.exists():
            self.unlock()    ; procs = self.storage.gc_procs    ; maxprocs = 16
            if len(procs) == maxprocs:
                for ii in reversed(range(len(procs))):
                    retcode = procs[ii].returncode
                    if retcode is not None:
                        if clean and retcode != 0:
                            raise CalledProcessError("lvremove failed "+str(retcode))
                        del(procs[ii])

            if debug:   print("Del vol", self.name, flush=True)
            cmds = [CP.sh, "-c", CP.lvm + " lvchange -p rw " + self.path + " ; "
                 +  CP.lvm + " lvremove -f " + self.path]
            if not (sync or clean):   cmds = [CP.ionice, "-c3"] + cmds
            p = SPr.Popen(cmds, shell=False,
                                stdout=SPr.DEVNULL, stderr=SPr.DEVNULL)
            if not sync and len(procs) < maxprocs:
                procs.append(p)
            else:
                retcode = p.wait()
                if check and retcode != 0:
                    raise CalledProcessError("lvremove failed "+str(retcode))

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        vg = self.storage.path    ; rwmode = ["-pr"+("w" if not ro else "")]
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        if debug:   print("Create vol", self.name, "from", snapshotfrom, flush=True)
        if snapshotfrom:
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay"] + rwmode + addtags + [
                        "-s", vg+"/"+snapshotfrom, "-n", self.name]])
        else:
            assert size is not None
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(size)+"b"] + rwmode
                      + addtags + ["--thin", "-n", self.name, vg+"/"+self.storage.lvpool]])

        self.storage.lvols[self.name] = self

    def update(self):
        self.storage.update_lvm_list(vol=self)
        return self.storage.lvols[self.name]

    def gettime(self):
        return int(time.mktime(time.strptime(self.lv_time, r"%Y-%m-%d %H:%M:%S %z"))*1000000000)

    def settime(self, t):
        if verbose:
            err_out("Not setting time on LVM object.")
        return False

    def resize(self, size):
        if not self.exists():   raise FileNotFoundError(self.path)
        do_exec([[CP.lvm, "lvresize", "-L", str(size)+"b", "-f", self.path]])

    def getsize(self):
        return int(re.sub("[^0-9]", "", self.lv_size))

    def getperms(self):
        return self.lv_attr[1]

    def is_arch_member(self):
        if not self.name.endswith(self.snap_ext) or "wyng" not in self.tags or not self.exists() \
        or not self.storage.auuid:
            return "na"
        if "arch-"+self.storage.auuid not in self.tags:
            return "false"
        else:
            return "true"

    def paired_state(self, mapfile, sestag):
        if not self.name.endswith(self.snap_ext):   raise ValueError("Not a snapshot.")

        if not self.exists() or not exists(mapfile) or self.gettime() != os.stat(mapfile).st_mtime_ns \
        or sestag not in self.tags.split(","):
            return "N"
        elif os.stat(mapfile).st_blocks != 0:
            return "D"
        else:
            return "Y"

    def check_pathname(path):
        if res := super.check_pathname(path):
            return res
        if re.match(LvmVolume.alphanumsym, path) is None:
            return "Only characters A-Z 0-9 . + _ - are allowed in LVM volume names."
        return ""

    def convert_pathname(self, path):
        raise NotImplementedError()


# Try to sync only selected filesystem
def fssync(path, force=False, sync=False):
    if options.maxsync or force: # global options
        rc = SPr.check_output([CP.sync,"-f",path]) if sync else SPr.Popen([CP.sync,"-f",path])


def rel_chmod(fpath, op, mask):
    assert op in ("-","+")
    current = stat.S_IMODE(os.stat(fpath).st_mode)
    os.chmod(fpath, (current & ~mask) if op=="-" else (current | mask))


def get_passphrase(prompt="Enter passphrase: ", verify=False, cmd=None):
    if cmd:
        try:
            return bytearray(SPr.check_output(CP.env + " -u LC_ALL " + cmd,
                                              shell=True, text=False).rstrip(b"\n"))
        except SPr.CalledProcessError as e:
            # Handle as error code so pw output does not get logged
            x_it(e.returncode, "Error running passcmd: "+cmd)

    def getpassinput(prompt):
        if attended:
            return getpass.getpass(prompt)
        else:
            err_out(prompt)
            return sys.stdin.readline().rstrip("\n")

    attended = sys.stdin.isatty()
    for ii in range(3):
        passphrase  = bytearray(getpassinput(prompt), encoding="UTF-8")
        if not verify or len(passphrase) >= 10 or (debug and len(passphrase) > 0):   break
        err_out("Passphrase must be 10 or more characters.")   ; clear_array(passphrase)
        if ii == 2 or not attended:   x_it(3, "Passphrase required.")
    if not attended or not verify:   return passphrase
    passphrase2 = bytearray(getpass.getpass("Re-enter passphrase: "), encoding="UTF-8")
    if passphrase != passphrase2:
        clear_array(passphrase)   ; clear_array(passphrase2)
        x_it(3, "Entries do not match.")
    clear_array(passphrase2)
    return passphrase


def clear_array(ar):
    for ii in range(len(ar)):   ar[ii] = 0


# Initialize a new ArchiveSet:

def arch_init(aset):

    if not zstd:   print("Package python3-zstd is not installed.")

    opts = aset.opts    ; data_ci = opts.encrypt or "xchacha20-dgr"
    # Fix: duplicates code in aset... move to aset class.
    if data_ci in (x[0] for x in DataCryptography.crypto_codes.values() if x[2]):
        aset.ci_mode, ci_t = [(x,y) for x,y in DataCryptography.crypto_codes.items()
                                    if y[0] == data_ci][0]

        if data_ci != "off":
            # Security Enh: Possibly use mmap+mlock to store passphrase/key values,
            #               and wipe them from RAM after use.
            passphrase      = get_passphrase(prompt="Enter new encryption passphrase: ",
                                             verify=True)
            aset.datacrypto = DataCryptography()
            aset.mcrypto    = DataCryptography()
            aset.mcrypto.load(ci_t[1], aset.saltpath, slot=1,
                                     passphrase=passphrase[:], init=True)
            aset.datacrypto.load(data_ci, aset.mcrypto.keyfile, slot=0,
                                     passphrase=passphrase)
    else:
        x_it(1,"Error: Invalid cipher option.")

    print(); print(f"Encryption    : {data_ci}", "" if data_ci=="off" else f"({ci_t[1]})")

    if opts.hashtype:
        if opts.hashtype not in hash_funcs or opts.hashtype == "sha256":
            x_it(1, "Hash function '"+opts.hashtype+"' is not available on this system.")

    # Use hmac-sha256 as data hash if the mode supports it:
    aset.hashtype = "hmac-sha256" if aset.datacrypto and aset.datacrypto.mhashkey else "blake2b"
    print("Data Hashing  :", aset.hashtype)

    if opts.compression:
        if ":" in opts.compression:
            compression, compr_level = opts.compression.strip().split(":")
        else:
            compression = opts.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not is_num(compr_level):
            x_it(1, "Invalid compression spec.")

        aset.compression = compression      ; aset.compr_level = compr_level
        compressors[compression][2](b"test", int(compr_level))

    print("Compression   : %s:%s" % (aset.compression, aset.compr_level))

    if opts.chfactor:
        # accepts an exponent from 1 to 6
        chfactor = int(opts.chfactor)
        if not ( 0 < chfactor < 7 ):
            x_it(1, "Requested chunk size not supported.")
        aset.chunksize = (aset.min_chunksize//2) * (2** chfactor)
        if aset.chunksize > 256 * 1024:
            print("Large chunk size set:", aset.chunksize)

    aset.save_conf()    ; fssync(aset.path, force=True, sync=True)
    update_dest(aset, pathlist=([aset.saltname, "salt.bak"]
                                if aset.datacrypto else []) + [aset.confname])


# Check/verify an entire archive

def arch_check(storage, aset, vol_list=None, startup=False, upgrade=False):
    options  = aset.opts                   ; error_cache.clear()
    dest     = aset.dest                   ; attended  = not options.unattended
    compare_digest = hmac.compare_digest   ; b64enc    = base64.urlsafe_b64encode

    if upgrade and aset.format_ver == 2 and sum(len(v.sessions) for v in aset.vols.values()):
        upgrade_from_v2(aset)    ; return
    elif upgrade:
        x_it(1, "Cannot upgrade.")

    # Test volume metadata loading
    for vid in aset.conf["volumes"] if not startup and dest.online else []:

        # try volume "volinfo" only
        v_errs = aset.load_volumes(1, vids=[vid], handle=True)
        for v, e in v_errs:
            err_out(v+" - "+repr(e)); error_cache.append(v)

        # Enh: Offer to reconstruct volinfo from available session data
        if v_errs:
            err_out(f"\n{vid} cannot be recovered.")
            continue

        # try sessions
        vol = [x for x in aset.vols.values() if x.vid == vid][0]
        ses_errs = vol.load_sessions(handle=True, force=True, mfdecode=True)
        seslist  = list(vol.sessions.values())    ; removed = []

        # un-loaded sessions cannot be sorted, so remove them from seslist
        for ses, sn, e in ((vol.sessions[x], x, y) for x, y in ses_errs):
            if ses in seslist:    del(seslist[seslist.index(ses)])
            removed.append(sn)   ; err_out(vol.name+" - "+sn+" - "+repr(e))

        if ses_errs:
            # find last good session and reset volume to it:

            # flag volume and sort the session list by sequence#
            error_cache.append(vol.name)    ; seslist.sort(key=lambda x: x.sequence)   ; si = -1

            # find longest coherent chain of sessions
            for si in range(len(seslist)):
                if not check_session_seq(vol, seslist=seslist, seq=si):   break
            else:
                si += 1

            if si < 1:
                err_out(f"\nVolume '{vol.name}' ({vid}) cannot be recovered.")
            else:
                sgood = seslist[si-1].name
                print(f"\nVolume '{vol.name}' error in session: {sn[2:]}"
                        f"\nLast good session: {sgood[2:]}")
                if removed and not options.unattended:
                    ans = ask_input("Rewind volume to this session to recover? ")
                    if ans.lower() in ("y","yes"):

                        print("\nRewinding volume to", sgood[2:])
                        while (ses := seslist.pop()).name != sgood:
                            removed.append(ses.name)
                        for sn in removed:
                            if sn in vol.sessions:   del(vol.sessions[ses.name])
                        vol.save_volinfo(ext=".tmp")
                        catch_signals()
                        update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
                        vol.rename_saved(ext=".tmp")
                        catch_signals(**signormal)
                        dest.run([f"rm -rf {vid}/{x}\n" for x in removed], destcd=dest.path)
                        for s in removed:
                            shutil.rmtree(f"{vol.path}/{s}", ignore_errors=True)
                        x_it(0, "Done.")
        else:
            print(f"{vol.vid}: OK", end="\r")

    # Remove orphan snapshots. Don't test storage.online bc --local might not be CoW path.
    if storage.path and exists(storage.path):
        remove_local_snapshots(storage, aset)

    vol_list = vol_list or sorted(set(aset.vols.keys()) - set(options.volex))
    vol_dirs = {x.name for x in os.scandir(aset.path) \
                       if x.is_dir() and x.name.startswith("Vol_")}

    vdir_strays = vol_dirs - set(aset.conf["volumes"].keys())
    if vdir_strays:
        print("Stray volume dirs:", vdir_strays)
        if options.clean and dest.online:
            assert len([v for v in aset.vols.values() if v.vid in vdir_strays]) == 0
            dest.run(["rm -rf"] + vdir_strays, destcd=dest.path)
            print("Stray dirs deleted.")

    # Check volume contents at various levels (dirs, metadata, content)
    for volname in vol_list if dest.online else []:
        if volname not in aset.vols:   continue

        vol = aset.vols[volname]
        if not startup or debug:   print(f"\nVolume '{volname}'", flush=True)

        # Remove session tmp dirs
        for sdir in os.scandir(vol.path):
            if sdir.name.startswith("S_") and sdir.name.endswith("-tmp"):
                if debug:   print("Removing partial session dir '%s'" % sdir.name)
                dest.run(["rm -rf " + vol.vid+"/"+sdir.name], destcd=dest.path)
                shutil.rmtree(vol.path+"/"+sdir.name, ignore_errors=True)

        if len(vol.sessions) > 1 and exists(vol.mapfile(-2)):   os.remove(vol.mapfile(-2))

        if not check_session_seq(vol):   x_it(1, "Session out of sequence error.")
        if startup:   continue

        # Remove stray volume dirs and other files
        deepclean = options.clean and options.force
        for pos in list(range(len(vol.sessions)))[:-1]:
            if exists(vol.mapfile(pos)):   os.remove(vol.mapfile(pos))

        # Check all combined manifests are correct
        print("  Checking indexes,", end="", flush=True) ; mset = []
        for ses in vol.sesnames:
            mset.append(ses)    ; check_manifest_sequence(vol, mset)
            #### FIX: implement deepclean

        # Check hashes of each session individually
        print(" data:")
        for sesname in reversed(vol.sesnames):
            if options.session and ((options.session.lower() == "newest" \
            and sesname != vol.sesnames[-1]) or "S_"+options.session < sesname):
                continue # Enh: use seq

            print(" ", sesname[2:], end=" :", flush=True)
            bcount = receive_volume(storage, vol, select_ses=sesname[2:], ses_strict=True,
                                    verify_only=2)
            #if attended:   print(bcount, "bytes OK")

def check_session_seq(vol, seslist=None, seq=None):
    seslist = seslist or vol._seslist    ; err = False
    if seq is None:   seq = len(seslist)-1
    if not seslist:   return True

    if seslist[0].previous != "None":
        err_out(f"Missing oldest. [0] = {seslist[0].name} -> {seslist[0].previous}"); err = True
    for si, ses in enumerate(seslist[1:seq+1]):
        if ses.previous != seslist[si].name:
            err_out(f"Prev Out of sequence: {ses.name} -> {seslist[si].name}"); err = True
        if ses.sequence == seslist[si].sequence:
            err_out(f"Duplicate sequence {ses.sequence} in {ses.name}"); err = True

    return not err

def check_manifest_sequence(vol, sesnames, addcol=False):
    volsize = vol.sessions[sesnames[-1]].volsize    ; aset = vol.archive    ; addr = 0
    sesname_sz = vol.sesname_sz
    manifest = merge_manifests(vol, msessions=sesnames, addcol=addcol)
    with open(manifest, "r") as mrgf:
        for addr in range(0, volsize, aset.chunksize):
            ln = mrgf.readline().strip()
            if not ln:   break
            ln1, ln2 = ln.split(maxsplit=1)
            if addcol:   ln2, ses = ln2.split()
            if ln1 != "0":
                assert len(ln1) == aset.mhash_sz    ; h1 = base64.urlsafe_b64decode(ln1)
            assert len(ln2) == sesname_sz      ; a1 = int("0"+ln2, 16)
            if addr != a1:
                print(ln); raise ValueError("Manifest seq error. Expected %d got %d." % (addr, a1))

    if addr != vol.last_chunk_addr(volsize)[0]:
        raise ValueError("Manifest range stopped short at", addr)

    os.remove(manifest)


# Remove Wyng snapshots and metadata from local system
#   archive == None: purge all wyng snapshots
#   archive != None and purge_other: purge snapshots except those linked to archive
#   archive != None and not purge other: purge only snapshots linked to archive

def remove_local_snapshots(storage, archive, del_all=False, purge=None):
    purge_other = purge == "other"    ; del_all = purge == "full"
    assert not (purge_other and not archive)

    if not storage.arch_vols:
        storage.update_vol_list({x.name: x.vid for x in archive.vols.values()})

    # Remove LVM snapshots
    for lv in (x for vg in storage.vgs_all.values() for x in vg.values()):
        if lv.name.endswith(lv.snap_ext) and "wyng" in lv.tags.split(","):

            if not archive or (purge_other and lv.is_arch_member() == "false") \
            or (del_all and lv.is_arch_member() == "true" ):
                lv.delete()
                if verbose:   print("Removed", lv.name)

    # Remove reflink snapshots by scanning local dir
    if storage.pooltype == "rlnk":
        exts = ReflinkVolume.snap_ext    ; lvols = storage.lvols
        if archive:
            archpaths = [x.path for x in lvols.values() if x.exists()]

        for ldir, _, files in os.walk(storage.path):
            for fname in (x for x in files if x.startswith("sn") and x.endswith(exts)):
                fpath = pjoin(ldir,fname)
                if not archive or (fname.startswith("sn"+archive.uuid)
                                   and (fpath not in archpaths or del_all)) \
                or (purge_other and not fname.startswith("sn"+archive.uuid)):
                    os.remove(fpath)
                    if verbose:   print("Removed", fpath)


# Parse args and global conf file

def parse_options(args, pdefs, etcconf=None):
    parser_defs_d = {x[0]: y for x, y in pdefs}
    conf_block = ("debug","force","save-to","session","unattended","volume-desc","tag","all",
                  "all-before","local-from","import-other-from","upgrade-format",
                  "skip-corrupt-chunks")

    if etcconf and exists(etcconf):
        confp = configparser.ConfigParser()    ; confp.optionxform = lambda option: option
        confp["var-global-default"] = {}       ; confp.read_file(open(etcconf, "r"))
        for k, v in confp["var-global-default"].items():
            if (kopt := "--"+k) in parser_defs_d and k not in conf_block:
                if parser_defs_d[kopt].get("action") == "append":
                    parser_defs_d[kopt]["default"] = list(filter(None, v.splitlines()))
                elif parser_defs_d[kopt].get("action") in ("store_true","store_false"):
                    parser_defs_d[kopt]["default"] = bool(int(v))
                else:
                    parser_defs_d[kopt]["default"] = v

    parser = argparse.ArgumentParser(description="")
    for x, y in pdefs:  parser.add_argument(*x, **y)
    return parser.parse_args(args)


# Get configuration settings:

def get_configs(opts):

    dest_url, local_url = opts.dest, opts.local
    dest = Destination(dest_url, user_ssh_opts=opts.opt_ssh, user_qubes_opts=opts.opt_qubes)

    if True:
        dest.detect_state(opts.dedup)

    # Check online status for certain commands.
    if not dest.online and (opts.remap  \
    or opts.action not in local_actions) and not (opts.action == "delete" and opts.clean):
        x_it(5, "Destination not ready to receive commands.")
    if not dest.writable and opts.action in write_actions \
    and not (opts.action == "delete" and opts.clean):
        x_it(5, "Destination not writable.")
    if dest.archive_ini_hash == "none" and opts.action not in local_actions+("arch-init",):
        x_it(6,"Archive not found at '%s'" % dest.spec)

    if opts.action in ("arch-init",):
        if dest.archive_ini_hash != "none":
            x_it(1, "Archive already exists: "+dest.spec)
        arch_init(ArchiveSet(tmpdir, dest, opts))
        x_it(0, "Done.")

    aset = get_configs_remote(dest, cachedir, opts)    ; os.utime(aset.path)
    if aset.updated_at is None:
        x_it(6, "Archive not found.")
    aset.apdays = opts.apdays

    return aset


# Fetch copy of archive metadata from remote/dest

def get_configs_remote(dest, base_dir, opts):

    recv_list = [(ArchiveSet.confname,  ArchiveSet.max_conf_sz),
                 (ArchiveSet.saltname, DataCryptography.max_keyfile_sz),
                 ("salt.bak", DataCryptography.max_keyfile_sz)]
    fetch_file_blobs(recv_list, tmpdir, dest, skip0=True)

    # Instantiate ArchiveSet to authenticate archive.ini
    aset = ArchiveSet(tmpdir, dest, opts, children=0, pass_agent=int(opts.authmin))
    if aset.format_ver == 2:
        if opts.action == "arch-check" and opts.upgrade_format:
            os.mkdir(v2dir := tmpdir+"/:oldv2:")
            os.replace(aset.path+"/"+aset.confname, v2dir+"/"+aset.confname)
            arch_check(None, OldArchiveSet_V2(v2dir, dest), upgrade=True); x_it(0, "")
        else:
            x_it(1, "Error:  Old V2 format. "
                    "To upgrade this archive run 'wyng arch-check --upgrade-format'.")
    else:
        assert aset.updated_at is not None

    arch_dir = base_dir + aset.subdir    ; ini = pjoin(arch_dir, ArchiveSet.confname)

    if exists(ini) and not hmac.compare_digest(aset.raw_hashval,
                                        hashlib.sha256(open(ini,"rb").read()).hexdigest()):
        try:
            cache_aset = ArchiveSet(arch_dir, dest, opts, children=0, prior_auth=aset)
        except Exception as e:
            err_out(repr(e)+"\nCached archive.ini didn't load.")
            shutil.rmtree(arch_dir)
        else:
            if cache_aset.updated_at > aset.updated_at:
                # check if any non-crypto-counter vars differ
                if aset.mcrypto and cache_aset.header != aset.header \
                or set(aset.conf) != set(cache_aset.conf) \
                or any((x != y for s in aset.conf
                  for x, y in zipln(dict(cache_aset.conf[s]).items(), dict(aset.conf[s]).items())
                  if x[0] not in ("mci_count","dataci_count","updated_at"))):
                    raise ValueError(f"Cached metadata is newer, from {cache_aset.path}\n"
                                     f"{cache_aset.updated_at} vs. {aset.updated_at}\n")
                elif aset.mcrypto:
                    # difference was only in counters, so advance them and continue
                    aset.mcrypto.set_counter(cache_aset.mci_count)
                    aset.datacrypto.set_counter(cache_aset.dataci_count)

                # Enh: Test-load the cache fully and/or fetch remote, but use largest counter#s
                # Fix: Check for .tmp versions and use local copy if all other fields match;
                # this allows recovery from interruption during send (archive.ini cadence).
            elif cache_aset.updated_at == aset.updated_at:
                err_out("Cached archive.ini differs with dest but timestamp matches.")
            else:
                if verbose:   err_out("Updating metadata cache.")
                shutil.rmtree(arch_dir+"-old", ignore_errors=True)
                os.replace(arch_dir, arch_dir+"-old")

    if not exists(ini):
        os.makedirs(arch_dir, exist_ok=True)
        for f, _ in recv_list:
            if exists(pjoin(tmpdir,f)):
                shutil.copyfile(pjoin(tmpdir,f), pjoin(arch_dir,f))

    # Initial auth successful! Fetch + auth volume metadata...
    if opts.action in ("arch-check","delete"):
        depth = 0
    elif opts.action in ("add",):
        depth = 1
    else:
        depth = 2
    return ArchiveSet(arch_dir, dest, opts, children=depth, allvols=True, prior_auth=aset)


# Returns either a single LocalStorage obj from --local, or a dict of them
# with associated list of vol name, alias pairs defined in --local-from json file:
# s = {"vg1/pool1": [("vol1",""), ("vol-xyz","xyz-alias")], ...}

# alias is used for receiving to a local vol name different from the archive vol name

def get_configs_storage(aset, require_local=True):
    storage, storagesets, vol_set = None, {}, set()
    fromlist    = aset.opts.local_from      ; local_url = aset.opts.local
    arch_vols   = {x.name: x.vid for x in aset.vols.values()}

    if fromlist and exists(fromlist):
        # Init a storage obj for each local source and subset of vols
        try:
            locallists = json.load(open(fromlist, "r"))
        except json.decoder.JSONDecodeError as e:
            x_it(1, "Empty/malformed json file")

        for url, vols in locallists.items():
            if not vols:   err_out("Empty vol list for "+url)   ; continue

            for vname, alias in vols:
                if debug:   print("alias", vname, "=", alias)
                if alias and vname in aset.vols:
                    aset.vols[vname].alias = alias    ; aset.vols[vname].aliastype = "rename"
                # Test for volname uniqueness
                if vname in vol_set:   x_it(1, f"Duplicate volume name: {vname}")
                vol_set.add(vname)

            storagesets[LocalStorage(url, require_online=require_local,
                                     arch_vols=arch_vols, auuid=aset.uuid)] \
                        = [x[0] for x in vols]

    else:
        storage = LocalStorage(local_url or None, require_online=require_local,
                                arch_vols=arch_vols, auuid=aset.uuid)
        storagesets[storage] = []

    return storage, storagesets


class Destination:

    url_types   = ("ssh", "file", "qubes-ssh", "qubes")

    ssh_opts    = ["-x", "-T", "-e", "none", "-o", "ControlPath=~/.ssh/ctrl-%C",
                         "-o", "ControlMaster=auto", "-o", "ControlPersist=120",
                         "-o", "ServerAliveInterval=60", "-o", "ConnectTimeout=60",
                         "-o", "Compression=no"]

    qubes_opts  = ["--no-color-stderr", "--no-color-output"]

    magic       = b"\xff\x11\x15"

    def __init__(self, dest_url, user_ssh_opts=[], user_qubes_opts=[]):

        # parse and validate dest spec
        self.ssh_opts   = user_ssh_opts or self.ssh_opts
        self.qubes_opts = user_qubes_opts or self.qubes_opts
        if not dest_url:                 x_it(1,"Error: Missing dest specification.")
        if not dest_url.isprintable():   x_it(1,"Error: [^control] not allowed in dest.")
        dparts      = urlparse(dest_url)    ; self.dtype = dtype = dparts.scheme
        if dtype not in self.url_types:
            x_it(1,"'%s' not an accepted dest type." % dtype)
        if (dtype == "file" and (dparts.netloc or not dparts.path)) \
        or (dtype in ("ssh","qubes","qubes-ssh") and not dparts.netloc) \
        or (dtype == "qubes-ssh" and not all(dparts.netloc.partition(":"))) :
            x_it(1,"Error: Malformed --dest specification.")

        self.spec   = dest_url
        self.sys    = dparts.netloc
        self.path   = os.path.normpath(dparts.path)
        self.archive_ini_hash = "none"
        self.free   = self.dtmp     =  None      ; self.last_chunkdir    = ""
        self.online = self.writable =  False

        self.run_map = {"file":       [CP.sh],
                        "ssh":        [CP.ssh] + self.ssh_opts + ["ssh://"+self.sys],
                        "qubes":      [CP.qvm_run] + self.qubes_opts
                                      + ["-p", self.sys],
                        "qubes-ssh":  [CP.qvm_run] + self.qubes_opts
                                      + ["-p", self.sys.split(":")[0]]
                        }


    def remove_dtmp(self):
        if self.online:
            tmpprefix = "/tmp/wyngrpc/"
            self.run([f"rm -rf {tmpprefix}tmp*"], timeout=10)
            if self.dtype == "qubes-ssh":
                self.run([f"rm -rf {tmpprefix}tmp*"], direct=True)

    def get_free(self, fpath):
        for ln in open(fpath,"r"):
            if ln.startswith("wyng_check_free"):  self.free = int(ln.split()[1])
        return self.free

    # Run system commands on destination
    def run(self, commands, direct=False, timeout=None, destcd=None, lc="C",
            infile="", inlines=None, out="", check=True, trap=False):

        if direct:
            cmd = self.run_map[self.dtype] + commands
        else:
            cmd = self.run_args(commands, trap=trap, destcd=destcd, lc=lc)

        if debug:   print(cmd)
        return do_exec([cmd], infile=infile, inlines=inlines, out=out,
                       check=check, timeout=timeout)

    # Build command lists that can be shunted to remote systems.
    # The input commands are stored in a temp file and a standard command that
    # runs the temp file is returned.

    def run_args(self, commands, trap=False, dest_type=None, destcd=None, lc="C"):

        dest_type = dest_type or self.dtype    ; tmpprefix = "/tmp/wyngrpc/"
        trapcmd   = "trap '' INT TERM QUIT ABRT ALRM TSTP USR1\n" if trap else ""
        # shunt commands to local tmp file
        with tempfile.NamedTemporaryFile(dir=tmpdir+"/rpc", delete=False) as tmpf:
            cmd = trapcmd + shell_prefix \
                + (f"cd '{destcd}'\n" if destcd else "") \
                + (f"export LC_ALL={lc}\n" if lc else "") \
                + " ".join(commands) + "\n"
            tmpf.write(bytes(cmd, encoding="UTF-8"))
            remotetmp = os.path.basename(tmpf.name)

        if dest_type in {"qubes","qubes-ssh"}:
            do_exec([[CP.qvm_run, "--no-color-stderr", "--no-color-output",
                    "--no-filter-escape-chars", "-p",
                      (self.sys if dest_type == "qubes" else self.sys.split(":")[0]),
                      (CP.mkdir+" -p "+tmpprefix+"; " if not self.dtmp else "")
                      + CP.cat+" >"+tmpprefix+remotetmp
                    ]], infile=pjoin(tmpdir,"rpc",remotetmp))
            if dest_type == "qubes":
                add_cmd = [CP.sh+" "+tmpprefix+remotetmp]
            else:
                add_cmd = [CP.ssh+" "+" ".join(self.ssh_opts)
                        +" ssh://"+self.sys.split(":", maxsplit=1)[1]
                        +' "$('+CP.cat+' '+tmpprefix+remotetmp+')"']

        elif dest_type == "ssh":
            #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
            add_cmd = [cmd]

        elif dest_type == "file":
            add_cmd = [pjoin(tmpdir,"rpc",remotetmp)]

        return self.run_map[dest_type] + add_cmd


    def detect_state(self, dedup):

        if self.dtype == "qubes-ssh":
            # fix: possibly remove dargs and use dest.run()
            dargs = self.run_map["qubes"][:-1] + [self.sys.split(":")[0]]

            cmd = dargs + [shell_prefix + "mkdir -p /tmp/wyngrpc"]
            do_exec([cmd])

        tmpprefix = "/tmp/wyngrpc/"    ; tmpdigits = 12
        cmd  = [r"mkdir -p '"+self.path+r"' && cd '"+self.path+"'"

                # send helper program to remote dest
                +r"  && mkdir -p -m 777 " + tmpprefix
                +r"  && tdir=$(mktemp -d " + tmpprefix +"tmp"+ ("X"*tmpdigits) + r") && echo $tdir"
                +r"  && cat >$tdir/dest_helper.py"

                # check free space and archive.ini status on remote
                +r"  && echo -n 'wyng_check_free ' && df -P . | tail -1"
                +r"  && echo -n 'wyng_archive_ini '"
                +r"  && { if [ -e archive.ini ]; then sha256sum archive.ini; else echo none; fi }"

                # test write access and hardlinks
                +r"  && touch archive.dat && echo 'wyng_writable'"
                +(r" && ln -f archive.dat .hardlink" if dedup else "")
                ]
        try:
            online = \
                not do_exec([self.run_args(cmd),
                            # sanitize remote output:
                            [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]],
                            pipefail=True,
                            out=tmpdir+"/dest-state.log", infile=tmpdir+"/rpc/dest_helper.py")
        except SPr.CalledProcessError as e:
            online = False    ; err_out(repr(e))

        if online:
            for ln in open(tmpdir+"/dest-state.log","r"):
                if ln.startswith("wyng_archive_ini"):
                    self.archive_ini_hash = ln.split()[1]
                elif ln.startswith("wyng_check_free"):
                    parts = ln.split()    ; self.free = int(parts[4]) * 1024
                elif ln.startswith("wyng_writable"):
                    self.writable = True
                elif ln.startswith(tmpprefix):
                    self.dtmp = ln.strip()

            if not self.dtmp or len(self.dtmp) != len(tmpprefix)+tmpdigits+3 \
            or not set(self.dtmp[5:]) <= set(string.ascii_letters + string.digits + "/"):
                raise ValueError("Missing or malformed tmp dir: "+repr(self.dtmp))
        else:
            for log in ("/dest-state.log", "/err.log"):
                if exists(tmpdir+log):
                    do_exec([ [CP.cat, "-v", tmpdir+log],  [CP.tail, "--bytes=1000"],
                              [CP.grep, "-vFx", " --+--"] ],
                              out=tmpdir+log+"-out", check=False)
                    err_out(open(tmpdir+log+"-out", "r").read())

        self.online = self.free is not None


    def write_helper_program(path):

        dest_program = r'''#  Copyright Christopher Laprise 2018-2024
#  Licensed under GNU General Public License v3. See wyng-backup/LICENSE file.
import os, sys, time, signal, shutil, subprocess as SPr, gzip, tarfile

def fssync(path, force=False):
    if msync or force:   misc_procs.append(SPr.Popen(["sync","-f",path]))

def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1"], iflag=False):
    for sval in (getattr(signal,"SIG"+x) for x in sel):
        signal.signal(sval, handle_signal) ; signal.siginterrupt(sval, iflag)

def handle_signal(sig, frame):
    if sig == signal.SIGALRM:   raise IOError("Timeout ALRM")

def x_it(ec, text=None):
    if text:   sys.stderr.write(text+"\n")
    sys.exit(ec)

def helper_send(volid=None, ses=None):
    mkdirs = os.makedirs    ; hlink = os.link    ; dirname = os.path.dirname
    print(volid, ses)    ; assert volid.startswith("Vol_") and ses.startswith("S_")

    with tarfile.open(mode="r|", fileobj=sys.stdin.buffer) as tarf:
        extract = tarf.extract    ; substitutions = {}    ; dirlist = set()
        for member in tarf:   destpath = member.name    ; break
        os.chdir(destpath)
        for i in os.scandir(volid):
            if i.path.endswith("-tmp") and not i.path.endswith(ses+"-tmp"):
                shutil.rmtree(i.path, ignore_errors=True)
        for member in tarf:
            if not member.islnk():
                extract(member, set_attrs=False)
            else:
                source = src_orig = member.linkname   ; dest = member.name   ; ddir = dirname(dest)
                if source in substitutions:   source = substitutions[source]
                if ddir not in dirlist:   mkdirs(ddir, exist_ok=True)   ; dirlist.add(ddir)
                try:
                    hlink(source, dest)
                except OSError as err:
                    print(err)    ; print("Substitution:", source, dest)
                    shutil.copyfile(source, dest)    ; substitutions[src_orig] = dest
            tarf.members = []
    fssync(".", force=True)

def helper_receive(lstf):
    stdout_write = sys.stdout.buffer.write   ; exists = os.path.exists   ; getsize= os.path.getsize
    stdout_flush = sys.stdout.buffer.flush   ; magicm = magic            ; fname, fsize= "(none)",0
    for line in lstf:
        fname = line.strip()
        if not fname:   break
        if not exists(fname):
            sys.stderr.write(fname+" does not exist!\n")
        fsize = getsize(fname) if exists(fname) else 0
        stdout_write(magicm + fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:   fdata = dataf.read(fsize)
            if len(fdata) != fsize:   x_it(20, f"Size mismatch: {len(fdata)} vs {fsize}")
            stdout_write(fdata); stdout_flush()
    stdout_write(magicm); stdout_write(b'\xFF'*128); stdout_flush(); time.sleep(0.2)
    sys.stdout.close()
    sys.stderr.write(f"Last item: {fname} {fsize}\n")

def helper_merge():
    exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
    try:
        if resume:
            if exists("merge-init") or not exists("merge"):
                raise RuntimeError("Merge: Init could not complete; Aborting merge.")
        else:
            print("Merge: Initialization.")
            replace("archive.ini", "../archive.ini")
            for f in (target+"/info", "volinfo", "archive.ini"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")    ; fssync(".")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):   replace(i.path, i.name)
        elif not exists("merge"):
            for f in (target+"/info.tmp", target+"/manifest.z.tmp", "volinfo.tmp", "merge.lst.gz"):
                if exists(f):   os.remove(f)
        fssync(".")    ; print(err)    ; sys.exit(50)
    try:
        os.chdir("merge")  #  CD
        if not resume or not exists("CHECK-mv-rm"):
            print("Merge: remove/replace files.")    ; subdirs = set()
            for src in src_list:  # Enh: replace os.scandir w manifest method
                for i in os.scandir(src):
                    if i.is_dir():   subdirs.add(i.name)
            for sdir in subdirs:   os.makedirs(merge_target+"/"+sdir, exist_ok=True)
            for line in lstf:
                ln = line.split() # default split() does strip()
                if ln[0] == "rename" and (not resume or exists(ln[1])):
                    replace(ln[1], ln[2])
                elif ln[0] == "-rm" and exists(ln[1]):
                    remove(ln[1])
            open("CHECK-mv-rm","w").close()
        os.chdir("..")     #  CD
    except Exception as err:
        print(err)    ; sys.exit(60)

def helper_merge_finalize():
    try:
        print("Merge: Finalize target")
        m = "merge/"    ; open(m+"CHECK-start-finalize","w").close()
        if not resume or exists(m+target+"/info.tmp"):
            replace(m+target+"/info.tmp", m+merge_target+"/info")
        if exists(m+target+"/manifest.z.tmp"):
            replace(m+target+"/manifest.z.tmp", m+merge_target+"/manifest.z")
        if not resume or not exists(target):         replace(m+merge_target, target)
        if not resume or exists("volinfo.tmp"):      replace("volinfo.tmp", "volinfo")
        if not resume or exists("archive.ini.tmp"):  replace("archive.ini.tmp", "../archive.ini")
        if not exists(target):   raise FileNotFoundError(target)
        fssync(".")
    except Exception as err:
        x_it(70, repr(err))
    shutil.rmtree("merge", ignore_errors=True)    ; remove("merge.lst.gz")
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

## MAIN ##
cmd = sys.argv[1]   ; msync = "--sync" in sys.argv   ; magic  = b"\xff\x11\x15"
tmpdir = os.path.dirname(os.path.abspath(sys.argv[0]))  ; misc_procs = []
exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
if cmd in ("receive","dedup") and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
else:
    lstf = None

if cmd == "merge":
    src_list = []    ; resume = "--resume" in sys.argv    ; mtpath = sys.argv[2]
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    if resume and not (exists("merge.lst.gz") and exists("merge")):
        print("Error: Remote dir not initialized.")
        sys.exit(50 if exists(mtpath) else 40)
    lstf = gzip.open("merge.lst.gz", "rt")
    merge_target, target = lstf.readline().split()
    while True:
        ln = lstf.readline().strip()
        if ln == "###":  break
        src_list.append(ln)
    if "--finalize" not in sys.argv:
        helper_merge()
    catch_signals()
    helper_merge_finalize()
elif cmd == "receive":
    helper_receive(lstf if lstf else sys.stdin)
elif cmd == "send":
    helper_send(volid=sys.argv[2], ses=sys.argv[3])
elif cmd == "dedup":
    ddcount = 0    ; substitutions = {}
    for line in lstf:
        source, dest = line.split()   ; src_orig = source    ; deststat = os.stat(dest)
        ddcount += deststat.st_size
        if source in substitutions:   source = substitutions[source]
        if os.stat(source).st_ino != deststat.st_ino:
            try:
                os.link(source, dest+"-lnk")    ; replace(dest+"-lnk", dest)
            except OSError as err:
                try:
                    remove(dest+"-lnk")
                except:
                    pass
                if err.errno == 31:
                    # source has too many links; substitute
                    substitutions[src_orig] = dest   ; ddcount -= deststat.st_size   ; continue
                else:
                    print(err)   ; raise err
    print(ddcount, "bytes reduced.")
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

sys.stderr.write("--==--\n")
'''
        with open(path+"/dest_helper.py", "wb") as progf:
            progf.write(bytes(dest_program, encoding="UTF-8"))

    #####>  End dest_helper program  <#####


# Run system commands with pipes, without shell:
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'. 'inlines' can be a list-like collection of strings
# to be used as input instead of 'infile'.
# List of commands may include 'None' instead of a child list; these will be ignored.
# 'getctl' returns a tuple(control_execs <generator>, close <func>, (inf, outf, errf))
# without waiting. The generator allows communicate() and checks to occur in tandem with other
# commands. The context may be fed into another do_exec invocation via 'otherctl', which will
# automatically process the separate pipe chains in tandem.

def do_exec(commands, cwd=None, check=True, out="", infile="", inlines=[], text=False,
            pipefail=False, timeout=None, getctl=False, otherctl=None):
    ftype   = "t" if text else "b"
    if issubclass(type(out), io.IOBase):
        outf = out
    else:
        outmode = "a" if out.startswith(">>") else "w"    ; out = out.lstrip(">>")
        if cwd and out and out[0] != "/":   out = pjoin(cwd,out)
        outfunc = gzip.open if out.endswith(".gz") else open
        outf    = outfunc(out, outmode+ftype) if out else SPr.DEVNULL

    if inlines:
        inf = SPr.PIPE  #io.StringIO("\n".join(inlines)+"\n")
    else:
        if cwd and infile and infile[0] != "/":   infile = pjoin(cwd,infile)
        infunc  = gzip.open if infile.endswith(".gz") else open
        inf     = infunc(infile, "r"+ftype) if infile else SPr.DEVNULL

    errf = open(tmpdir+"/err.log", "a")  ; print(" --+--", file=errf)   ; err = None
    if debug:   print(commands, file=errf, flush=True)

    # Start each command, linking them via pipes
    commands = list(filter(None, commands))    ; procs = []    ; start_t = time.monotonic()
    for i, clist in enumerate(commands):
        p = SPr.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else SPr.PIPE,
                             stderr=errf)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)

    try:
        if inlines:   procs[0].communicate(("\n".join(inlines)+"\n").encode("UTF-8"),
                                            timeout=timeout)
    except SPr.TimeoutExpired:
        pass

    # Monitor and control processes
    def _iterate_execs():
        while True:
            err = None    ; finish = to_flag = False
            for p1 in reversed(procs):
                retcode = p1.poll()
                if not finish and retcode is None:
                    try:
                        p1.communicate(timeout=2)
                    except SPr.TimeoutExpired:
                        to_flag = True
                        continue
                    retcode = p1.returncode   ; finish = True
                    if check and (retcode != 0):
                        err = p1              ; finish = True
                elif finish and retcode is None:
                    p1.terminate()
                    continue

            if err or not to_flag or (timeout and time.monotonic() - start_t > timeout):
                break
            else:
                yield err

    def control_execs():
        for ii in zipln(_iterate_execs(), otherctl[0]() if otherctl else []):
            yield ii

    def close():
        other_rcs = otherctl[1]() if otherctl else [] # close other
        rclist = [0] + list(filter(bool if pipefail else str, ( x.returncode for x in procs )))

        if check and any(rclist): # Enh: test pipefail and/or last rc here
            print(repr(rclist), file=errf, flush=True)
            raise SPr.CalledProcessError([x for x in rclist if x][0], commands)

        for f in [inf, outf, errf]:
            if type(f) is not int: f.close()

        return other_rcs + rclist

    if getctl:
        return control_execs, close, (inf, outf, errf)

    for ii in control_execs() if otherctl else _iterate_execs():   pass
    return close()[-1]


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Session objects.
# Returns False if local and dest hashes match.

def compare_files(arch, pathlist=[], volumes=[], sessions=[], clear=True, manifest=False):
    dest = arch.dest    ; cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions) and not x.meta_checked]
    realses   = [x for x in sessions if not x.meta_checked and not x.just_fetched]
    if len(volumes)+len(sessions)+len(pathlist) == 0:   return False

    with open(cmp_list, "a") as flist:
        for pth in pathlist:
            #if not exists(pth):   raise FileNotFoundError(pth)
            print(pth, file=flist)
        for v in realvols:
            v.meta_checked = True    ; print(v.vid+"/volinfo", file=flist)
        for s in realses:
            s.meta_checked = True
            for sf in ["info"] + (["manifest.z"] if manifest else []):
                print(pjoin(s.volume.vid,s.name,sf), file=flist)

    do_exec([[CP.xargs, CP.sha256sum]], cwd=arch.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest.run(["xargs sha256sum"],
             destcd=dest.path, infile=cmp_list, out=tmpdir+"/compare-hashes.dest")
    # maybe switch cmp to diff/sha256sum if they are safe enough to read untrusted input
    files  = [tmpdir+"/compare-hashes.local", tmpdir+"/compare-hashes.dest"]
    result = do_exec([[CP.cmp] + files], check=False) > 0

    return result


## Enh: Create aset functions to supply meta filenames for objects

def update_dest(arch, pathlist=[], volumes=[], sessions=[], ext="", delete=False):
    lcd = arch.path    ; dest = arch.dest

    update_list = [x.volume.vid+"/"+x.name+"/manifest.z" for x in sessions] \
                + [x.volume.vid+"/"+x.name+"/info" for x in sessions] \
                + [x.vid+"/volinfo" for x in volumes] + pathlist

    if delete:
        assert not ext;  dest.run(["xargs rm -f"], destcd=dest.path, inlines=update_list)
        return

    do_exec([[CP.tar,"-cf","-","--no-recursion","--verbatim-files-from","--files-from", "-"],
             dest.run_args(["tar -o -xf -"]
                            + [" && mv '"+x+ext+"' '"+x+"'" for x in update_list if ext],
                            destcd=dest.path)
            ], inlines=[x+ext for x in update_list],
            cwd=lcd)


def fetch_file_blobs(recv_list, recv_dir, dest, ext="", skip0=False, skip_exists=False,
                     verifier=None):

    magic = dest.magic    ; exists = os.path.exists
    recv_list = [(x,y) for x,y in recv_list if not skip_exists or not exists(recv_dir+"/"+x)]
    if not recv_list:   return []

    cmd = dest.run_args(
            [ f"exec 2>>{dest.dtmp}/receive.log"
             +f" && rm -f {dest.dtmp}/dest.lst.gz && python3 {dest.dtmp}/dest_helper.py receive"
            ], destcd=dest.path)
    recvp = SPr.Popen(cmd, stdout=SPr.PIPE, stdin=SPr.PIPE)
    recvp.stdin.write(("".join((x+"\n" for x,y in recv_list))).encode("UTF-8"))
    recvp.stdin.flush()    ; rc = recvp.poll()    ; recvp.stdin.close()

    for fname, fsz in list(recv_list):
        if rc is not None:   raise RuntimeError("Process terminated early. rc="+repr(rc))
        fpath = recv_dir+"/"+fname+ext
        if exists(fpath):   os.remove(fpath)
        assert recvp.stdout.read(3) == magic
        # Read chunk size
        assert len(sizeb := recvp.stdout.read(4)) == 4
        untrusted_size = int.from_bytes(sizeb,"big")
        if untrusted_size == 0 and skip0:   continue
        # Check for reasonable size to avoid overflow:
        if not fsz + 2000 >= untrusted_size > 0:
            raise BufferError(f"Bad file size {untrusted_size}, expected 1-{fsz + 2000} max.\n"
                              f"{fpath} - {skip0}")

        # Size is OK.
        size = untrusted_size
        # Read chunk buffer
        untrusted_buf = recvp.stdout.read(size)    ; rc  = recvp.poll()
        if rc is not None:   raise RuntimeError("Process terminated early. rc="+repr(rc))
        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))
        if verifier:   verifier.auth(untrusted_buf)

        os.makedirs(os.path.dirname(fpath), exist_ok=True)
        with open(fpath, "wb") as outf:   outf.write(untrusted_buf)
        if debug:   print("Fetched", fname, len(untrusted_buf))

    rc  = recvp.poll()
    assert recvp.stdout.read(3) == magic
    recvp.kill()
    return recv_list


# Prepare snapshots and check consistency with metadata:
# Normal use will have a snap1 snapshot of the volume already in place.  Here we
# create a fresh snap2 so 'update_delta_digest' can compare it to the older snap1
# and then rotate snap2 -> snap1.

def prepare_snapshots_lvm(storage, aset, datavols, monitor_only, other_vols={}):
    incr_vols, complete_vols, unchanged_vols = [], [], []   ; lvols = storage.lvols

    for datavol in datavols + list(other_vols.keys()):

        if not exists(vpath := other_vols.get(datavol) or pjoin(storage.path, datavol)):
            err_out("Warning: Local '%s' does not exist!" % vpath)
            if datavol in other_vols:   del(other_vols[datavol])
            continue

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, aset.opts.voldesc)

        vol   = aset.vols[datavol]    ; mapfile = vol.mapfile()
        if vol.aliastype == "import_other":   continue
        if datavol in other_vols:
            print("  Queuing full scan of import '%s'" % datavol)
            vol.alias, vol.aliastype = other_vols[datavol], "import_other"
            continue

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        l_vol = lvols[datavol]   ; snap1, snap2 = l_vol.snap1, l_vol.snap2

        if lvols[snap1].is_arch_member() == "false":
            if aset.opts.remap:
                l_vol.delete    ; print("  Removed mis-matched snapshot", datavol)
            else:
                print("  Skipping %s; snapshot is from a different archive. Use --remap to clear it."
                      % datavol)   ; error_cache.append(datavol)
                continue

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing timestamps; a match means remap is unnecessary.
        if len(vol.sessions):
            s1tags = lvols[snap1].tags.split(",")

            if not exists(mapfile) and lvols[snap1].exists() \
            and vol.last in s1tags and "delta" not in lvols[snap1].tags:
                # Latest session matches current snapshot; OK to make blank map.
                vol.init_deltamap(timestamp=lvols[snap1].gettime())

            if not lvols[snap1].exists() or not exists(mapfile)    \
            or os.stat(mapfile).st_mtime_ns != lvols[snap1].gettime() \
            or vol.last not in s1tags:

                # Handle inadvertant mapfile snapshot mis-match
                lvols[snap1].delete()
                print("  Removed mis-matched snapshot for '%s'" % datavol)

            if not lvols[snap1].exists() and lvols[snap2].exists() \
            and vol.last in lvols[snap2].tags.split(",") and exists(mapfile)  \
            and os.stat(mapfile).st_mtime_ns == lvols[snap2].gettime():
                lvols[snap2].rename(snap1)
                print("  Recovered interrupted snapshot rotation:", datavol)
 
        elif monitor_only:
            print("  Skipping '%s'; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            incr_vols.append(datavol)
        elif not monitor_only:
            print("  Queuing full scan of '%s'" % datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping '%s'; No paired snapshot." % datavol)
            error_cache.append(datavol)    ; continue

        # Make fresh snap2vol
        lvols[snap2].delete()
        tagopts = ["--addtag=wyng", "--addtag=arch-"+aset.uuid]
        if monitor_only:   tagopts.append("--addtag=delta")
        lvols[snap2].create(snapshotfrom=datavol, ro=True, addtags=tagopts)

    return incr_vols, complete_vols, unchanged_vols, other_vols


def prepare_snapshots_reflink(storage, aset, datavols, monitor_only, other_vols={}):
    incr_vols, complete_vols, unchanged_vols = [], [], []   ; lvols = storage.lvols

    for datavol in datavols + list(other_vols.keys()):

        if not exists(vpath := other_vols.get(datavol) or pjoin(storage.path, datavol)):
            err_out("Warning: Local '%s' does not exist!" % vpath)
            if datavol in other_vols:   del(other_vols[datavol])
            continue


        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, aset.opts.voldesc)

        vol   = aset.vols[datavol]    ; mapfile = vol.mapfile()
        if vol.aliastype == "import_other":   continue
        if datavol in other_vols:
            print("  Queuing full scan of import '%s'" % datavol)
            vol.alias, vol.aliastype = other_vols[datavol], "import_other"
            continue

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        l_vol = lvols[datavol]   ; snap1, snap2 = l_vol.snap1, l_vol.snap2

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing timestamps; a match means remap is unnecessary.
        if len(vol.sessions):
            lastses = vol.sessions[vol.last]

            if lvols[snap1].exists() and lvols[snap1].gettime() == lvols[datavol].gettime() \
            and lvols[snap1].getsize() == lvols[datavol].getsize() \
            and exists(mapfile) and vol.map_used() == 0 and not options.send_unchanged:
                unchanged_vols.append(datavol)   ; continue

            if not exists(mapfile) and lvols[snap1].exists() \
            and lastses.gettime() == lvols[snap1].gettime()  \
            and lastses.volsize == lvols[snap1].getsize():
                # Create deltamap if its missing but datavol and lastses timestamps align
                vol.init_deltamap(timestamp=lvols[snap1].gettime())

            if exists(mapfile) and not lvols[snap1].exists() \
            and os.stat(mapfile).st_mtime_ns == l_vol.gettime() == lastses.gettime() \
            and lastses.volsize == l_vol.getsize():
                # Create snap1 from datavol if its missing but other timestamps align
                lvols[snap1].create(snapshotfrom=datavol, ro=True)
                # Avoid race condition
                if lvols[snap1].gettime() != lastses.gettime():   lvols[snap1].delete()

            if not lvols[snap1].exists() or not exists(mapfile) \
            or (os.stat(mapfile).st_mtime_ns != lvols[snap1].gettime()
                and os.stat(mapfile).st_blocks == 0):
                # Handle inadvertant mapfile snapshot mis-match
                lvols[snap1].delete()
                print("  Removed mis-matched snapshot for '%s'" % datavol)

        elif monitor_only:
            print("  Skipping '%s'; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            incr_vols.append(datavol)
        elif not monitor_only:
            print("  Queuing full scan of '%s'" % datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping '%s'; No paired snapshot." % datavol)
            error_cache.append(datavol)    ; continue

        # Make fresh snap2vol
        lvols[snap2].delete()
        lvols[snap2].create(snapshotfrom=datavol, ro=True)

    return incr_vols, complete_vols, unchanged_vols, other_vols


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(storage, aset, datavols):

    #Enh: construct 'poolset' as vg,pool pairs; change vgname handling to track 'apool'
    vgname   = storage.vgname.replace("-","--")    ; lvols = storage.lvols
    poolset  = set(lvols[x].pool_lv for x in datavols)
    assert aset.chunksize % storage.block_size == 0

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    catch_signals()
    for apool in sorted(poolset):
        poolname = apool.replace("-","--")
        try:
            storage.metadata_lock(apool)
            for datavol in datavols:
                lv = lvols[datavol]
                if lv.pool_lv != apool:   continue
                if debug:   print("Get tlvm deltas for", datavol, flush=True)

                cmds = [[CP.thin_delta, "--metadata-snap",
                                        "--thin1=" + lvols[lv.snap1].thin_id,
                                        "--thin2=" + lvols[lv.snap2].thin_id,
                                        "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", r"^\s*<same .*\/>$"],
                        [CP.gzip, "-2"]
                        ]
                do_exec(cmds,  out=tmpdir+"/delta."+datavol)
        except Exception as e:
            err_out("ERROR running thin_delta process.")
            raise e
        finally:
            storage.metadata_unlock(apool)

    catch_signals(**signormal)
    return datavols


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest_lvm(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = lvols[vol.name].snap1      ; snap2vol   = lvols[vol.name].snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    volperms    = vol.sessions[vol.last].permissions
    assert len(vol.sessions) and exists(vol.mapfile())
    assert storage.block_size == 512

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(gzip.open(tmpdir+"/delta."+datavol,"r")).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    dnewchunks  = isnew  = anynew = dfreedblocks = 0

    # Check for volume perms & size changes;
    # Chunks from 'markall_pos' onward will be marked for backup.
    changed = snap2size != vol.volsize() or storage.lvols[datavol].getperms() != volperms
    next_chunk_addr  = vol.last_chunk_addr()[0] + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile(), "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // storage.block_size
        chunkblocks  = chunksize // storage.block_size
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                isnew = anynew = 1
                #if delta.tag == "right_only":   tally -= blocklen
            elif delta.tag == "left_only":
                isnew = 0    ; dfreedblocks += blockend - blockbegin
                #tally += blocklen
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm, dtree)
        if dnewchunks+dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())
        if not debug:   os.remove(tmpdir+"/delta."+datavol)

    catch_signals()
    changed = changed or dnewchunks+dfreedblocks+anynew+vol.map_used() > 0
    if monitor_only:
        t1   = lvols[snap2vol].gettime()
        tags = ["--addtag="+vol.last, "--addtag=delta-"+str(t1)]
        t2   = lvols[datavol].rotate_snapshots(rotate=changed, timestamp_path=vol.mapfile(),
                                               addtags=tags)
        assert t1 == t2
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if changed else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    catch_signals(**signormal)
    return changed, snap2size, volperms


def get_reflink_deltas(storage, aset, datavols):

    assert aset.chunksize % storage.block_size == 0
    blksz = str(storage.block_size)
    generation, stpath = storage.metadata_lock()    ; lvols = storage.lvols

    for vol in (aset.vols[x] for x in datavols[:]):
        lv = lvols[vol.name]                        ; cmds, devs = [], set()
        dpath = tmpdir+"/delta."+vol.vid

        # Create a pair of piped command sets for diffing metadata of the two snapshots:
        for side, snapname, outp in (("11", lv.snap1, ' > "'+dpath+'-p"'), ("22", lv.snap2, "")):
            if not exists(stpath+snapname):
                err_out("'%s' snapshot not found (different subvolume?); Skipping..." % fpath)
                datavols.remove(vol.name);   break

            # filefrag output: file extent ranges, their 'physical' block ranges, extent length
            # sort -m will merge this output from two vols (file extent #s are pre-sorted)
            # uniq -u will filter-out exact extent/phys/len matches between the two vols
            # The result will be a list of extent ranges that have changed.

            devs.add(os.stat(stpath+snapname).st_dev)
            cmds.append([
              [CP.filefrag, "-vs", "-b"+blksz, stpath+snapname],
              # BEGIN defines the expected filefrag header layout (PAT);
              # a non-matching line (00..wyng_input) is printed if 0 extents found.
              # END exits with code 2 if no end line (extents found) was found or HEADER != PAT;
              # printed fields are arranged to complement 'uniq' as it can only skip fields and
              # then must compare the rest of the line.  gsub removes punctuation.
              # 'outp' is used to redirect output for only one side (other side goes to stdout).
              [CP.awk, r'BEGIN {PAT="^\\s*ext:\\s+logical_offset:\\s+physical_offset:\\s+length:"}'
                       r' NR==1, $0 ~ PAT {if ($0 ~ /\s0 extents found$/) {ZXT="Y";'
                       r' print "00 0 1 0 0 0 wyng_input"' +outp+ r'} HEADER=$0; next}'
                       r' END {if (!(ZXT=="Y" || HEADER ~ PAT && $0 ~ /extents? found$/)) exit 2}'
                       r' /^\s*[0-9]+:/ {gsub(/\.|:/, " ", $0);'
                                        r' print '+side+r', $2, $6, $3, $4, $5, $NF'+outp+r'}']
             ]
            )

        if vol.name not in datavols:   continue
        assert len(devs) == 1
        if debug:   print("Get rlnk deltas for", vol.name, flush=True)

        # Save output from first volume to a named pipe. 'getctl' means run it async; below we
        # will add it to the sort/uniq pipeline via 'otherctl'. This prevents tmp files
        # being created, which would be very large due to high granularity of fiemap data.
        os.mkfifo(dpath+"-p")
        p11context = do_exec(cmds[0], timeout=60, getctl=True)
        #Enh: maybe use dev+inode as fname

        # Pipe output from second volume to 'sort' and include pipe from first
        retcode = do_exec(cmds[1] +
                             [ [CP.sort, "-s", "-k2n,2", dpath+"-p", "-"],
                               [CP.uniq, "-u", "-f1"],
                               [CP.gzip, "-2"]
                              ],
                                 out=dpath, timeout=60, otherctl=p11context
                            )
        os.remove(dpath+"-p")

    # Check here that Btrfs snapshot gen#s did not change.
    # If changed: retry, x_it(7), use remap mode or pause/cancel the Btrfs op.
    # This could be extended with a fallback mode that checks _get_btrfs_generation() directly
    # while doing one snap pair at a time to increase chances of success.
    if storage.metadata_unlock()[0] != generation:
        x_it(7, "fs gen ids differ.")

    return datavols


def update_delta_digest_reflink(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols              ; l_vol      = lvols[datavol]
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = l_vol.snap1                ; snap2vol   = l_vol.snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    volperms    = vol.sessions[vol.last].permissions
    assert len(vol.sessions) and exists(vol.mapfile())

    # Check for volume perms & size changes;
    # Chunks from 'markall_pos' onward will be marked for backup.
    changed = snap2size != vol.volsize() or storage.lvols[datavol].getperms() != volperms
    next_chunk_addr  = vol.last_chunk_addr()[0] + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None
    bsize = storage.block_size

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile(), "r+b") as bmapf, gzip.open(tmpdir+"/delta."+vol.vid, "r") as deltaf:
        snap_ceiling = max(snap1size, snap2size) // bsize
        chunkblocks  = chunksize // bsize
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)
        dnewblocks   = dfreedblocks = highwater = 0

        # Cycle through the merged 'filefrag' metadata, marking bits in bmap_mm as needed.
        # Entries carry a beginning block position and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for dln in deltaf:
            side, blockbegin, blocklen = map(int, dln.split()[:3])

            blockend     = min(blockbegin + blocklen, snap_ceiling)
            if blockend <= highwater:   continue
            highwater    = blockend

            if side == 22:
                dnewblocks += blocklen
            else:
                dfreedblocks += blockend - blockbegin

            # blockpos iterates over disk blocks.
            # block_size (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewblocks += (bmap_size - markall_pos) * 8 * (chunksize // bsize)

        if dnewblocks or dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())

        #estimate = sum((x.count("1") for x in map(bin, filter(None, memoryview(bmap_mm))))) \
                 #* chunksize if vol.map_used() else 0 ####
        #print(f"Estimate: {estimate}, New {dnewblocks*bsize}, Freed {dfreedblocks*bsize}" ) ####
        del(bmap_mm)
        if not debug:   os.remove(tmpdir+"/delta."+vol.vid)

    catch_signals()
    changed = changed or dnewblocks+dfreedblocks+vol.map_used() > 0
    if monitor_only:
        t = lvols[datavol].rotate_snapshots(rotate=changed, timestamp_path=vol.mapfile())
        print(("\r  %d ch, %d dis" % (dnewblocks, dfreedblocks//chunksize))
                if changed else "\r  No changes   ")

    if dnewblocks:   vol.changed_bytes_add(dnewblocks * bsize, save=True)
    catch_signals(**signormal)
    return changed, snap2size, volperms


# Reads addresses from manifest and marks corresponding chunks in a volume's deltamap.

def manifest_to_deltamap(volume, manifest, mapsize):
    aset = volume.archive
    with open(manifest, "r") as mf, \
         open(volume.mapfile(), "r+b") as bmapf:

        bmapf.truncate(mapsize)    ; bmapf.flush()
        bmap_mm = mmap.mmap(bmapf.fileno(), 0)       ; chunksize  = aset.chunksize
        for ln in mf:
            addr = int(ln.split()[1][1:], 16)        ; volsegment = addr // chunksize
            bmap_pos = volsegment // 8               ; bmap_mm[bmap_pos] |= 1 << (volsegment % 8)


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a new deltamap and snapshot pair to track
# future changes. After an incremental send, snapshots are rotated and the deltamap is reset.
# An exception is when vol.aliastype is set to "import_other" and no deltamap or snapshot is
# used so the send will perform more slowly like a traditional incremental backup.
#
# Returns (bool, int, int, int) representing whether a session was sent, the current volume size,
# the number of bytes sent and the runtime in seconds.

def send_volume(storage, vol, curtime, ses_tags, send_all=False, benchmark=False):

    def send_chunk_local(tar_info, etag, buf):
        chunkdir = pdirname(_fp := pjoin(destpath, tar_info.name))
        if not _fp.startswith(dest.last_chunkdir):
            makedirs(chunkdir, exist_ok=True)    ; dest.last_chunkdir = chunkdir
        with open(_fp, "wb") as cf:   cf.write(etag); cf.write(buf)

    def send_chunk_remote(tar_info, etag, buf):
        fileobj = BytesIO()
        tar_info.size = fileobj.write(etag) + fileobj.write(buf)   ; fileobj.seek(0)
        tarf_addfile(tarinfo=tar_info, fileobj=fileobj)

    if vol.aliastype == "import_other":
        # Use a plain file or blockdev as a source volume
        volpath = vol.alias                     ; send_all    = True
        vol_ts  = os.stat(volpath).st_mtime_ns  ; volperms    = storage.getperms(volpath)
        with open(volpath, "rb") as f:   volsize = f.seek(0,2)
    else:
        lvol    = storage.lvols[storage.lvols[vol.name].snap2]    ; volpath = lvol.path
        volsize = lvol.getsize()                ; volperms    = storage.lvols[vol.name].getperms()
        vol_ts  = lvol.gettime()

    dest        = (aset := vol.archive).dest    ; pjoin       = os.path.join
    options     = aset.opts                     ; status1     = not options.unattended
    chunksize   = aset.chunksize                ; dedup       = False
    chdigits    = max_address.bit_length()//4   ; chformat    = "x%0"+str(chdigits)+"x"
    bksession   = "S_"+curtime                  ; sdir        = pjoin(vol.vid, bksession)
    prior_size  = vol.volsize()                 ; prior_ses   = vol.last
    old_mapfile = vol.mapfile()                 ; addrsplit   = -address_split[1]
    compress = compressors[aset.compression][2] ; compresslevel = int(aset.compr_level)

    if not status1:   print("  [", end="")
    progress, MB = 0, 2**20
    testtime = time.monotonic()
    def tar_add_pass(*args, **kwargs):   pass

    if len(vol.sessions):
        # Generate a full manifest to detect unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(vol, vsize=prior_size), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None
    fman_hash     = fman_fname = ""

    ses = vol.new_session(bksession, addtags=list(ses_tags.items()))
    ses.localtime   = str(vol_ts)
    ses.volsize     = volsize
    ses.permissions = volperms
    ses.path        = vol.path+"/"+bksession+"-tmp"

    # Code from init_dedup_indexN() localized here for efficiency.
    if aset.dedupindex:
        dedup       = True
        hashtree, ht_ksize, hash_w, ddataf, chtree, chdigits, ch_w, ses_w  =  aset.dedupindex
        ddataf_read = ddataf.read    ; ddataf_seek = ddataf.seek    ; ddataf_write = ddataf.write
        chtree_max  = 2**(chtree[0].itemsize*8)
        idxcount    = ddataf.seek(0,2) // (ch_w+ses_w)   ; ddblank_ch = bytes(hash_w)
        ddsessions  = aset.dedupsessions                 ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(aset.path)    ; os.makedirs(sdir+"-tmp")
    do_exec([[CP.chattr, "+c", sdir+"-tmp"]], check=False)


    if aset.datacrypto:
        crypto   = True    ; crypto_cadence = aset.datacrypto.ctcadence
        encrypt  = aset.datacrypto.encrypt
    else:
        crypto   = False   ; crypto_cadence = 0    ; etag     = b''


    destpath = dest.path
    if dest.dtype == "file" and options.tar_bypass:
        # Bypass tar stream when sending chunks to local fs
        if debug:   print("\nBypassing tar stream...")
        send_chunk = send_chunk_local
        pdirname   = os.path.dirname   ; makedirs  = os.makedirs
    else:
        send_chunk = send_chunk_remote

    # Use tar to stream files to destination
    untar_cmd = ["exec >>"+dest.dtmp+"/send.log 2>&1"
                 + " && python3 "+dest.dtmp+"/dest_helper.py send "+vol.vid+" "+bksession
                 + (" --sync" if options.maxsync else "")]
    untar = SPr.Popen(dest.run_args(untar_cmd), stdin=SPr.PIPE, stdout=SPr.DEVNULL)
    tarf = tarfile.open(mode="w|", fileobj=untar.stdin)
    tarf_addfile = tar_add_pass if benchmark else tarf.addfile
    tarf_add     = tar_add_pass if benchmark else tarf.add
    TarInfo  = tarfile.TarInfo         ; LNKTYPE = tarfile.LNKTYPE
    tar_info = TarInfo(destpath)       ; tar_info.type  = tarfile.DIRTYPE
    tarf_addfile(tarinfo=tar_info)     ; stream_started = True


    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(volpath,"rb", buffering=chunksize) as vf,    \
         open(sdir+"-tmp/manifest.tmp", "wt") as hashf,            \
         open("/dev/zero" if send_all else old_mapfile,"rb") as bmapf:

        vf_seek = vf.seek; vf_read = vf.read   ; BytesIO = io.BytesIO
        gethash = aset.getdatahash             ; b64enc  = base64.urlsafe_b64encode
        b2int   = int.from_bytes               ; islice  = itertools.islice
        compare_digest = hmac.compare_digest   ; zeros   = bytes(chunksize)

        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        zdelim  = bytes(64)    ; zdlen = len(zdelim)*8      ; minibmap = None   ; bmap_list = []
        addr    = counter = percent = bcount = ddbytes = 0  ; checkpt = 128     ; bmsz = zdlen*50

        while addr < volsize:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  list(islice(fullmanifest, zdlen))
                addr += chunksize*zdlen
                minibmap = bmap_list.pop(0)
            elif not send_all:
                # Get more: split(zdelim) shows where large unmodified zones exist.
                bmap_list.extend(bmapf.read(bmsz).split(zdelim))
                minibmap = bmap_list.pop(0)

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, volsize if send_all
                        else min(volsize, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = chformat % addr

                if fullmanifest:
                    try:
                        fman_hash, fman_fname = fullmanifest_readline().split()
                    except ValueError: # EOF
                        fullmanifest.close()   ; os.remove(fullmanifest.name)
                        fullmanifest = None    ; fman_hash = ""
                    else:
                        if fman_fname != destfile:
                            raise ValueError("expected manifest addr %s, got %s"
                                            % (destfile, fman_fname))

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                # Fetch chunk as buf
                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Process checkpoint
                if counter > checkpt:
                    # Keep updating aset counters if key has a low cadence
                    if 0 < crypto_cadence < 101 and not benchmark:
                        aset.save_conf()    ; tarf_add(aset.confname)
                    # Show progress.
                    if status1:
                        percent = int(addr/volsize*1000)
                        print(f"\r{bcount/MB: 7.1f}MB"[:10] + f" | {(percent//10):0d}%"[:7],
                              end="", flush=True)
                    elif (addr/volsize)*100 > progress:
                        print("|", end="", flush=True)   ; progress += 5

                    counter = 0

                # Compress & write only non-empty chunks
                if buf == zeros:
                    if fman_hash != "0":   print("0", destfile, file=hashf)
                    continue

                # Compress chunk and hash it
                buf    = compress(buf, compresslevel)
                bhashb = gethash(buf)   ; b64hash = b64enc(bhashb).decode("ascii")
                # Skip when current and prior chunks are the same
                if compare_digest(fman_hash, b64hash):  continue

                # Add buffer to stream
                print(b64hash, destfile, file=hashf)
                tar_info = TarInfo("%s-tmp/%s/%s" % (sdir, destfile[1:addrsplit], destfile))

                # If chunk already in archive, link to it
                if dedup:
                    i = b2int(bhashb[:ht_ksize], "big")   ; ddses = None   ; pos = 0
                    while (pos := hashtree[i].find(bhashb, pos)) >= 0:
                        if pos % hash_w == 0:
                            ddataf_seek(chtree[i][pos//hash_w] * (ses_w+ch_w))
                            ddses  = ddsessions[b2int(ddataf_read(ses_w),"big")]
                            if ddses is None:
                                hashtree[i][pos:pos+hash_w] = ddblank_ch # zero-out obsolete entry
                            else:
                                ddchx = ddataf_read(ch_w).hex().zfill(chdigits)
                                tar_info.type = LNKTYPE
                            ddataf_seek(0,2)    ; break
                        pos += 1

                    if ddses is None and bhashb != ddblank_ch and idxcount < chtree_max:
                        hashtree[i].extend(bhashb)   ; chtree[i].append(idxcount)   ; idxcount += 1
                        ddataf_write(ses_index.to_bytes(ses_w,"big") + addr.to_bytes(ch_w,"big"))

                if tar_info.type == LNKTYPE:
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.vid,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)

                else:
                    # Encrypt the data chunk
                    if crypto:
                        etag, buf = encrypt(buf, bhashb)

                    # Send data chunk to the archive
                    send_chunk(tar_info, etag, buf)
                    bcount += len(buf)    ; counter += 1

                tarf.members = []

            # Advance addr, except when minibmap is zero len.
            if minibmap or send_all:  addr += chunksize

    # Send session info, end stream and cleanup
    if fullmanifest:   fullmanifest.close()   ; os.remove(fullmanifest.name)
    if (sent := stream_started) and not benchmark:
        # Save session info
        if crypto:   aset.datacrypto.save_counter()
        for fpath in (sdir+"-tmp/"+x for x in ses.save_info(ext=".tmp")):
            tarf_add(fpath, arcname=fpath[:-4])
        tarf_add(vol.vid+"/volinfo.tmp")   ; tarf_add(aset.confname+".tmp")

        tarf.close()    ; untar.stdin.close()
        try:
            untar.wait(timeout=180)
        except SPr.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 99
            untar.kill()
        else:
            retcode = untar.poll()

        if retcode != 0:
            raise RuntimeError("tar transport failure code %d" % retcode)

        if ses.volsize != prior_size and len(vol.sessions) > 1:
            os.link(ses.path+"/manifest.tmp", ses.path+"/manifest")
            check_manifest_sequence(vol, vol.sesnames)

        # Finalize on VM/remote
        catch_signals()
        dest.run(["mv -T "+sdir+"-tmp "+sdir
                 +" && mv "+vol.vid+"/volinfo.tmp "+vol.vid+"/volinfo"
                 +" && mv archive.ini.tmp archive.ini"
                 +(" && ( nohup sync -f . 2&>/dev/null & )" if options.maxsync else "")
                 ], destcd=dest.path, trap=True)

        # Local finalize
        ses.path = ses.path.rsplit("-tmp",maxsplit=1)[0]   ; os.replace(ses.path+"-tmp", ses.path)
        ses.rename_saved(ext=".tmp")
        if exists(ses.path+"/manifest.tmp"):   os.remove(ses.path+"/manifest.tmp")
        if old_mapfile and exists(old_mapfile):   os.remove(old_mapfile)
        fssync(vol.path)

    else:
        if stream_started:   tarf.close(); untar.kill()
        catch_signals()
        vol.delete_session(bksession)    ; shutil.rmtree(aset.path+"/"+sdir+"-tmp")

    if vol.aliastype != "import_other" and not benchmark:
        vol.init_deltamap()
        storage.lvols[vol.name].rotate_snapshots(rotate=True, timestamp_path=vol.mapfile(),
                                                 addtags=["--addtag="+vol.last])
    catch_signals(**signormal)

    runtime  = time.monotonic() - testtime
    t_m, t_s = divmod(runtime, 60); t_p = (f"{int(t_m)}m" if t_m else "") + f"{ceil(t_s)}s"
    if status1:
        print(f"\r{bcount/MB: 7.1f}MB"[:10] +" |"+ (t_p[:5].center(5) if sent else ""), end="")
    else:
        print("|"*((100-progress)//5) + "]", f" 100% {bcount/MB: 0.1f}MB ", end="")
    if ddbytes and options.verbose:
        print("\n" if status1 else "", f"(reduced {ddbytes/MB:0.1f}MB)")
    if dedup and debug:   show_mem_stats()

    return stream_started, ses.volsize, bcount, ddbytes, runtime


# Build deduplication hash index and list

def init_dedup_index(aset, listfile=""):

    ctime      = time.perf_counter()    ; makelist = bool(listfile)
    addrsplit  = -address_split[1]
    # Define arrays and element widths
    hash_w     = hash_bits // 8
    ht_ksize   = 2 # binary digits for tree key
    hashtree   = tuple(bytearray() for x in range(2**(ht_ksize*8)))
    chtree     = tuple(array("I") for x in range(2**(ht_ksize*8)))
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2

    # Create master session list, limit to ses_w range
    for vol in aset.vols.values():
        aset.dedupsessions += vol.sessions.values()
        vol.decode_manifests(vol.sesnames)
    aset.dedupsessions.sort(key=lambda x: x.name, reverse=True)
    ddsessions = aset.dedupsessions[:2**(ses_w*8)-(len(aset.vols))-1]

    ddataf  = open(tmpdir+"/hashindex.dat","w+b")
    ddataf_read = ddataf.read    ; ddataf_seek = ddataf.seek    ; b2int  = int.from_bytes
    ddataf_write= ddataf.write   ; bfromhex = bytes().fromhex   ; b64dec = base64.urlsafe_b64decode
    if makelist:   dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = 0
    for sesnum, ses in enumerate(ddsessions):
        vol = ses.volume    ; vid = vol.vid    ; sesname = ses.name
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":   continue
                bhashb = b64dec(ln1)
                i      = b2int(bhashb[:ht_ksize], "big")   ; pos = 0
                while (pos := hashtree[i].find(bhashb, pos)) >= 0:
                    if pos % hash_w == 0:
                        if makelist:
                            ddataf_seek(chtree[i][pos//hash_w] * (ses_w+ch_w))
                            ddses  = ddsessions[b2int(ddataf_read(ses_w),"big")]
                            ddchx  = ddataf_read(ch_w).hex().zfill(chdigits)
                            print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                                (ddses.volume.vid, ddses.name, ddchx[:addrsplit], ddchx,
                                vid, sesname, ln2[1:addrsplit], ln2),
                                file=dedupf)
                            ddataf_seek(0,2)
                        break
                    pos += 1
                else: # while
                    if count < chtree_max:
                        hashtree[i].extend(bhashb)
                        chtree[i].append(count)
                        ddataf_write(sesnum.to_bytes(ses_w,"big"))
                        ddataf_write(bfromhex(ln2[1:]))  # Enh: scale no. digits to match vol size
                        count += 1

    if listfile:   dedupf.close()

    aset.dedupindex    = (hashtree, ht_ksize, hash_w, ddataf, chtree, chdigits, ch_w, ses_w)
    aset.dedupsessions = ddsessions

    if not debug:  return
    print("\nIndexed in %.1f seconds." % (time.perf_counter()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


# Deduplicate data already in archive

def dedup_existing(aset):

    print("\nBuilding deduplication index...", end="", flush=True)
    init_dedup_index(aset, "dedup.lst.gz")
    dest = aset.dest

    print("\n%dMB free on destination." % (dest.free//1024//1024))
    print("linking...", end="", flush=True)
    do_exec( [dest.run_args([
               "/bin/cat >"+dest.dtmp+"/dest.lst.gz"
               +" && /usr/bin/python3 "+dest.dtmp+"/dest_helper.py dedup"
               ], destcd=dest.path),
              [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]
            ], infile=tmpdir+"/dedup.lst.gz", out=tmpdir+"/arch-dedup.log")
    print(" done.")
    dest.get_free(tmpdir+"/arch-dedup.log")
    print("%dMB free after." % (dest.free//1024//1024))
    if verbose:   print("".join(open(tmpdir+"/arch-dedup.log","r")))


# Controls flow of monitor and send_volume procedures:

def monitor_send(storage, aset, datavols, monitor_only, imports={}, use_sesid=None):

    options = aset.opts                ; dest = aset.dest
    status1 = not options.unattended   ; MB   = 1024**2     ; ms_start = time.monotonic()
    curtime = use_sesid or time.strftime("%Y%m%d-%H%M%S", time.localtime(time_start))

    if options.autoprune.lower() == "full":
        for vol in aset.vols.values():   autoprune(vol, apmode="full")

    storage.check_support()
    if not storage.can_snapshot:
        reason = "Local not a subvolume or no snapshot feature."
        if not storage.online:   reason = "Local path not found."
        err_out(f"\nError: {reason}\nOffline volumes: " + ", ".join(datavols))
        error_cache.extend(datavols)
        return curtime

    # Try to rename aliased archive volume before sending
    for dv, vol in ((x, aset.vols[x]) for x in datavols[:] if x in aset.vols and not monitor_only):
        if vol.aliastype and vol.aliastype == "rename" and vol.alias != vol.name \
        and vol.alias not in aset.vols:
            rename_volume(storage, aset, vol.name, vol.alias)
            del(datavols[datavols.index(dv)])   ; datavols.append(vol.alias)

    print(f"\nPreparing snapshots in '{storage.path}'...")
    incrementals, send_alls, unchanged_vols, other_vols \
        = storage.prep_snapshots(storage, aset, datavols, monitor_only, other_vols=imports)
    storage.update_vol_list({x.name: x.vid for x in aset.vols.values()})

    if monitor_only:   send_alls.clear()

    if len(incrementals)+len(send_alls)+len(other_vols) \
        +(len(unchanged_vols if options.send_unchanged else [])) == 0:
        x_it(0, "No new data.")

    # Process session tags
    ses_tags = {}
    if options.tag and not monitor_only:
        for tag_opt in options.tag:
            if tag_opt in ("", "@"):   continue
            tag = ArchiveSession.tag_parse(tag_opt, delim=",")
            if not tag:   raise ValueError("Invalid tag "+tag_opt)
            ses_tags.update((tag,))
    if options.tag and not monitor_only and (options.tag == [""] or "@" in options.tag):
        print("Enter tag info as 'tagID[, tag description]'. Blank to end input.")
        while ans := ask_input("[%d]: " % (len(ses_tags)+1)).strip():
            tag = ArchiveSession.tag_parse(ans, delim=",")
            if not tag or tag[0] in ses_tags:   print("(not added)"); continue
            ses_tags.update((tag,))
            if len(ses_tags) == ArchiveSession.max_tags:   break
        print(len(ses_tags), "tags total.")

    if len(incrementals) > 0:
        print("Acquiring deltas.", end="", flush=True)
        incrementals = storage.acquire_deltas(storage, aset, incrementals)
        print("", flush=True)

    if not monitor_only:
        cmpvols = [x for x in aset.vols.values()
                         if x.name in incrementals + send_alls and x.sessions]
        cmpses  = [v.sessions[v.sesnames[-1]] for v in cmpvols]
        if compare_files(aset, volumes=cmpvols, sessions=cmpses):
            x_it(1, "Error: Local and archive metadata differ.")

        print(f"\nSending backup session {curtime}:", flush=True)
        hl = "â€”" * (min(max([len(aset.vols[v].name) for v in incrementals+send_alls]+[22]), 60)+20)
        print(hl)

    totalvsize = totalbcount = 0
    for datavol in list(other_vols.keys()) + sorted(incrementals + send_alls + unchanged_vols):
        vol = aset.vols[datavol]    ; updated = sent = False    ; svsize = sbcount = stime = 0
        print(f"Volume '{datavol}'\n" if not status1 else
              f"          |     | {datavol}\r", flush=True, end="")

        if datavol in incrementals:
            updated, svsize, vperms = storage.process_deltas(storage, aset, datavol, monitor_only)

        if monitor_only:   continue

        if datavol in send_alls+list(other_vols) or updated:
            if options.dedup and not aset.dedupindex:
                if status1:   print("I/", end="\r", flush=True)
                init_dedup_index(aset)

        if datavol in send_alls+list(other_vols) or updated or options.send_unchanged:

            # Enh: include send_alls and other_vols in changed bytes tally
            if vol.changed_bytes > dest.free:
                autoprune(vol, needed_space=vol.changed_bytes, apmode=options.autoprune)
                if vol.changed_bytes > dest.free:
                    print(" %d additional bytes needed." % (vol.changed_bytes-dest.free))
                    print("Insufficient space on destination %d; Skipping." % dest.free)
                    error_cache.append(datavol)
                    continue

            sent, svsize, sbcount, ddbytes, stime = \
                send_volume(storage, vol, curtime, ses_tags, send_all=datavol in send_alls)
            dest.free -= int(sbcount + (sbcount * 0.05))
            if vol.aliastype == "import_other":   del(imports[datavol])

        else:
            storage.lvols[vol.name].rotate_snapshots(rotate=False)
            svsize = vol.volsize()

        p1, p2 = ("\r", " |  -" if not sent else "") if status1 else ("", "")
        print(p1 if sbcount or ddbytes else p1+"no change"+p2, flush=True)
        totalvsize += svsize    ; totalbcount += sbcount

    if not monitor_only and (volct:= sum(map(len,(incrementals,send_alls,unchanged_vols,other_vols)))):
        print(hl, "\n%0d volumes, %0dâ€”â€”>%0d MB in %.1f seconds."
              % (volct, totalvsize/MB, totalbcount/MB, time.monotonic() - ms_start)
              )

    return curtime


# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS or ^tagname format.

def prune_sessions(vol, times):

    options = vol.archive.opts    ; allbefore = options.allbefore
    sesnames = vol.sesnames       ; t1, t2    = "", ""                ; to_prune  = []

    print(vol.name, end=" -", flush=True)
    if len(sesnames) < 2:    print(" No extra sessions to prune.")    ; return
    if options.allbefore and len(times) != 1:  x_it(1, "Option --all-before requires one session.")
    for ii in range(len(times)):
        if times[ii] == "newest":   times[ii] = sesnames[-1]

    # Validate date-time params
    for pos, dt in enumerate(times[:]):
        if not dt[0].startswith("^"):
            if not dt.startswith("S_"):   times[pos] = "S_"+dt.strip()
            datetime.datetime.strptime(times[pos][2:], "%Y%m%d-%H%M%S")
        elif dt[1:] not in vol.tags:
            print(" No match for", dt)    ; return
        elif pos == 0:
            for sn in sesnames:
                if dt[1:] in vol.sessions[sn].tags:
                    if len(times) == 1 and not allbefore:
                        to_prune.append(sn)
                    else:
                        t1 = sn   ; break
        elif pos == 1:
            for sn in reversed(sesnames):
                if dt[1:] in vol.sessions[sn].tags:   t2 = sn   ; break

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    if allbefore:   t1 = sesnames[0]    ; t2 = times[0]
    if not t1:   t1 = times[0]
    if not t2 and len(times) > 1:
        t2 = times[1]
        if t2 <= t1:  x_it(1, "Error: Second date-time must be later than first.")

    # Find specific sessions to prune in contiguous range
    if to_prune:
        pass

    elif t2 == "":
        # find single session
        if t1 in sesnames:   to_prune.append(t1)

    else:
        # find sessions in a date-time range
        start = len(sesnames)   ; end = 0
        if t1 in sesnames:
            start = sesnames.index(t1)
        else:
            for ses in sesnames:
                if ses > t1:   start = sesnames.index(ses)    ; break
        if t2 in sesnames:
            end = sesnames.index(t2) + int(not allbefore)
        else:
            for ses in reversed(sesnames):
                if ses < t2:   end = sesnames.index(ses) + 1  ; break
        to_prune = sesnames[start:end]

    if len(to_prune) and to_prune[-1] == sesnames[-1]:
        print(" Preserving latest session -", end="")
        del(to_prune[-1])
    if len(to_prune) == 0:
        print(" No selections in this date-time range.")
        return

    print(autoprune(vol, apmode=options.autoprune, include=set(to_prune)))


# Parameters / vars for autoprune:
# oldest (date): date before which all sessions are pruning candidates
# thin_days (int): number of days ago before which the thinning params are applied
# ndays & nsessions: a days/sessions ratio for amount of sessions left after thinning
# nthresh: min number of sessions to prune this time (0 = prune all candidates)
# target_size (0 or MB int): User-selected size cap for archive (future)

def autoprune(vol, needed_space=0, apmode="off", include=set()):
    options = vol.archive.opts

    dtdate = datetime.date
    def to_date(sesdate):
        return dtdate(int(sesdate[2:6]),int(sesdate[6:8]),int(sesdate[8:10]))

    if len(vol.sesnames) < 2:   return 0

    today = dtdate.today()
    if (apdays := vol.archive.apdays or "0:62:1:2") and all(map(is_num, tup := apdays.split(":"))):
        ap1, ap2, ndays, nsessions = map(int, tup)
        if ap1 < 0 or ap2 < 0 or ndays < 1 or nsessions < 1:
            x_it(1, "Days must be at least '0:0:1:1'")
        oldest    = (today - datetime.timedelta(days=ap1)) if ap1 else 0
        thin_days = datetime.timedelta(days=ap2) if ap2 else 0
    else:
        apmode = "off"
    if not apdays or not (ap1 + ap2):
        apmode = "off"

    if apmode not in ("on","full") and not include:   return 0

    datavol   = vol.name                      ; dest    = vol.archive.dest
    apmode    = apmode.lower()                ; exclude = set()
    sessions  = vol.sesnames[:-1]             ; marked  = 0
    startdate = to_date(sessions[0])          ; nthresh = 3 if apmode == "on" else 0
    enddate   = min(to_date(sessions[-2]), today-thin_days) if len(sessions) > 2 else 0

    # Make a 2d array of ordinal dates and populate with session id + flag
    apcal = { day: [] for day in range(dtdate(startdate.year,1,1).toordinal(),
                                       to_date(sessions[-1]).toordinal()+ndays) }
    for ses in sessions:  apcal[to_date(ses).toordinal()].append([ses, True])

    # Build set of excluded sessions
    for sx in options.keep:
        if not sx.startswith("^"):
            datetime.datetime.strptime(sx, "%Y%m%d-%H%M%S")    ; exclude.add("S_"+sx)
        else:
            if sx[1:] == "all":
                exclude += {x.name for x in vol.sessions if x.tags}
            else:
                exclude += {x.name for x in vol.sessions if sx[1:] in x.tags}
    include -= exclude

    # Mark all sessions prior to oldest date setting, plus include list
    for ses in sessions:
        sdate = to_date(ses)
        if (apmode != "off" and oldest and sdate <= oldest) or ses in include:
            for dses in apcal[sdate.toordinal()]:
                if dses[0] == ses and ses not in exclude:
                    dses[1] = False    ; vol.sessions[ses].toggle = False

    # Mark sessions for thinning-out according to ndays + nsessions;
    # avoid for "wyng-*-metadata" volumes.
    if apmode != "off" and thin_days and enddate \
    and not (vol.name.startswith("wyng-") and vol.name.endswith("-metadata")):
        for year in range(startdate.year, enddate.year+1):
            for span in range(dtdate(year,1,1).toordinal(),
                            min(dtdate(year,12,31), enddate).toordinal(), ndays):
                dlist = []    ; offset = 0
                for day in range(span, min(span+ndays, enddate.toordinal())):
                    dlist.append(sum( x[1] for x in apcal[day] ))
                while sum(dlist) > nsessions: ## Enh: Make even distribution
                    bigday = dlist.index(max(dlist[offset:]), offset)
                    offset += (ndays//nsessions)+1    ; offset %= min(ndays, len(dlist))
                    for dses in apcal[span+bigday]:
                        if dses[1]:
                            # always decr bigday, but don't toggle if session is excluded
                            dlist[bigday] -= 1
                            dses[1] = vol.sessions[dses[0]].toggle = dses[0] in exclude
                            break

    # Find contiguous marked ranges and merge/prune them. Repeat until free >= needed space.
    factor = 1    ; removed_total = 0    ; sessions.append("End")
    while True:
        to_prune = []    ; removed_ct = 0    ; skipped = False
        for ses in sessions:
            if ses is None:   continue
            if ses == "End" or vol.sessions[ses].toggle :
                if to_prune:
                    # prioritize ranges that overlap with requested includes
                    if include and not (set(to_prune) & include):
                        to_prune.clear()    ; skipped = True    ; continue

                    target_s = vol.sesnames[vol.sesnames.index(to_prune[-1]) + 1]
                    merge_sessions(vol, to_prune, target_s, clear_sources=True)

                    for i in to_prune:   sessions[sessions.index(i)] = None
                    include -= set(to_prune)    ; removed_ct += len(to_prune);   to_prune.clear()
                    if not include and nthresh and removed_ct >= nthresh*factor:  break # for ses
            else:
                to_prune.append(ses)

        if removed_ct:
            removed_total += removed_ct
            dest.get_free(tmpdir+"/merge.log")    ; os.remove(tmpdir+"/merge.log")

        if skipped and apmode != "off":   continue # while
        if removed_ct == 0 or nthresh == 0 or needed_space <= dest.free:
            break # while
        elif factor > 4:
            nthresh = 0
        else:
            factor += 2

    if options.verbose:   print(datavol+": Removed", removed_total)
    return removed_total


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session name.

def merge_manifests(vol, msessions=None, mtarget=None, vsize=None, addcol=False, rdiff=[]):
    # Enh: implement mtarget to support merge_sessions()
    aset      = vol.archive                    ; slist  = []
    msessions = msessions or vol.sesnames
    sespaths  = [ os.path.basename(vol.sessions[x].path) for x in msessions ]
    tmp       = aset.big_tmpdir if vol.volsize() > 128000000000 else tmpdir
    outfile   = tempfile.NamedTemporaryFile(dir=tmp, prefix="mout_", delete=False)

    if not aset.dedupsessions:   vol.decode_manifests(msessions)
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, prefix="sl_", delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(sespaths)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir_obj = tempfile.TemporaryDirectory(dir=tmp, prefix="m_")
        cdir     = cdir_obj.name    ; slsort  = slist[1]

        # fix: extrapolate path with filename
        for fname in sespaths:
            open(cdir+"/"+fname, "w").close()
            do_exec([[CP.awk, r'{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                                +cdir+r'/"FILENAME}', fname+"/manifest"]], cwd=vol.path)
    else:
        cdir  = vol.path     ; slsort  = slist[0]

    cmds = [[CP.sort, "-umsd", "-k2,2", "--batch-size=16", "--files0-from="+slsort]]

    if rdiff:
        # merge a different set of manifests, use their address column to filter the main merge:
        mfilter = merge_manifests(vol, msessions=list(reversed(rdiff)), vsize=vsize, addcol=False)
        cmds.append([CP.sort, "-msd", "-k2,2", mfilter, "-"])
        cmds.append([CP.awk, r'{if (PREV==$2) print $0} {PREV=$2}'])

    if vsize is not None and vsize < max(vol.sessions[x].volsize for x in msessions):
        # session vol size shrank, so filter result by requested vsize
        cmds.append([CP.awk, r'$2<="'+vol.last_chunk_addr(vsize)[1]+'"'])

    do_exec(cmds, out=outfile.file, cwd=cdir)

    for f in slist:  os.remove(f)
    if rdiff:        os.remove(mfilter)
    if addcol:       cdir_obj.cleanup()
    return outfile.name


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume, source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(volume, sources, target, clear_sources=False):

    aset       = volume.archive    ; dest = aset.dest    ; resume = bool(aset.in_process)
    m_tmp      = tmpdir if volume.volsize() < 128000000000 else aset.big_tmpdir

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)

    if not resume:
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max(x.volsize for x in volume.sessions.values()
                                    if x.name in sources)
        lc_filter  = '"'+volume.last_chunk_addr(volsize)[1]+'"'

        with open("merge.lst", "wt") as lstf:
            print(merge_target, target, file=lstf)
            volume.decode_manifests(merge_sources + [merge_target])

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                do_exec([[CP.sed, "-E", r"s|$| "+ses+r"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umsd", "-k2,2", "--batch-size=16"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umsd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]],
                out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, r"$2<="+lc_filter+r" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, r"$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                r"s|^0 x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"-rm " +merge_target+ r"/\1/x\1\2|; t; "

                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"rename \3/\1/x\1\2 " +merge_target+ r"/\1/x\1\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.lst")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, r"$2>"+lc_filter, m_tmp+"/manifest.two"],
                     [CP.sed, "-E", r"s|^\S+\s+x(\S{" + str(address_split[0]) + r"})(\S+)|"
                                    r"-rm " + merge_target + r"/\1/x\1\2|"]
                    ], out=">>merge.lst")

        do_exec([[CP.gzip, "-f", "merge.lst"]])

    if not resume:
        # Set archive in_process state to "merge"
        aset.set_in_process(["merge", volume.vid, str(clear_sources), target, sources],
                            tmp=False, todest=False)

    # Update & send new metadata and process lists to dest
    if clear_sources:
        for ses in sources:
            if ses in volume.sessions:   volume.delete_session(ses, remove=False)

    cmds = []    ; dest_cmds = "cd " + volume.vid
    if not resume:
        aset.set_in_process(None, tmp=True, todest=False)
        sesfiles = volume.sessions[target].save_info(".tmp")
        cmds += [CP.tar, "-cf", "-"] + [target+"/"+x for x in sesfiles] + ["../archive.ini",
                 "../archive.ini.tmp", "volinfo.tmp", "merge.lst.gz"]
        dest_cmds += " && tar -o -xf -"

    # Start merge operation on dest
    catch_signals()
    retcode = do_exec([cmds, dest.run_args([dest_cmds + " && python3 "
                            + dest.dtmp+"/dest_helper.py merge "+merge_target
                            + (" --resume" if resume else "")+(" --sync" if aset.opts.maxsync else "")
                            ], destcd=dest.path),
                       [CP.cat, "-v"], [CP.tail, "--bytes=2000"]
                      ], cwd=volume.path, check=False, out=tmpdir+"/merge.log"
                     )

    if retcode == 50:
        # Initialization didn't complete, so reload aset and abort
        aset.stop()
        aset = ArchiveSet(aset.path, aset.dest, aset.opts, prior_auth=aset)
        aset.set_in_process(None)
        for f in ("merge.lst.gz", "volinfo.tmp", "vi.dat.tmp", target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest.run([f"cd {volume.vid}  &&  rm -rf merge merge.lst.gz"],
                 destcd=dest.path, check=False)
        x_it(1, "Error: Merge could not initialize!")
    elif retcode != 0:
        x_it(retcode, "Error: Remote exited!")

    # Local finalize
    volume.sessions[target].rename_saved(ext=".tmp")
    aset.set_in_process(None, save=False)
    catch_signals(**signormal)

    # Check consistency after resuming merge
    if resume and compare_files(aset, volumes=[volume], sessions=[volume.sessions[target]],
                                manifest=True):
        x_it(1, "Error: Local and dest metadata differ.")

    os.remove("merge.lst.gz")
    for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(storage, vol, select_ses="", ses_strict=False, save_path="",
                   diff=False, verify_only=0):

    def diff_compare(dbuf,z):
        if dbuf != volf.read(chunksize):
            if remap:
                volsegment = addr // chunksize 
                bmap_pos = volsegment // 8
                bmap_mm[bmap_pos] |= 1 << (volsegment % 8)
            return len(dbuf)
        else:
            return 0

    def punch_zero_hole(fn, loc):
        if not punch_hole(fn, loc, chunksize):
            volf_seek(loc)   ; volf_write(zeros)


    dest        = (aset := vol.archive).dest      ; options      = aset.opts
    attended    = not options.unattended          ; debug, remap = options.debug, False
    verbose     = (options.verbose or attended) and verify_only != 2    ; MB = 2**20
    sparse      = options.sparse                  ; sparse_write = options.sparse_write or sparse
    chunksize   = aset.chunksize                  ; use_snapshot = False
    sessions    = vol.sesnames                    ; zeros        = bytes(chunksize)
    # functions
    compress    = compressors[aset.compression][2]; compresslevel = int(aset.compr_level)
    decompress  = compressors[aset.compression][0].decompress
    decrypt     = aset.datacrypto.decrypt if aset.datacrypto else None
    compare_digest = hmac.compare_digest          ; b64enc    = base64.urlsafe_b64encode
    gethash     = aset.getdatahash

    if diff or verify_only:
        save_path = ""

    # Set the session to retrieve
    if not sessions:
        err_out("No sessions available.")
        return None

    if select_ses:
        if select_ses[0] == "^":
            # match tag to session id
            tag = select_ses[1:]
            if tag in vol.tags:
                select_ses = sorted(vol.tags[tag])[-1]
                print("Matched tag to", select_ses)
        else:
            # validate date-time input
            datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
            select_ses = "S_"+select_ses

        if (ses_strict and select_ses not in sessions) \
        or (not ses_strict and select_ses < sessions[0]):
            err_out(f"No volume '{vol.name}' in session {select_ses}.")
            return None

        if not ses_strict and select_ses not in sessions:
            print(f"Selecting {select_ses[2:]} as ", end="")
            select_ses = [s for s in sessions if s < select_ses][-1]   ; print(select_ses[2:]+".")

    else:
        # default to last session
        select_ses = sessions[-1]

    if diff and remap and select_ses != sessions[-1]:
        err_out("Cannot use prior session for remap.")
        return None

    ses_obj     = vol.sessions[select_ses]           ; volsize     = ses_obj.volsize
    addrsplit   = -address_split[1]                  ; rc = l_vol  = volf = None
    lchunk_addr, last_chunkx = vol.last_chunk_addr(volsize)
    assert vol.aliastype in (None, "rename")

    # Prepare save volume
    if not (diff or verify_only):

        if not LocalStorage.fallocate:
            sparse = sparse_write = use_snapshot = False
            print("Sparse and snapshot modes disabled.")

        # Decode save path semantics
        if save_path:
            save_storage = None       ; returned_home = False
            save_type,_,_,_ = LocalStorage.parse_local_path(ldir := os.path.dirname(save_path))
            if save_type not in (None,"block device"):
                save_storage = LocalStorage(ldir, auuid=aset.uuid,
                                            arch_vols=storage.arch_vols.copy())
        else:
            if not storage.online:   x_it(1, "Local storage is offline!")
            save_storage = storage    ; returned_home = storage.online
            save_type = "tlvm pool" if storage.pooltype=="tlvm" else "file"
            l_vol = save_storage.new_vol_entry(vol.alias, vol.vid)    ; save_path = l_vol.path

        # possibly use snapshot as baseline for receive
        if returned_home and options.use_snapshot and save_storage.pooltype in ("rlnk","tlvm") \
        and (snap_lv := save_storage.lvols[l_vol.snap1]).paired_state(vol.mapfile(), vol.last)=="Y":
            assert l_vol.path == save_path
            sparse_write = use_snapshot = True    ; sparse = False
            l_vol.delete(force=True)
            l_vol.create(snapshotfrom=snap_lv.name, ro=False)

        elif save_type == "tlvm pool":
            if not l_vol.exists():
                print("Creating '%s' in thin pool %s[%s]." %
                      (l_vol.name, save_storage.path, save_storage.lvpool))
                # Fix:  translate to safe lvm name
                l_vol.create(volsize, ro=False)    ; sparse_write = sparse = False

            elif l_vol.getsize() != volsize:
                if debug:   print("Re-sizing LV to %d bytes." % volsize)
                l_vol.resize(volsize)

        if save_type in("tlvm pool","block device") and exists(save_path):
            punch_hole = save_storage.block_discard_chunk
            if not sparse_write:
                do_exec([[CP.env, "-u", "LC_ALL", CP.blkdiscard, save_path]])
            volf = open(save_path, "r+b")

        elif os.path.abspath(save_path).startswith("/dev/"):
            x_it(1, "Cannot create new volume from ambiguous /dev path.\n"
                    " Please create the volume before using 'receive', or specify"
                    " --save-to=volgroup/pool/volname in case of a thin LVM volume.")
        else:
            save_type = "file"   ; punch_hole = save_storage.file_punch_hole
            if not exists(save_path) or not sparse_write:
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                open(save_path, "wb").close()    ; sparse_write = sparse = False

            volf = open(save_path, "r+b")    ; volf.truncate(volsize)

    elif diff:
        l_vol = storage.lvols[vol.alias]     ; snap1vol = l_vol.snap1
        if not l_vol.exists():
            err_out("Local volume must exist for diff.")
            return None
        if remap:
            pass # raise NotImplementedError()
        else:
            if storage.lvols[snap1vol].exists():
                l_vol = storage.lvols[snap1vol]
            else:
                print("Snapshot not available; Comparing with source volume instead.")

            if volsize != l_vol.getsize():
                err_out("Volume sizes differ:\n  Archive = %d \n  Local   = %d"
                        % (volsize, l_vol.getsize()))   ; return None

        volf  = open(l_vol.path, "r+b")

    if volf:
        volf_read = volf.read     ; volf_write = volf.write    ; volf_seek = volf.seek
        volfno    = volf.fileno() ; fcntl.lockf(volf, fcntl.LOCK_EX|fcntl.LOCK_NB)

    if verify_only != 2:
        print("\nReceiving" if save_path else "\nVerifying", f"volume '{vol.name}'",select_ses[2:])
        if save_path:    print("Saving to %s '%s'" % (save_type, save_path))
        if not verbose:   print("[", end="")

    # Collect session manifests
    diff_ses = []
    if use_snapshot and save_storage:
        # sessions after selected, except if last ses (then we're already done)
        if select_ses == vol.last:
            save_storage.setperms(save_path, ses_obj.permissions)
            if is_num(ses_obj.localtime):   save_storage.settime(save_path, int(ses_obj.localtime))
            print("Snapshot retrieved...Done.")
            return volsize
        print("Using snapshot as baseline.")
        incl_ses = sessions[:sessions.index(select_ses)+1]
        diff_ses = sessions[sessions.index(select_ses)+1:]
    elif verify_only == 2:
        # only selected session
        incl_ses = [select_ses]
    else:
        # everything upto & including selected
        incl_ses = sessions[:sessions.index(select_ses)+1]

    # Verify metadata then merge manifests and send chunk list to dest system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    if compare_files(aset, volumes=[vol], sessions=[vol.sessions[x] for x in incl_ses]):
        x_it(1, "Error: Local and archive metadata differ.")
    vol.decode_manifests(incl_ses+diff_ses, force=True)
    manifest = merge_manifests(vol, msessions=incl_ses, addcol=True, rdiff=diff_ses)

    if debug:
        print(f"{vol.vid} CHUNK SIZE: {chunksize}\nEND SEQUENCE:")
        import collections
        print("\n".join(collections.deque(  (" ".join(x.split()[1:]) for x in open(manifest)
                                            if not x.startswith("0 ")), 10)))

    if not sparse:
        cmds = [[CP.sed, "-E", r"/" +last_chunkx+ r"/q", manifest],  ## Enh: detect vol_shrank
                [CP.sed, "-E", r"/^0\s/ d; "
                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|\3/\1/x\1\2|;"],
                [CP.gzip, "-c", "-4"
                ],
                dest.run_args(["cat >"+dest.dtmp+"/dest.lst.gz"]),
            ]   # Enh: replace list with range
        do_exec(cmds)

    # Create retriever process using py program
    cmd = dest.run_args(
            ["cd " + vol.vid
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ], destcd=dest.path)
    getvol   = SPr.Popen(cmd, stdout=SPr.PIPE, stderr=open(tmpdir+"/receive.log","w"),
                                     stdin =SPr.PIPE if sparse else SPr.DEVNULL)
    gv_stdin = io.TextIOWrapper(getvol.stdin, encoding="utf-8") if sparse else None

    # Open manifest then receive, check and save data
    addr = -1    ; bcount = diff_count = progress = 0    ; buf = b''    ; magic = dest.magic
    for mfline in open(manifest, "r") if volsize else []:
        if lchunk_addr <= addr >= 0:   break
        cksum, faddr, ses = mfline.split()    ; addr = int(faddr[1:], 16)

        if debug:
            print(f"\r{(addr/volsize):.1%}", faddr, ses, end=" ")
        elif verbose:
            print("".join(("\r[", ("|" * ceil((x:=addr/volsize)*20)).ljust(20,"."),
                  f"] {x:.1%}")), end=" ", flush=True)
        elif (addr/volsize)*100 > progress and verify_only != 2:
            print("|", end="", flush=True)   ; progress += 5


        # Process zeros quickly
        if cksum == "0":
            buf = zeros
            if save_path:
                volf_seek(addr)
                if sparse_write and volf_read(chunksize) != zeros:
                    punch_zero_hole(volfno, addr)    ; diff_count += chunksize
            elif diff:
                volf_seek(addr)    ; diff_count += diff_compare(zeros,True)
                if diff_count and not remap:   break

            continue

        # Request chunks on-demand if local chunk doesn't match cksum
        if sparse and save_path:
            volf_seek(addr)
            if b64enc(gethash(compress(volf_read(chunksize),
                                       compresslevel))).decode("ascii") == cksum:
                continue
            else:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)

        # Read chunk size
        assert getvol.stdout.read(3) == magic
        untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

        # allow for slight expansion from compression algo
        if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
            if options.skip_corrupt_chunks and save_path:
                print("Skipping wrong-sized chunk at", addr)
                punch_zero_hole(volfno, addr)
                continue
            else:
                if save_path and save_type == "file":
                    volf.truncate(0)
                elif save_path:
                    do_exec([[CP.env, "-u", "LC_ALL", CP.blkdiscard, save_path]])
                raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

        ##  Size is OK  ##
        size = untrusted_size

        # Read chunk buffer
        untrusted_buf = getvol.stdout.read(size)
        rc  = getvol.poll()
        if rc is not None and len(untrusted_buf) == 0:
            break

        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)    ; rc  = getvol.poll()
            raise BufferError("rc=%s. Got %d bytes, expected %d" % (repr(rc), len(untrusted_buf), size))


        # Decrypt the data chunk
        # Validation MUST be next step!
        if decrypt:
            untrusted_buf = decrypt(untrusted_buf)

        # Validate data chunk
        if not compare_digest(cksum, b64enc(gethash(untrusted_buf)).decode("ascii")):
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            print(size, mfline)
            if options.skip_corrupt_chunks and save_path:
                print("Skipping corrupt chunk at", addr)
                punch_zero_hole(volfno, addr)
                continue
            else:
                if save_path and save_type == "file":
                    volf.truncate(0)
                elif save_path:
                    do_exec([[CP.env, "-u", "LC_ALL", CP.blkdiscard, save_path]])
                raise ValueError("Bad hash "+faddr+" :: "+str(b64enc(gethash(untrusted_buf))))

        ##  Buffer is OK  ##
        buf = untrusted_buf   ; bcount += len(buf)

        if verify_only:   continue

        # Proceed with decompress.
        decomp = decompress(buf)
        if len(decomp) != chunksize and addr < lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        if addr == lchunk_addr and len(decomp) != volsize - lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        buf = decomp

        # Write data to volume (receive)
        if save_path:
            volf_seek(addr)
            # Don't re-check buffer for sparse mode, check for sparse_write:
            if sparse:
                volf_write(buf)    ; diff_count += len(buf)
            elif sparse_write:
                if use_snapshot or volf_read(chunksize) != buf:
                    volf_seek(addr)    ; volf_write(buf)    ; diff_count += len(buf)
            else:
                volf_write(buf)
        elif diff:
            volf_seek(addr)    ; diff_count += diff_compare(buf,False)
            if diff_count and not remap:   break

    if debug:   print("\nEND receive stream.")
    if gv_stdin:   gv_stdin.close()
    rc = getvol.poll()
    if rc and debug:
        err_out("Warn: Helper exited rc="+repr(rc))
    elif getvol.stdout.read(3) != magic and debug:
        err_out("Warn: No magic.")
    getvol.kill()

    if ((not sparse and verify_only != 2 and not use_snapshot and not diff) \
    or (diff and remap)) and addr != lchunk_addr:
        err_out("Last chunk at %d does not match volume %d." % (addr, lchunk_addr))
        return None

    if verbose:   print("\r[" + "|"*20, end="")
    print("] 100% " if verify_only != 2 else "", ": OK " if not diff_count or save_path else " ",
          f" Diff bytes: {diff_count}" if diff_count else "",
          end="" if verbose else "\n", sep="")
    if verbose:
        print(f" Data bytes: {bcount}", f"/ {volsize}" if verify_only != 2 else "")

    if save_path:
        volf.flush()    ; os.fsync(volf.fileno())    ; volf.close()
        if save_storage:
            save_storage.setperms(save_path, ses_obj.permissions)
            if is_num(ses_obj.localtime):   save_storage.settime(save_path, int(ses_obj.localtime))
        if returned_home and save_storage.pooltype in ("rlnk","tlvm") \
        and select_ses == sessions[-1]:
            if debug:   print(f"Pairing snapshot.")
            vol.init_deltamap()
            tags = ["--addtag=wyng", "--addtag=arch-"+aset.uuid, "--addtag="+sessions[-1]]
            save_storage.lvols[l_vol.snap2].delete()
            save_storage.lvols[l_vol.snap2].create(snapshotfrom=vol.alias, addtags=tags)
            l_vol = l_vol.update()    ; save_storage.lvols[l_vol.snap2].update()
            l_vol.rotate_snapshots(timestamp_path=vol.mapfile())
    if remap:
        bmapf.close()
        if diff_count > 0 and options.action != "send":
            print("\nNext 'send' will bring this volume into sync.")
    elif diff and diff_count:
        return None

    if not debug:   os.remove(manifest)
    return bcount


# Rename a volume in the archive

def rename_volume(storage, aset, oldname, newname):

    os.chdir(aset.path)    ; vol = aset.vols[oldname]
    if not aset.rename_volume_meta(oldname, newname, ext=".tmp"):
        x_it(1, "Error: Cannot rename '%s' to '%s'." % (oldname,newname))

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    vol.rename_saved(ext=".tmp")
    catch_signals(**signormal)
    if verbose:   print("Archive volume renamed.")

    # move snapshots to new pathname
    if not storage.online:
        print("Note: A valid --local location was not specified; no local snapshots"
              " will be renamed which may result in a slow rescan on the next 'send'.\n")
        return

    new_lvol = storage.new_vol_entry(newname, vol.vid)    ; old_lvol = storage.lvols[oldname]
    for atr in ("snap1","snap2"):
        oldsnap, newsnap = storage.lvols[getattr(old_lvol, atr)], \
                           storage.lvols[getattr(new_lvol, atr)]
        newsnap.delete()
        if new_lvol.exists() and oldsnap.name != newsnap.name:
            if oldsnap.exists():
                oldsnap.rename(newsnap.name)
                if verbose:   print("Local snapshot renamed.")
        else:
            oldsnap.delete()


def add_volume(aset, datavol, desc):
    vol = aset.add_volume_meta(datavol, desc=desc, ext=".tmp")
    if not vol:   return

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    vol.rename_saved(ext=".tmp")
    catch_signals(**signormal)

    new_lvol = storage.new_vol_entry(datavol, vol.vid)
    for atr in ("snap1","snap2"):
        if (lv := storage.lvols[getattr(new_lvol, atr)]).is_arch_member() == "true":
            lv.delete()


# Remove a volume from the archive

def delete_volume(storage, aset, dv=None, vid=None):
    assert not (dv and vid)   ; inproc = aset.in_process and aset.in_process[0] == "delete"

    if not storage.online:
        print("Note: A valid --local location was not specified; no volume snapshots"
              " will be removed.\n")

    if vid:
        if vid not in aset.conf["volumes"] or not vid.startswith("Vol_"):
            if inproc:   aset.set_in_process(None)
            err_out(f"Vid '{vid}' not found.")
    else:
        aset.load_volumes(1)
        if dv not in aset.vols:
            x_it(1, f"Volume '{dv}' not found.")
        vid = aset.vols[dv].vid

    if not options.unattended and not options.force and not inproc:
        print("\nWarning! Delete will remove ALL metadata AND archived data",
              f"for volume '{dv or vid}'")

        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:   x_it(0,"")

    if vid in aset.conf["volumes"]:
        print(f"\nDeleting volume '{dv or vid}' from archive.")
        catch_signals()    ; inproc = True
        aset.set_in_process(["delete", vid])
        dvid = aset.delete_volume_meta(vid=vid)
        update_dest(aset, pathlist=[aset.confname])
        aset.set_in_process(None)
        catch_signals(**signormal)

    if dvid.startswith("Vol_") and inproc:
        aset.dest.run(["rm -rf '%s'" % dvid], destcd=aset.dest.path)

    if storage.online and dv:
        storage.new_vol_entry(dv, dvid)
        for lvol_name in (storage.lvols[dv].snap1, storage.lvols[dv].snap2):
            storage.lvols[lvol_name].delete()
    else:
        print("(Skipping snapshot removal.)")

    x_it(int(not inproc))


def show_list(aset, selected_vols):

    if aset.dest.archive_ini_hash == "none":   print("(CACHED)")
    if options.json:
        try:
            print(json.dumps({"wversion": prog_version, "wdate": prog_date,
              "uuid": aset.uuid, "updated_at": aset.updated_at,
              "encrypted": bool(aset.mcrypto and aset.datacrypto), "is_auth": bool(aset.mcrypto),
              "pooltype": storage.pooltype, "url": aset.dest.spec,
              "volumes": {v.name: {"vid": v.vid, "desc": v.desc,
                "sessions": {s.name: {"volsize": s.volsize, "sequence": s.sequence,
                  "tags": {x:y for x,y in s.tags.items()}}
              for s in v.sessions.values()} } for v in aset.vols.values() }}, indent=1)
            )
        except BrokenPipeError:
            pass
        return
    if options.verbose:
        print("\nArchive Settings:")
        for key, val in aset.conf["var"].items():
            print(" %-15s = %s" % (key, val))

    # Print list of sessions grouped by tag. First, organize by tag:
    if options.tag:
        print("\nTag Assignments")    ; ltags = {}
        for dv in selected_vols or sorted(aset.vols.keys()):
            vol = aset.vols[dv]
            for tag, tses in vol.tags.items():
                tset = ( (dv, x[2:], vol.sessions[x].tags[tag] ) for x in tses )
                if tag not in ltags:
                    ltags[tag] = list(tset)
                else:
                    ltags[tag] += tset
        # Print result:
        for tag in sorted(ltags):
            print("\n"+tag+":")
            for ses in sorted(ltags[tag]):   print(" ", ses)
        return

    # Print list of volumes if no volume is selected.
    if not aset.vols:
        print("\nNo volumes.")    ; return
    elif not selected_vols and not len(options.volumes):
        print("\nVolumes:")
        maxwidth = max(len(x.name) for x in aset.vols.values())    ; session = options.session
        fmt    = "%7.2f GB  %s  %-" + str(maxwidth+2) + "s  %s (%3d)"
        for vname in sorted(x.name for x in aset.vols.values()
                            if not session or "S_"+session in x.sesnames):
            vol = aset.vols[vname]   ; sname = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            if options.verbose:
                print(fmt % ((vol.volsize() / 1024**3), vol.vid, vname, sname, len(vol.sessions)))
                if vol.desc:   print("  desc:", vol.desc)
            else:
                print("",vname)
        return

    # Print list of sessions grouped by volume.
    # Get the terminal column width and set cols to number of list columns.
    cols = max(4, min(10, shutil.get_terminal_size().columns // 17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        if not aset.vols[dv].sessions:   print("None.")    ; continue
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for sname in vol.sesnames + [""]:
            month = sname[2:8]    ; ses = vol.sessions[sname] if sname else None
            if options.unattended or options.verbose:
                # plain listing
                print(sname[2:])
                if ses and options.verbose:
                    for tag in sorted(ses.tags.items()):
                        print(" tag:", tag[0]+(", "+tag[1] if tag[1] else ""))
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(sname)
            else:
                # print the month list: 'rows' & 'c' are adjusted to reduce ragged columns.
                rows, extra = divmod(size := len(slist), c := cols)   ; rows += bool(extra)
                if extra and rows < 3 and (cols - extra) > 1:   c -= (cols - extra) // 2
                for r in range(rows):
                    print("  ".join(slist[x*rows+r][2:] for x in range(c) if x*rows+r < size))

                # start new month list
                print()    ; lmonth = month    ; slist = [sname]


def ts_to_datetime(ts):
    raw = datetime.datetime.fromtimestamp(ts, datetime.timezone.utc)
    return raw.astimezone().isoformat(sep=u" ")


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def is_num(val):
    try:
        float(val)
    except:
        return False
    else:
        return True


def os_kill(pid, sig=signal.SIGTERM):
    try:
        os.kill(pid, sig)
    except ProcessLookupError:
        pass


def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1","USR2"], iflag=False):
    # Remove existing handlers
    for sig in list(signal_handlers):   signal.signal(sig, signal_handlers.pop(sig))
    # Set handler on requested signals
    for sig in (getattr(signal,"SIG"+x) for x in (sel or [])):
        signal_handlers[sig] = signal.getsignal(sig)   ; signal.signal(sig, handle_signal)
        signal.siginterrupt(sig, iflag)

    while not sel and signals_caught:   os.kill(os.getpid(), signals_caught.pop(0))


def handle_signal(signum, frame):
    if signum == signal.SIGINT:   x_it(8)
    if signum == signal.SIGALRM:  raise TimeoutError("SIGALRM")
    if signum not in signals_caught:   signals_caught.append(signum)
    sys.stderr.write(" *** Caught signal "+str(signum))


def ask_input(text):
    sys.stderr.write(text)
    return input()


# Exit with simple message
def x_it(code, text=None):
    if text:   err_out(text)
    sys.exit(code)


def err_out(text):
    sys.stdout.write(text+"\n")


@atexit.register
def cleanup():
    if debug:
        shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
        if tmpdir:   shutil.move(tmpdir, "/tmp/"+prog_name+"-debug")

    try:
        if not debug:
            if dest:   dest.remove_dtmp()
            if aset:   shutil.rmtree(aset.big_tmpdir, ignore_errors=True)
            if tmpdir and exists(tmpdir):   shutil.rmtree(tmpdir, ignore_errors=True)

        if cachedir and meta_reduce in ("on","extra"):
            SPr.run([CP.find, cachedir, "(", "-name", "manifest", "-o", "-name", "manifest.tmp"]
                    + (["-o", "-name", "manifest.z"] if meta_reduce=="extra" else [])
                    + [")", "-cmin", "+"+str(meta_min), "-delete"])
    except Exception as e:
        err_out("Cleanup error: "+repr(e))

    if error_cache:
        err_out("Error on volumes: " + ", ".join(sorted(set(error_cache))))
        os._exit(2)




##  MAIN  #########################################################################################

# Constants / Globals
prog_name             = "wyng"
prog_version          = "0.8 beta"      ; prog_date = "20240827"
format_version        = 3               ; debug     = False
admin_permission      = os.getuid() == 0

meta_reduce = meta_min = vardir = cachedir = tmpdir = None
aset = dest = storage = None
time_start, monotonic_start = time.time(), time.monotonic()
signal_handlers  = {}    ; signals_caught = []    ; error_cache = []


if sys.hexversion < 0x3080000:
    x_it(1, "Python ver. 3.8 or greater required.")

# Allow only one instance at a time
lockpath = "/var/lock/"+prog_name
try:
    os.makedirs(os.path.dirname(lockpath), exist_ok=True)
    lockf = open(lockpath, "w")
    fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
except PermissionError:
    x_it(1, "ERROR: No writing permission on %s.  Please run %s as root." % (lockpath,prog_name))
except IOError:
    x_it(1, "I/O ERROR obtaining lock on "+lockpath)

cpu_flags = [x for x in open("/proc/cpuinfo") if x.startswith("flags")] [0].split()[1:]

max_address     = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split   = [len(hex(max_address))-2-7, 7]
hash_bits       = 256    ; hash_bytes   = hash_bits // 8

LC_ALL          = os.environ.get("LC_ALL")   ; os.environ["LC_ALL"] = "C"
shell_prefix    = "set -e\n"

pjoin           = os.path.join     ; exists = os.path.exists   ; zipln = itertools.zip_longest


## Parse Arguments & Etc Config ##

local_actions   = ("monitor","version")
write_actions   = ("add","send","prune","delete","rename","arch-deduplicate")
dest_actions    = ("list","receive","verify","diff","arch-check","arch-init") + write_actions

parser_defs = [
 [["action"],         {"choices": local_actions+dest_actions, "help": "Wyng Commands"}],
 [["volumes"],        {"nargs": "*"}],
 [["--unattended", "-u"], {"action": "store_true", "help": "Non-interactive, supress prompts"}],
 [["--authmin"],      {"default": "5", "help": "Minutes to remember authentication (default=2)"}],
 [["--all-before"],   {"dest": "allbefore", "action": "store_true",
                       "help": "Select all sessions before --session date-time."}],
 [["--all", "-a"],    {"action": "store_true", "help": "Select all known volumes"}],
 [["--session"],      {"help": "YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS] or ^tag_id"
                             " select session date|tag, singular or range."}],
 [["--session-strict"],{"choices": ["on","off"], "default": "on",
                        "help": "Accept/reject inclusive session date"}],
 [["--tag"],          {"action": "append", "default": [],
                       "help": "tag_id[,description] Add tags when sending"}],
 [["--keep"],         {"action": "append", "default": [],
                       "help": "YYYYMMDD-HHMMSS or ^tag_id exclude session by date|tag (prune)"}],
 [["--volex"],        {"action": "append", "default": [],
                       "help": "Exclude volume"}],
 [["--autoprune"],    {"default": "off",       "help": "Automatic pruning"}],
 [["--apdays"],       {"help": "Parameters for autoprune"}],
 [["--save-to"],      {"help": "Path to store volume for receive"}],
 [["--sparse"],       {"action": "store_true", "help": "Retrieve differences only"}],
 [["--sparse-write"], {"action": "store_true", "help": "Save differences only"}],
 [["--skip-corrupt-chunks"], {"action": "store_true",
                       "help": "Continue despite corrupt chunks (receive)"}],
 [["--use-snapshot"], {"action": "store_true",
                       "help": "Use local snapshot as baseline for receive"}],
 [["--import-other-from"], {"action": "append", "default": [],
                       "help": "Import volume from non-snapshot file"}],
 [["--remap"],        {"action": "store_true", "help": "Remap volumes from other archive"}],
 [["--send-unchanged"], {"action": "store_true", "help": "Record unchanged volumes (send)"}],
 [["--dest"],         {"default": "", "help": "URL to archive"}],
 [["--local"],        {"default": "", "help": "Init: LVM vg/pool containing source volumes"}],
 [["--local-from"],   {"help": "Specify multiple --local volume sets from text file (send)"}],
 [["--encrypt"],      {"help": "Encryption mode"}],
 [["--compression"],  {"default": "", "help": "Init: compression type:level"}],
 [["--hashtype"],     {"default": "", "help": "Init: hash function type"}],
 [["--chunk-factor"], {"dest": "chfactor",
                       "help": "Init: set chunk size to N*64kB"}],
 [["--vid"],          {"help": "Reference a volume by ID number (delete)"}],
 [["--passcmd"],      {"help": "Command to fetch auth passphrase"}],
 [["--meta-dir"],     {"dest": "metadir", "default": "", "help": "Use alternate metadata path"}],
 [["--meta-reduce"],  {"default": "on:3000", "help": "Metadata retention policy (see doc)"}],
 [["--json"],         {"action": "store_true", "help": "Output as json format (list)"}],
 [["--force"],        {"action": "store_true", "help": "Force action"}],
 [["--clean"],        {"action": "store_true", "help": "Delete orphan snapshots, metadata"}],
 [["--purge"],        {"choices": ["other","full"], "default": None, "help": "Delete valid snapshots"}],
 [["--maxsync"],      {"action": "store_true", "help": "Use more fs sync"}],
 [["--upgrade-format"], {"action": "store_true",
                       "help": "Upgrade archive to current spec (arch-check)"}],
 [["--tar-bypass"],   {"action": "store_true", "help": "Bypass tar for file: archives (send)"}],
 [["--opt-ssh"],      {"action": "append", "default": [], "help": "Override ssh options"}],
 [["--opt-qubes"],    {"action": "append", "default": [], "help": "Override qvm-run options"}],
 [["--debug"],        {"action": "store_true", "help": "Debug mode"}],
 [["--quiet"],        {"action": "store_true"}],
 [["--verbose"],      {"action": "count", "default": 0}],
 [["--dedup", "-d"],  {"action": "store_true", "help": "Data deduplication (send)"}],
 [["--volume-desc"],  {"dest": "voldesc", "default": "",
                       "help": "Set volume description (add, rename, send)"}]
]

options = parse_options(sys.argv[1:], parser_defs, "/etc/wyng/wyng.ini")
options.action= options.action.lower()

# Set stdout to devnull if --quiet is specified
debug = options.debug    #; assert not (options.quiet and (options.verbose or debug))
verbose = options.verbose or debug
if debug:   options.verbose = True; options.quiet = False
if options.quiet and options.action not in ("list","version"):
    options.unattended = True
    sys.stdout = open(os.devnull, "w")
signormal = {"sel": ["INT"], "iflag": True} if not debug else {"sel": None}
catch_signals(**signormal)

print("%s %s release %s" % (prog_name.capitalize(), prog_version, prog_date), flush=True)

# Check consistency of arguments
if options.action == "version":   x_it(0,"")
if options.local_from and options.action not in ("send","monitor","receive","diff"):
    x_it(1, "local-from cannot be used with "+options.action)
if options.action in write_actions+("arch-init","monitor") and not admin_permission:
    x_it(1, f"Must be root/admin user for {options.action}.")
if options.save_to and options.action != "receive":
    x_it(1, "Receive required for save-to option.")
if not (options.volumes or options.all or options.import_other_from or options.local_from) \
and options.action not in ("arch-init","arch-check","arch-deduplicate","list","delete"):
    x_it(1, f"Volume name(s) or --all required for {options.action}.")
if options.import_other_from and options.action != "send":
    x_it(1, "Option --import-other-from may only be used with send.")
if options.action not in ("send", "arch-deduplicate"):   options.dedup = False
if options.action == "arch-deduplicate":   options.dedup = True
meta_reduce = options.meta_reduce.split(":")
if meta_reduce[0] not in ("off","on","extra") \
or len(meta_reduce) != 2 or not is_num(meta_reduce[1]):
    x_it(1, "Malformed --meta-reduce argument.")
options.unattended = options.unattended or not sys.stdin.isatty()


## General Configuration ##

# vardir  : holds data not directly related to an archive, such as nicknames for archive URLs.
# cachedir: holds cached archive metadata.
vardir      = "/var/lib/"+prog_name
cachedir    = options.metadir or vardir
tmpdir      = tempfile.mkdtemp(prefix=prog_name, dir="/tmp")

os.makedirs(vardir, exist_ok=True)
shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
os.makedirs(tmpdir+"/rpc")

Destination.write_helper_program(tmpdir+"/rpc")
agent_helper_write(tmpdir)

meta_reduce, meta_min = options.meta_reduce.split(":")

# Dict of compressors in the form: (library, default_compress_level, compress_func)
compressors      =    {"zlib":   (zlib, 4, zlib.compress),
                       "bz2" :   (bz2,  9, bz2.compress)}
if zstd:   compressors["zstd"] = (zstd, 3, lambda data, lvl: zstd.compress(data, lvl, 3))

hash_funcs       = {"hmac-sha256": None,
                    "sha256"     : lambda x: hashlib.sha256(x).digest(),
                    "blake2b"    : lambda x: hashlib.blake2b(x, digest_size=hash_bytes).digest()}

# Create ArchiveSet, LocalStorage and Destination objects with get_configs().
# Passphrase input happens here; no stdin before this point!
aset        = get_configs(options)    ; dest = aset.dest
storage, storagesets = get_configs_storage(aset, require_local=not options.save_to and
                                           options.action in ("monitor","send","receive","diff"))

# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.
if aset.in_process and dest.online  \
    and options.action not in ("delete","list"):

    if (options.action == "delete" and aset.in_process[1] == options.volumes[0]) \
        or (options.clean and options.force):
        # user is currently deleting the in_process volume or using --clean --force
        aset.set_in_process(None)
    elif aset.in_process:
        open(aset.path+"/in_process_retry","w").close()
    elif exists(aset.path+"/in_process_retry"):
        x_it(1, "Interrupted process already retried; Exiting.")

    if aset.in_process and aset.in_process[0] in ("delete","merge") \
    and aset.in_process[1].startswith("Vol_"):
        print("Completing prior operation in progress:", " ".join(aset.in_process[0:2]))

        if aset.in_process[0] == "delete":
            delete_volume(storage, aset, vid=aset.in_process[1])

        elif aset.in_process[0] == "merge":
            vname = [x.name for x in aset.vols.values() if x.vid == aset.in_process[1]][0]
            merge_sessions(aset.vols[vname], aset.in_process[4].split(":|"),
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))
    else:
        raise ValueError("Bad in_process descriptor: "+repr(aset.in_process[0]))

        aset.stop()
        aset = ArchiveSet(aset.path, aset.dest, aset.opts, allvols=True, prior_auth=aset)
        if compare_files(aset, pathlist=[aset.confname], volumes=[aset.vols[vname]]):
            x_it(1, "Error: Local and archive metadata differ.")

# Display archive update time as local time
arch_time = ts_to_datetime(float(aset.updated_at))
print("Encrypted" if (aset.mcrypto and aset.datacrypto) else "Un-encrypted",
      f"archive '{dest.spec}'",
      "\nLast updated %s (%s)" % (arch_time[:-6], arch_time[-6:]), flush=True)


## Process Volume Selections ##

exclude_vols  = set(options.volex or [])
datavols      = sorted(set(aset.vols.keys()) - exclude_vols) if options.all else []
selected_vols = sorted(set(options.volumes[:] + datavols))
for v in selected_vols[:]:
    if v not in aset.vols and options.action not in {"add","delete","rename","send","arch-check"}:
        print(f"Volume '{v}' not found; Skipping.")
        selected_vols.remove(v)


## Process Commands ##

if options.action   == "monitor":
    for storage, vols in storagesets.items():
        monitor_send(storage, aset, vols or selected_vols, monitor_only=True)
    print()


elif options.action == "send":
    sid = None
    other_vols = {x: os.path.abspath(y)
                  for x,y in map(lambda a: a.split(":|:", maxsplit=1), options.import_other_from)}

    for storage, vols in storagesets.items():
        sid = monitor_send(storage, aset, vols or selected_vols, monitor_only=False, use_sesid=sid,
                           imports=other_vols)

    if other_vols:   error_cache.extend(list(other_vols))


elif options.action == "prune":
    if options.autoprune.lower() == "off" and not options.session:
        x_it(1, "Must specify --autoprune or --session for prune.")
    dvs = selected_vols

    if not options.unattended and len(dvs):
        print("This operation will delete session(s) from the archive;")
        ans = ask_input("Are you sure? [y/N]: ").strip()
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    for dv in dvs:
        if options.session:
            prune_sessions(aset.vols[dv], options.session.split(",", maxsplit=1))
        elif options.autoprune.lower() in ("on","full"):
            autoprune(aset.vols[dv], apmode=options.autoprune)


elif options.action == "receive":

    if len(selected_vols) != 1 and options.save_to:
        x_it(1, "Specify one volume for receive --save-to.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify only one session for receive.")

    if not options.unattended and not options.force:
        print("\nWarning:  Receiving to existing volumes will overwrite them!")
        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:   x_it(1)

    for storage, vols in storagesets.items():
        dvlist = vols or selected_vols
        if not storage.online and vols:
            err_out("\n Offline volumes: " + ", ".join(vols))
            error_cache.extend(vols)   ; continue

        for dv in dvlist:
            count = receive_volume(storage, aset.vols[dv], select_ses=options.session or "",
                                   ses_strict=options.session_strict=="on",
                                   save_path=options.save_to or "")
            if count is None:   error_cache.append(dv)


elif options.action == "verify":
    for dv in selected_vols:
        count = receive_volume(storage, aset.vols[dv],
                    select_ses="" if not options.session else options.session.split(",")[0],
                    ses_strict=options.session_strict=="on",
                    verify_only=1, save_path="")
        if count is None:   error_cache.append(dv)


elif options.action == "diff":
    for storage, vols in storagesets.items():
        dvlist = vols or selected_vols
        if not storage.online and vols:
            err_out("\n Offline volumes: " + ", ".join(vols))
            error_cache.extend(vols)   ; continue

        for dv in dvlist:
            count = receive_volume(storage, aset.vols[dv], save_path="", diff=True)
            if count is None:   error_cache.append(dv)


elif options.action == "list":
    show_list(aset, selected_vols)


elif options.action == "add":
    if len(options.volumes) < 1:
        x_it(1, "Volume name(s) required for add.")

    for dv in options.volumes:
        if dv not in aset.vols:   add_volume(aset, dv, options.voldesc)


elif options.action == "rename":
    if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
    rename_volume(storage, aset, options.volumes[0], options.volumes[1])


elif options.action == "delete":
    if options.clean or options.purge:
        if options.volumes:
            x_it(1, "Cannot delete volumes with clean/purge.")

        print(f"Remove local Wyng snapshots from path '{cachedir}'.")
        if not options.unattended and not options.force:
            ans = ask_input("Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:
                x_it(0,"")
        elif not options.force:
            x_it(1, "Ignoring clean/purge without --force.")

        if options.all:   meta_reduce, meta_min = "extra", 0
        remove_local_snapshots(storage, None if options.all else aset, purge=options.purge)

    elif len(options.volumes) + bool(options.vid) != 1:
        x_it(1, "Specify one volume to delete.")

    else:
        delete_volume(storage, aset, **{"vid": options.vid} if options.vid
                                  else { "dv": options.volumes[0]})


elif options.action == "arch-init":
    pass


elif options.action == "arch-check":
    arch_check(storage, aset, selected_vols)


elif options.action == "arch-deduplicate":
    dedup_existing(aset)


## END ##
