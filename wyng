#!/usr/bin/env python3
# editor width: 100   -----------------------------------------------------------------------------


###  Wyng â€“ Logical volume backup tool
###  Copyright Christopher Laprise 2018-2023 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.


# Return codes:
# 1 - Fatal error
# 2 - Error on specific volume(s)
# 3 - Auth error
# 4 - Timeout
# 5 - Destination not ready
# 6 - Archive not found
# 7 - Local storage not ready


import sys, signal, os, stat, shutil, subprocess as SPr, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, hashlib, hmac, functools, uuid
import getpass, base64, platform, resource, itertools, string, struct
import xml.etree.ElementTree, ctypes, ctypes.util
from array import array         ; from urllib.parse import urlparse

try:
    import zstd, warnings
    # Required for crippled zstd library
    warnings.filterwarnings("ignore", category=DeprecationWarning)
except:
    zstd = None

try:
    from Cryptodome.Cipher import AES as Cipher_AES
    from Cryptodome.Cipher import ChaCha20 as Cipher_ChaCha20
    from Cryptodome.Cipher import ChaCha20_Poly1305 as Cipher_ChaCha20_Poly1305
    from Cryptodome.Random import get_random_bytes
    import Cryptodome
except:
    Cipher_AES = Cipher_ChaCha20 = Cipher_ChaCha20_Poly1305 = None


# ArchiveSet manages archive metadata incl. volumes and sessions

class ArchiveSet:

    confname      = "archive.ini"   ; max_volumes  = 4096
    mhash_sz      = 44              ; max_sessions = 16384          ; comment_len = 100
    sesname_sz    = 17              ; tag_len      = 25             ; max_tags = 5
    min_chunksize = 0x10000         ; volname_len  = 4000           ; vid_len  = 10

    max_conf_sz = (volname_len + comment_len + mhash_sz + 25) * max_volumes + 1000
    max_infosz  = (tag_len+comment_len + 10) * max_tags + 1000
    max_volinfosz= (mhash_sz + sesname_sz + 10) * max_sessions + 1000

    attr_ints   = {"format_ver","chunksize","mci_count","dataci_count"}

    bkdir       = "/wyng.backup040/default" ####

    def __init__(self, top, dest, ext="", allvols=False, children=2,
                 pass_agent=0, passphrase=None, prior_auth=None):
        self.dest        = dest
        self.path        = top + self.bkdir
        self.confpath    = pjoin(self.path, self.confname)
        self.confprefix  = b"[WYNG%02d]\n" % format_version
        self.modeprefix  = b"ci = "
        self.mcrypto     = mcrypto = None
        self.datacrypto  = datacrypto = None
        self.vols        = {}
        self.dedupindex  = {}
        self.dedupsessions = []
        self.raw_hashval = None
        self.just_fetched= False
        self.Volume      = ArchiveVolume
        self.big_tmpdir  = self.path+"/.tmp"

        # persisted:
        self.format_ver  = 0
        self.chunksize   = self.min_chunksize * 2
        self.compression = "zstd" if zstd else "zlib"
        self.compr_level = str(compressors[self.compression][1])
        self.hashtype    = "blake2b"
        self.vgname      = None
        self.poolname    = None
        self.uuid        = None
        self.updated_at  = None
        self.data_cipher = self.ci_mode = None
        self.mci_count   = self.dataci_count  =  0
        self.in_process  = []

        shutil.rmtree(self.big_tmpdir, ignore_errors=True)
        os.makedirs(self.big_tmpdir, exist_ok=True)

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"], cp["volumes"], cp["in_process"]  =  {},{},{}

        # halt configuration if this is a new or temp config
        os.makedirs(self.path, exist_ok=True)
        if not exists(self.confpath+ext):
            self.uuid = str(uuid.uuid4())
            return

        # decode archive.ini file
        # Format "magic" is "[WYNGvv]\n" where vv = format version: 9 bytes
        # Format mode is next as "ci = m0\n" where m = cipher mode: 8 bytes
        with open(self.confpath+ext, "rb") as f:
            header0 = f.read(9)    ; header1 = f.read(8)    ; buf = f.read(self.max_conf_sz)
        self.raw_hashval = hashlib.sha256(header0 + header1 + buf).hexdigest()
        if not (header0 == self.confprefix and header1.startswith(self.modeprefix)):
            if not header0.startswith(b"[var]\n"):
                raise ValueError("Not a Wyng format.")
            else:
                old_v2_format = True  ; raise NotImplementedError("Old format conversion")
        else:
            old_v2_format = False    #; untrusted_ver = 2

        # use existing auth if specified
        if isinstance(prior_auth, ArchiveSet):
            self.mcrypto    = mcrypto = prior_auth.mcrypto    ; self.ci_mode = prior_auth.ci_mode
            self.datacrypto = datacrypto = prior_auth.datacrypto
            self.data_cipher = prior_auth.data_cipher
        else:
            self.ci_mode = header1[-3:-1]

        # parse metadata crypto mode and instantiate it
        if self.ci_mode != b"00" and not mcrypto:
            ci_types = DataCryptography.crypto_codes[self.ci_mode]
            if debug:   print("metadata cipher =", ci_types[1])

            # Access a key agent, if available for this UID and archive URL
            agent_name = "wyng-agent-" + str(os.getuid()) + hashlib.blake2b(
                           bytes(self.dest.spec, encoding="UTF-8"), digest_size=16).hexdigest()
            agentkeys  = agent_get(agent_name, pass_agent)
            if not passphrase and not agentkeys:   passphrase = ask_passphrase()
            if passphrase:
                assert passphrase != bytes(len(passphrase))
            elif not agentkeys:
                raise ValueError("No authentication input.")
            passphrase_s1 = passphrase[:] if passphrase else None

            self.mcrypto = mcrypto = DataCryptography(ci_types[1], self.confpath+".salt", slot=1,
                                    passphrase=passphrase_s1, agentkeys=agentkeys)

        # initial decryption + auth
        try:
            if mcrypto:   buf = mcrypto.decrypt(buf)
        except ValueError as e:
            if "MAC check failed" in e.args:
                x_it(3, "Error: Could not decrypt/authenticate archive.\n")
            raise e

        # read attributes from archive.ini
        if debug:   print(gzip.decompress(buf).decode("UTF-8"))
        cp.read_string(gzip.decompress(buf).decode("UTF-8"))
        for key, value in cp["var"].items():
            setattr(self, key, int(value) if key in self.attr_ints else value)
        self.in_process  = [ ln for ln in cp["in_process"].values() ]

        if self.format_ver > format_version or not self.format_ver:
            x_it(1,"Archive format ver = "+str(self.format_ver)+
                                ". Expected = "+str(format_version))
        #if self.compression == "zstd" and not zstd.version().startswith("1.4."):
        #    x_it(1,"python3-zstd version 1.4.x is required.")

        self.compress    = compressors[self.compression][2]
        self.decompress  = compressors[self.compression][0].decompress
        self.gethash     = hash_funcs[self.hashtype]

        # init data crypto
        if mcrypto and not self.datacrypto:
            if not float(self.updated_at) < time.time():
                raise ValueError("Current time is less than archive timestamp!")
            self.datacrypto = datacrypto \
                = DataCryptography(self.data_cipher, mcrypto.keyfile, slot=0, cadence=20,
                                   passphrase=passphrase, agentkeys=agentkeys)

            # persist keys to allow future invocations w/o passphrase input
            if pass_agent > 0 and not agentkeys:
                agent_make(agent_name, pass_agent, [datacrypto.key, mcrypto.key])

        if mcrypto:
        # use higest known counter values:
        # Enh: Examine imported .salt counters to ensure they are not wildly increased
        # above values in archive.ini (anti-DoS). Possibly warn or prompt user.
            mcrypto.counter    = self.mci_count    = max(mcrypto.counter, self.mci_count)
            datacrypto.counter = self.dataci_count = max(datacrypto.counter, self.dataci_count)

        # load volume metadata objects
        convlist = []
        for key, value in cp["volumes"].items() if children else []:
            # conditions when a volume must be loaded:
            # - allvols flag is set
            # - in_process flag from an unfinished action
            # - volume specified on command line
            # - no volumes specified (hence all)
            # - deduplication is in effect
            #if children: ## and (allvols or self.in_process or options.from_arch or \
            ##(len(options.volumes)==0 or key in options.volumes or options.dedup)):
                # instantiate:
            vid = key; hashval = value; vname = None

            #loadses = allvols or self.in_process or options.from_arch or options.dedup or
            volume  = self.Volume(self, vid, hashval, pjoin(self.path,vid),
                                    self.vgname, name=vname, children=children)
            self.vols[volume.name] = volume

        # datacrypto may not have AE, so test data chunk against a volinfo hash
        for vol, vf in ((x, x.path+"/vi.dat") for x in self.vols.values() if datacrypto):
            if exists(vf):
                if not hmac.compare_digest(vol.hashval,
                                           self.b64hash(datacrypto.decrypt(open(vf,"rb").read()))):
                    x_it(3, "Error: Data hash check failed.")
                break


    def save_conf(self, ext=""):
        c = self.conf['var']    ; c.clear()    ; mcrypto = self.mcrypto
        c['uuid']        = self.uuid or str(uuid.uuid4())
        c['updated_at']  = self.updated_at = str(time.time())
        c['format_ver']  = str(format_version)
        c['chunksize']   = str(self.chunksize)
        c['compression'] = self.compression
        c['compr_level'] = self.compr_level
        c['hashtype']    = self.hashtype
        c['vgname']      = self.vgname      # move to volname
        c['poolname']    = self.poolname    # move to ses info
        c['data_cipher'] = self.data_cipher
        if mcrypto:
            if self.datacrypto.counter > self.datacrypto.ctstart:
                self.dataci_count = self.datacrypto.counter
            self.mci_count = mcrypto.counter
            c['dataci_count']= str(self.dataci_count)
            c['mci_count']   = str(self.mci_count)
        self.conf['in_process'] = { x: ln if type(ln) is str else ":|".join(ln)
                                          for x,ln in enumerate(self.in_process) }

        os.makedirs(self.path, exist_ok=True)    ; etag = b''
        with io.StringIO() as fs:
            self.conf.write(fs)    ; fs.flush()
            buf = gzip.compress(fs.getvalue().encode("UTF-8"), 4)
        if mcrypto:   etag, buf = mcrypto.encrypt(buf)
        with open(self.confpath+ext, "wb") as f:
            f.write(b''.join((self.confprefix, self.modeprefix, self.ci_mode, b"\n")))
            f.write(etag)    ; f.write(buf)

    # Set or clear state for the archive as 'in_process' in case of interruption during write.
    # Format is list containing strings or list of strings. For latter, ':|' is the delimiter.
    def set_in_process(self, outer_list, tmp=False, todest=True):
        fssync(aset.path)
        self.in_process = [] if outer_list is None else outer_list
        self.save_conf(".tmp" if tmp else "")

        #if not tmp:   os.replace(self.confpath+".tmp", self.confpath)
        if todest:      update_dest(aset, pathlist=[self.confname], ext=".tmp" if tmp else "")

    def add_volume_meta(self, datavol, desc="", ext=""):
        errs = []
        if len(self.conf["volumes"]) >= self.max_volumes:   x_it(1, "Too many volumes")
        if datavol in self.vols:
            print(datavol+" is already configured.")    ; return None

        namecheck = self.Volume.volname_check(datavol)
        if namecheck:
            errs.append(namecheck+"\n")
        if len(desc.encode("UTF-8")) > self.comment_len:
            errs.append("Error: Max "+self.comment_len+" size for volume desc.\n")
        if not desc.isprintable():
            errs.append("Error: [^control] not allowed in volume desc.\n")
        if errs:
            sys.stderr.write("".join(errs))    ; error_cache.append(datavol)    ; return None

        while (vid := "Vol_"+os.urandom(3).hex()) in self.conf["volumes"]:   pass

        self.vols[datavol] = vol = self.Volume(self, vid, "0", pjoin(self.path,vid),
                                               self.vgname, name=datavol, children=0)
        vol.save_volinfo(ext)
        return vol

    def delete_volume_meta(self, datavol):
        # Enh: add delete-by-vid
        vid = self.vols[datavol].vid    ; vpath = self.vols[datavol].path
        del(self.vols[datavol], self.conf["volumes"][vid])
        self.save_conf()

        if exists(vpath):    shutil.rmtree(vpath)
        return vid

    def rename_volume_meta(self, datavol, newname, ext=""):
        vol = self.vols[datavol]
        if newname in self.vols or self.Volume.volname_check(newname):   return False

        vol.name = newname
        vol.save_volinfo(ext)
        return True

    def b64hash(self, buf):
        return base64.urlsafe_b64encode(self.gethash(buf).digest()).decode("ascii")

    def encode_file(self, fname, fdest=None, get_digest=True, compress=True):
        # Enh: optimize memory use
        mcrypto = self.mcrypto    ; digest = None    ; etag = b''
        destname= fdest or fname+(".z" if compress else "")

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            inbuf = bytes(inmap) if self.compression == "zstd" else inmap
            mbuf  = self.compress(inbuf, int(self.compr_level)) if compress else inbuf
            if get_digest:   digest = self.b64hash(mbuf)
            if mcrypto:      etag, mbuf = mcrypto.encrypt(mbuf)
            with open(destname,"wb") as f:   f.write(etag); f.write(mbuf)

        return digest

    def decode_file(self, fname, fdest=None, digest=None, max_sz=16000000):
        # Enh: optimize memory use
        destname= fdest or (fname[:-2] if fname.endswith(".z") else fname)
        mcrypto = self.mcrypto         ; buf_start = mcrypto.buf_start if mcrypto else 0

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            assert buf_start < len(inmap) <= max_sz ## Fix: move to get_configs_remote()
            mbuf = mcrypto.decrypt(inmap) if mcrypto else bytes(inmap)
            assert hmac.compare_digest(digest, self.b64hash(mbuf))
            open(destname,"wb").write(self.decompress(mbuf))

    def set_local(self, localpath):
        res = LocalStorage.parse_local_path(localpath)
        if not res:   x_it(7, "Ambiguous --local spec.")
        _, self.vgname, self.poolname = res


class ArchiveVolume:

    __slots__ = ("vid","name","hashval","archive","path","vgname","mapfile","sessions","sesnames",
                 "_seslist","last","meta_checked","tags","desc","changed_bytes")

    def __init__(self, archive, vid, hashval, path, vgname, name=None, children=2):
        self.vid       = vid                       ; self.tags    = {}
        self.archive   = archive                   ; self.path    = path
        self.vgname    = vgname                    ; self.mapfile = path+"/deltamap"
        self.last      = "None"                    ; self.meta_checked = False
        self.hashval   = hashval
        # persisted here:
        self.name      = name                      ; self.desc      = ""
        self.sessions  = {}                        ; self.sesnames= []
        #self.fshint    = None
        # other:
        self.changed_bytes = 0

        Ses = ArchiveSession
        if debug:  print("\n", vid, "-", name)
        if path and not exists(path):   os.makedirs(path)
        if path and hashval != "0":
            with open(pjoin(path,"volinfo"), "rb") as f:
                fsize = os.fstat(f.fileno()).st_size
                if fsize > ArchiveSet.max_volinfosz:   raise ValueError("volinfo too large")
                buf = f.read(fsize)
            if archive.mcrypto:   buf = archive.mcrypto.decrypt(buf)
            if not hmac.compare_digest(hashval, archive.b64hash(buf)):
                raise ValueError("Volume %s hash %s, expected %s" \
                                 % (vid, archive.b64hash(buf), hashval))
            if debug:   print(archive.decompress(buf).decode("UTF-8"))
            with io.StringIO(archive.decompress(buf).decode("UTF-8")) as f:
                for ln in f:
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if vname.startswith("S_"):
                        self.sessions[vname] = Ses(self, vname, value,
                                                   path+"/"+vname if children > 1 else "")
                    else:
                        setattr(self, vname, value)
        if not self.name: raise ValueError("Vol name missing")

        # session name list sorted by sequence field
        self._seslist = seslist = list(self.sessions.values()) if children > 1 else []
        seslist.sort(key=lambda x: x.sequence)
        self.sesnames = sesnames = [y.name for y in seslist]
        if sesnames:   self.last = sesnames[-1]

        if exists(pjoin(path,"volchanged")):
            self.changed_bytes = int(open(pjoin(path,"volchanged"),"r").readlines()[0].strip())


    def save_volinfo(self, ext=""):
        os.makedirs(self.path, exist_ok=True)   ; fname = "volinfo"   ; etag = b''
        with io.StringIO() as f:
            print("name =", self.name, file=f)
            print("desc =", self.desc, file=f)
            for ses in self.sessions.values():
                if ses.saved:   print(ses.name, "=", ses.hashval, file=f)

            buf = self.archive.compress(f.getvalue().encode("UTF-8"),
                                        int(self.archive.compr_level))
            self.archive.conf["volumes"][self.vid] = self.archive.b64hash(buf)

        if self.archive.mcrypto:
            # First encrypt as data, for data cipher verification at startup
            open(pjoin(self.path,"vi.dat"+ext), "wb").write(
                b''.join(self.archive.datacrypto.encrypt(buf)))
            # encrypt as metadata
            etag, buf = self.archive.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
            f.flush()      ; os.fsync(f.fileno())
        self.archive.save_conf(ext)

    def volname_check(vname): # Fix: move to LocalStorage class
        if not 0 < len(vname) <= ArchiveSet.volname_len:
            return f"Volume path/name length must be 1 - {ArchiveSet.volname_len}."
        if not vname.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in vname.strip().split("/")]):
            return "Bad volume name."
        return ""

    def volsize(self):
        return self.sessions[self.last].volsize if self.sessions else 0

    def last_chunk_addr(self, vsize=None):
        if vsize is None:  vsize = self.volsize()
        return (vsize-1) - ((vsize-1) % self.archive.chunksize)

    # Based on last session size unless volume_size is specified.
    def mapsize(self, volume_size=None):
        if not volume_size:
            volume_size = self.volsize()
        return (volume_size // self.archive.chunksize // 8) + 1

    def map_used(self, ext=""):
        return os.stat(self.mapfile+ext).st_blocks if exists(self.mapfile+ext) else 0

    def changed_bytes_add(self, amount, reset=False, save=False):
        if reset:
            if exists(self.path+"/volchanged"):   os.remove(self.path+"/volchanged")
            self.changed_bytes = 0  ; return

        self.changed_bytes += amount
        if save:
            with open(self.path+"/volchanged", "w") as f:
                print(self.changed_bytes, file=f)
                f.flush()    ; os.fsync(f.fileno())

    def init_deltamap(self, bmfile, bmsize):
        self.changed_bytes_add(0, reset=True)
        if exists(bmfile):
            os.remove(bmfile)
        with open(bmfile, "wb") as bmapf:
            bmapf.truncate(bmsize)    ; bmapf.flush()

    def new_session(self, sname, addtags={}):
        ns = ArchiveSession(self, sname, "0", addtags=addtags)
        ns.path = pjoin(self.path, sname)
        ns.sequence = self.sessions[self.last].sequence + 1 if self.sessions else 0
        ns.previous = self.last

        self.last = sname
        self.sesnames.append(sname)
        self.sessions[sname] = ns
        if self.archive.dedupindex:    self.archive.dedupsessions.append(ns)
        return ns

    def delete_session(self, sname, remove=True, force=False):
        ses     = self.sessions[sname]
        index   = self.sesnames.index(sname)    ; affected = None
        if sname == self.last and ses.saved and not force:
            raise ValueError("Cannot delete last session")

        for tag in list(ses.tags.keys()):   self.sessions[sname].tag_del(tag)
        del(self.sesnames[index], self.sessions[sname])

        if self.archive.dedupsessions:
            indexdd = self.archive.dedupsessions.index(ses)
            self.archive.dedupsessions[indexdd] = None

        # Following condition means:
        #   * sesnames cannot be empty
        #   * ses wasn't deleted from end of list

        if len(self.sesnames) > index:
            affected = self.sesnames[index]
            self.sessions[affected].previous = ses.previous

        self.last  = self.sesnames[-1] if len(self.sesnames) else "None"

        if remove and exists(pjoin(self.path, sname)):   shutil.rmtree(ses.path)
        return affected

    def decode_one_manifest(self, ses, force=False):
        if not exists(ses.path+"/manifest") or force:
            self.archive.decode_file(ses.path+"/manifest.z",
                                        digest=ses.manifesthash, max_sz=ses.manifest_max())

    def decode_manifests(self, sesnames, force=False):
        for ses in (self.sessions[x] for x in sesnames):
            if ses.path and force:
                os.makedirs(ses.path, exist_ok=True)
                do_exec([[CP.chattr, "+c", ses.path]], check=False)
            self.decode_one_manifest(ses, force=force)


class ArchiveSession:

    attr_str  = ("localtime","previous","uuid","manifesthash")
    attr_int  = ("volsize","sequence")
    attr_misc = ("tags","volume","archive","name","path","saved","loaded","toggle",
                 "hashval","meta_checked")
    __slots__ = attr_str + attr_int + attr_misc

    def __init__(self, volume, name, hashval, path="", addtags={}):
        self.volume   = volume;    self.archive = arch = volume.archive
        self.name     = name
        self.path     = path
        self.saved    = self.loaded = False
        self.toggle   = True
        self.hashval  = hashval
        self.meta_checked = False
        # persisted:
        self.uuid     = None
        self.localtime= None
        self.volsize  = None
        self.sequence = None
        self.previous = "None"
        self.tags     = {}
        self.manifesthash = None

        if path and hashval != "0":
            if debug:   print(name, ":", path)
            with open(pjoin(path,"info"), "rb") as sf:
                fsize = os.fstat(sf.fileno()).st_size
                if fsize > ArchiveSet.max_infosz:   raise ValueError("info too large")
                buf = sf.read(fsize)
            if arch.mcrypto:   buf = arch.mcrypto.decrypt(buf)
            if not hmac.compare_digest(hashval, arch.b64hash(buf)):
                raise ValueError("Session %s hash %s, expected %s" \
                                 % (name, arch.b64hash(buf), hashval))
            with io.StringIO(arch.decompress(buf).decode("UTF-8")) as sf:
                for ln in sf:
                    if ln.strip() == "uuid =":  continue
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if value == "none":   value = "None"
                    if vname == "tag":
                        self.tag_add(ArchiveSession.tag_parse(value))
                        continue

                    setattr(self, vname, 
                        int(value) if vname in self.attr_int else value)

            self.saved = self.loaded = True

        for tag in addtags:   self.tag_add(tag)


    def manifest_max(self):
        return self.volsize // self.archive.chunksize * ((256//4) + 20)


    def tag_parse(tag, delim=" "):
        result = tuple()    ; errs = []      ; parts = tag.strip().split(delim, maxsplit=1)
        tag_id = parts[0].strip().lower()    ; comment = parts[1].strip()

        if len(tag_id.encode("UTF-8")) > ArchiveSet.tag_len:
            errs.append("Error: Max "+ArchiveSet.tag_len+" size for tag ID.\n")
        if len(comment.encode("UTF-8")) > ArchiveSet.comment_len:
            errs.append("Error: Max "+ArchiveSet.comment_len+" size for comment.\n")
        if re.match(".*[,=\^]", tag_id) or any(map(str.isspace, tag_id)) \
        or not tag_id.isprintable():
            errs.append("Error: [^control], [space], and ',^=' not allowed in tag ID.\n")
        if not comment.isprintable():
            errs.append("Error: [^control] not allowed in tag comment.\n")
        if tag_id == "all":   errs.append("Error: tag 'all' is reserved.\n")
        sys.stderr.write("".join(errs))

        if not errs:
            result = (tag_id, "" if len(parts) == 1
                                    else parts[1].strip()[:ArchiveSet.comment_len])
        return result

    def tag_add(self, tag):
        if len(self.tags) >= ArchiveSet.max_tags:
            x_it(1, ArchiveSet.max_tags+" maximum tags.")
        self.saved = False    ; voltags = self.volume.tags    ; tid = tag[0]
        if tid not in self.tags:   self.tags[tid] = tag[1]
        if tid in voltags:
            voltags[tid].add(self.name)
        else:
            voltags[tid] = {self.name}
        return True

    def tag_del(self, tag):
        del(self.tags[tag])   ; voltags = self.volume.tags
        if tag in voltags:
            if self.name in voltags[tag]:   voltags[tag].remove(self.name)
            if len(voltags[tag]) == 0:    del(voltags[tag])

    def save_info(self, ext=""):
        assert self.path   ; etag = b''   ; fname = "info"   ; arch = self.volume.archive
        self.manifesthash = arch.encode_file(self.path+"/manifest"+ext,
                                             fdest=self.path+"/manifest.z"+ext)
        with io.StringIO() as f:
            for attr in self.attr_str+self.attr_int:
                print(attr, "=", getattr(self, attr), file=f)
            for tkey, tdesc in self.tags.items():
                print("tag =",   tkey, tdesc, file=f)
            buf = arch.compress(f.getvalue().encode("UTF-8"),
                                int(arch.compr_level))
            self.hashval = arch.b64hash(buf)

        if arch.mcrypto:   etag, buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(etag)  ; f.write(buf)
            f.flush()      ; os.fsync(f.fileno())
        self.saved = self.loaded = True
        self.volume.save_volinfo(ext)

# END class ArchiveSet, ArchiveVolume, ArchiveSession


def agent_helper_write(path):
    agent_program = r'''#  Copyright Christopher Laprise 2018-2023
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, signal

def sighandler(s,f):
    if s == ALRM:   raise TimeoutError("SIGALRM")

def do_mkpipe():
    if exists("/tmp/"+agent_name):   os.remove("/tmp/"+agent_name)
    os.mkfifo("/tmp/"+agent_name, mode=0o600)

class KeyHandler:
    def __init__(self):
        self.keys = []

    def __del__(self):
        for key in self.keys:
            for ii in range(len(key)):   key[ii] = 0
        os.remove("/tmp/"+agent_name)

## MAIN ##
cmd    = sys.argv[1]      ; agent_name = sys.argv[2]     ; inread = sys.stdin.buffer.read
KH     = KeyHandler()     ; magic  = b"\xff\x11\x15"     ; exists = os.path.exists
SIGINT = signal.SIGINT    ; USR1   = signal.SIGUSR1      ; ALRM   = signal.SIGALRM
for s in (SIGINT,USR1,ALRM):   signal.signal(s, sighandler)   ; signal.siginterrupt(s, True)

signal.alarm(10)    ; do_mkpipe()    ; duration = min(inread(1)[0], 60) * 60
for slot in range(2):
    KH.keys.append(bytearray(inread(inread(1)[0])))   ; assert inread(3) == magic

while True:
    try:
        signal.alarm(0)   ; res = signal.sigtimedwait({USR1}, duration)   ; signal.alarm(10)
        if res is None:   break
        with open("/tmp/"+agent_name,"wb") as npipe:
            for key in KH.keys:
                npipe.write(magic + len(key).to_bytes(1,"big"))   ; npipe.write(key)
    except TimeoutError:
        print("timeout")    ; do_mkpipe()    ; continue
'''
    with open(path+"/agent_helper.py", "wb") as progf:
        progf.write(bytes(agent_program, encoding="UTF-8"))


def agent_get(agname, duration):
    ps = SPr.check_output([CP.ps, "-u"+str(os.getuid()), "-o", "pid,command"], text=True)
    findp  = [x for x in ps.splitlines() if (agname in x and "agent_helper.py store" in x)]
    if not findp:   return None
    pid    = int(findp[0].split()[0])   ;  os_kill(pid, signal.SIGUSR1)
    result = []   ; catch_signals(["ALRM"], iflag=True) ;   signal.alarm(10)
    try:
        with open("/tmp/"+agname, "rb") as npipe:
            for slot in range(2):
                if m := npipe.read(3) != Destination.magic:
                    raise ValueError("*Magic not found, got "+repr(m))
                result.append(key := bytearray(npipe.read(ksz := npipe.read(1)[0])))
                if len(key) != ksz:   raise ValueError("Key length")
    except (TimeoutError, ValueError) as e:
        for k in result:   clear_array(k)
        result = None    ; os_kill(pid)
    finally:
        if duration == -1:   os_kill(pid)
        signal.alarm(0)  ; catch_signals(None)
        return result


def agent_make(agname, duration, keys):
    p = SPr.Popen([CP.python, tmpdir+"/agent_helper.py","store",agname], text=False,
                          shell=False, stdin=SPr.PIPE, stdout=SPr.DEVNULL, stderr=SPr.DEVNULL)
    pwrite = p.stdin.write   ; pwrite(min(duration,60).to_bytes(1,"big"))
    for k in keys:   pwrite(len(k).to_bytes(1,"big"))   ; pwrite(k)   ; pwrite(Destination.magic)
    p.poll(); p.stdin.flush(); p.stdin.close()
    return p


# DataCryptography(): Handle crypto functions and state for volume data

class DataCryptography:

    crypto_key_bits = 256        ; max_ct_bits = 128
    max_keyfile_sz  = ((crypto_key_bits*2) + max_ct_bits) * 2 // 8
    time_headroom   = int(60*60*24*365.25*50)             ; timesz  = 32 // 8

    # Matrix of recommended mode pairs = 'formatcode: (data, metadata)'
    # User selects a data cipher which is automatically paired w a metadata authentication cipher.
    crypto_codes    = {b"00":  ("off",                "off"),
                       b"10":  ("aes-256-siv",        "aes-256-siv"),
                       b"20":  ("aes-256-cbc",        "aes-256-siv"),
                       b"30":  ("xchacha20",          "xchacha20-poly1305"),
                       b"40":  ("xchacha20-poly1305", "xchacha20-poly1305")}


    def __init__(self, ci_type, keyfile, slot, passphrase, agentkeys=None, cadence=1, init=False):

        if tuple(time.gmtime(0))[:6] != (1970, 1, 1, 0, 0, 0):
            x_it(1, "System time epoch is not 1970-01-01.")
        assert passphrase is None or type(passphrase) == bytearray
        assert type(cadence) is int and cadence > 0

        if not issubclass(type(keyfile), io.IOBase):
            if not exists(keyfile) and init:   open(keyfile, "wb").close()
            keyfile =  open(keyfile, "r+b")

        self.keyfile = keyfile          ; self.ci_type = ci_type
        self.key     = None             ; kbits = self.crypto_key_bits
        self.slot    = slot             ; self.counter  = self.ctstart = None
        self.ctcadence = cadence        ; self.auth     = False
        self.timer   = time.time        ; self.get_rnd  = get_random_bytes

        if ci_type == "aes-256-cbc":
            raise NotImplementedError()

        elif ci_type == "aes-256-siv":
            if not (platform.machine() == "x86_64" and "aes" in cpu_flags):
                print("Warning: x86_64 + AES crypto sidechannel resistance not detected.")
            self.key_sz  = 2*kbits//8              ; self.max_count = 2**48-64
            self.nonce_sz= 12                      ; self.tag_sz    = 16
            self.buf_start = self.nonce_sz+self.tag_sz
            self.mode    = Cipher_AES.MODE_SIV     ; self.AES_new = Cipher_AES.new
            self.countsz = self.max_count.bit_length() // 8
            self.randomsz=self.nonce_sz - self.countsz
            self.encrypt = self._enc_aes_256_siv
            self.decrypt = self.auth = self._dec_aes_256_siv

        elif ci_type == "xchacha20":
            if Cryptodome.version_info[0:2] < (3,9):
                raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")
            self.key_sz  = kbits//8                ; self.max_count = 2**80-64
            self.nonce_sz= 24                      ; self.buf_start = self.nonce_sz
            self.countsz = self.max_count.bit_length() // 8
            self.randomsz=self.nonce_sz - self.countsz - self.timesz
            self.ChaCha20_new = Cipher_ChaCha20.new
            self.encrypt = self._enc_chacha20
            self.decrypt = self._dec_chacha20

        elif ci_type == "xchacha20-poly1305":
            if Cryptodome.version_info[0:2] < (3,9):
                raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")
            self.key_sz  = kbits//8         ; self.max_count = 2**80-64    ; self.tag_sz = 16
            self.nonce_sz= 24               ; self.buf_start = self.nonce_sz + self.tag_sz
            self.countsz = self.max_count.bit_length() // 8
            self.randomsz=self.nonce_sz - self.countsz - self.timesz
            self.ChaCha20_Poly1305_new = Cipher_ChaCha20_Poly1305.new
            self.encrypt = self._enc_chacha20_poly1305
            self.decrypt = self.auth = self._dec_chacha20_poly1305

        else:
            raise ValueError("Invalid cipher spec "+ci_type)

        self.slot_offset = (kbits*2//8 + self.max_ct_bits//8) * self.slot

        # Load counter and key
        if init:
            self.key     = self.new_key(passphrase)
        else:
            keyfile.seek(self.slot_offset)    ; ctbytes = keyfile.read(self.countsz)
            self.counter = self.ctstart = cadence - 1 + int.from_bytes(ctbytes, "big")
            salt         = keyfile.read(self.key_sz)   ; assert len(salt) == self.key_sz

            if agentkeys:
                self.key = agentkeys[slot]
            else:
                self.key = self.derive_key(salt, passphrase)
            assert len(self.key) == self.key_sz and type(self.key) is bytearray


    def __del__(self):
        if self.counter and self.counter > self.ctstart:   self.save_counter()
        if self.key:   clear_array(self.key)


    # Key file binary format: counter=8B, key=key_sz
    def new_key(self, passphrase):
        salt = self.get_rnd(self.key_sz)      ; self.counter = self.ctstart = 0
        self.keyfile.seek(self.slot_offset)   ; self.keyfile.write(bytes(self.countsz) + salt)
        return self.derive_key(salt, passphrase)

    def derive_key(self, salt, passphrase):
        key = bytearray(hashlib.scrypt(passphrase, salt=salt, n=2**19, r=8, p=1,
                                        maxmem=640*1024*1024, dklen=self.key_sz))
        clear_array(passphrase)
        return key

    def save_counter(self):
        self.keyfile.seek(self.slot_offset)
        self.keyfile.write(self.counter.to_bytes(self.countsz, "big"))

    # Encrypt aes-256-siv:
    def _enc_aes_256_siv(self, buf):
        self.counter += 1
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        nonce  = self.get_rnd(self.randomsz) + self.counter.to_bytes(self.countsz, "big")
        cipher = self.AES_new(self.key, self.mode, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        return  nonce + ci_tag + buf

    # Decrypt aes-256-siv:
    def _dec_aes_256_siv(self, untrusted_buf):
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.AES_new(self.key, self.mode, nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)

    # Encrypt [X]ChaCha20:
    def _enc_chacha20(self, buf):
        self.counter += 1
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.timer()) - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        buf    = cipher.encrypt(buf)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        return  nonce, buf

    # Decrypt [X]ChaCha20:
    def _dec_chacha20(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        return cipher.decrypt(untrusted_buf[self.nonce_sz:])

    # Encrypt [X]ChaCha20-Poly1305:
    def _enc_chacha20_poly1305(self, buf):
        self.counter += 1
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = b''.join(( (int(self.timer()) - self.time_headroom).to_bytes(self.timesz, "big"),
                            self.get_rnd(self.randomsz),
                            self.counter.to_bytes(self.countsz, "big")
                 ))
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        return  b''.join((nonce, ci_tag)), buf

    # Decrypt [X]ChaCha20-Poly1305:
    def _dec_chacha20_poly1305(self, buf):
        untrusted_buf = memoryview(buf)
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)


# Define absolute paths of commands

class CP:
    awk    = "/usr/bin/awk"     ; sed   = "/bin/sed"        ; sort     = "/usr/bin/sort"
    cat    = "/bin/cat"         ; mkdir = "/bin/mkdir"      ; python   = "/usr/bin/python3"
    mv     = "/bin/mv"          ; grep  = "/bin/grep"       ; ssh      = "/usr/bin/ssh"
    sh     = "/bin/sh"          ; tar   = "/bin/tar"        ; tail     = "/usr/bin/tail"
    rm     = "/bin/rm"          ; lvm   = "/usr/sbin/lvm"   ; qvm_run  = "/usr/bin/qvm-run"
    tee    = "/usr/bin/tee"     ; sync  = "/bin/sync"       ; dmsetup  = "/sbin/dmsetup"
    chattr = "/usr/bin/chattr"  ; xargs = "/usr/bin/xargs"  ; sha256sum= "/usr/bin/sha256sum"
    cmp    = "/usr/bin/cmp"     ; gzip  = "/bin/gzip"       ; env      = "/usr/bin/env"
    ps     = "/usr/bin/ps"      ; uniq  = "/usr/bin/uniq"   ; filefrag = "/usr/sbin/filefrag"
    thin_delta = "/usr/sbin/thin_delta"    ; cp     = "/bin/cp"
    blkdiscard = "/sbin/blkdiscard"        ; ionice = "/usr/bin/ionice"
    btrfs      = "/usr/sbin/btrfs" if os.path.exists("/usr/sbin/btrfs") else "/bin/btrfs"


# Manage local (source) data volumes, thin lvm and reflink image files and snapshots.
# Volume names are mapped into the 'lvols' cache based on whether they are found
# under the current --local path or if a volume name exists in the archive.
# lvols enties may point to local volumes that are non-existant, so use v.exists().

class LocalStorage:

    BLKDISCARD               = 0x1277           ; BLKDISCARDZEROES        = 0x127c
    FALLOC_FL_KEEP_SIZE      = 0x01             ; FALLOC_FL_PUNCH_HOLE    = 0x02
    FALLOC_FL_COLLAPSE_RANGE = 0x08             ; FALLOC_FL_ZERO_RANGE    = 0x10
    FALLOC_FL_INSERT_RANGE   = 0x20             ; FALLOC_FL_UNSHARE_RANGE = 0x40
    FALLOC_FL_PUNCH_FULL     = FALLOC_FL_KEEP_SIZE | FALLOC_FL_PUNCH_HOLE

    fallocate = ctypes.CDLL(ctypes.util.find_library("c")).fallocate
    fallocate.restype = ctypes.c_int
    fallocate.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_int64, ctypes.c_int64]

    def __init__(self, localpath_t, auuid=None, arch_vols=[], clean=False, sync=False,
                 require_online=False):

        self.stypes  = { "tlvm":  LvmVolume,   "rlnk": ReflinkVolume }
        self.rltypes = { "btrfs", "xfs" }

        assert len(auuid) > 8
        self.gc_procs  = []          ; self.locked  = False      ; self.auuid      = auuid
        self.clean     = clean       ; self.sync    = sync
        self.lvols     = {}          ; self.vgs_all = {}
        self.path  = self.pooltype = self.fstype = self.lvpool  = None

        if localpath_t[0]:
            self.pooltype       = "tlvm"
            self.block_size     = 512
            self.path           = "/dev/"+localpath_t[0]+"/"    ; self.vgname = localpath_t[0]
            self.online         = LocalStorage.vg_exists(localpath_t[0]) and exists(self.path)
            self.lvpool         = localpath_t[1]
            self.acquire_deltas = get_lvm_deltas
            self.process_deltas = update_delta_digest_lvm
            self.prep_snapshots = prepare_snapshots_lvm

        elif not localpath_t[0] and localpath_t[1].startswith("/"):
            self.pooltype       = "rlnk"
            self.block_size     = 4096   # Test this with XFS ###
            self.path           = localpath_t[1]
            self.online         = exists(self.path)
            self.acquire_deltas = get_reflink_deltas
            self.process_deltas = update_delta_digest_reflink
            self.prep_snapshots = prepare_snapshots_reflink
            if self.online:
                if (fs := LocalStorage.get_fs_type(self.path)) in self.rltypes:   self.fstype = fs
                self.snappath   = self.path + ("wyng_snapshot/" if self.fstype == "btrfs" else "")

        elif require_online:
            raise ValueError("Indeterminate local path.")

        else:
            self.online = False

        if require_online and not self.online:
            x_it(7, "Local storage is offline: "+repr(localpath_t))

        if self.online:  #Fix: make conditional on send/monitor/receive
            self.LVolClass    = self.stypes[self.pooltype]
            self.path = self.path.rstrip("/")+"/"
            self.update_vol_list(arch_vols)

        if debug:
            print("**fstype is", self.fstype)
            print("**pooltype", self.pooltype, "not" if not self.online else "", "online")
            print(self.path, self.lvpool)

    def __del__(self):
        if self.clean:
            for p in self.gc_procs:   p.wait()


    # Note: file_punch_hole() and block_discard_chunk() have the same arg signature...
    def file_punch_hole(self, fn, start, length):
        return self.fallocate(fn, self.FALLOC_FL_PUNCH_FULL, start, length)

    def block_discard_chunk(self, fn, start, length):
        try:
            return fcntl.ioctl(fn, self.BLKDISCARD, struct.pack("LL", start, length))
        except Exception as e:
            return None

    def metadata_lock(self, lvpool=None):
        mark = 0    ; spath = self.path
        if self.pooltype == "tlvm":
            self._lvm_meta_snapshot("reserve")
        elif self.pooltype == "rlnk":
            if not self.fstype or self.fstype not in ("btrfs","xfs"):
                raise ValueError("Bad fstype "+repr(self.fstype))
            if self.fstype == "btrfs":
                mark, spath = self._btrfs_subvol_snapshot()
            elif self.fstype == "xfs":
                pass # possibly file-lock and chmod -r rlnk snapshots
        self.locked = True
        return mark, spath

    def metadata_unlock(self, lvpool=None):
        mark = 0
        if self.pooltype == "tlvm":
            self._lvm_meta_snapshot("release")
        elif self.pooltype == "rlnk":
            if self.fstype == "btrfs":
                mark = self._btrfs_subvol_snapshot(delete=True)
        self.locked = False
        return mark

    def _btrfs_subvol_snapshot(self, delete=False):
        svpath = self.path    ; dest = self.snappath    ; gen = 0

        if exists(dest):
            if delete:   gen = self._get_btrfs_generation(dest)
            do_exec([[CP.btrfs, "subvolume", "delete", dest]])
        if not delete:
            do_exec([[CP.btrfs, "subvolume", "snapshot", "-r", svpath, dest]])
            gen = self._get_btrfs_generation(dest)
            # possibly check /sys/fs/btrfs/uuid#/exclusive_operation

            if debug:   print("Created subvol snapshot at", dest)
        return gen, dest

    def _get_btrfs_generation(self, path):
        res = SPr.check_output([CP.btrfs, "subvolume", "list", path])
        if debug:   print("*generation", res)
        return int(res.split()[3])

    # Reserve or release lvm thinpool metadata snapshot.
    # action must be "reserve" or "release".
    def _lvm_meta_snapshot(self, action, pool=None):
        vgname   = os.path.basename(self.path).replace("-","--")
        poolname = (pool or self.lvpool).replace("-","--")
        do_exec([[CP.dmsetup,"message", vgname+"-"+poolname+"-tpool",
                "0", action+"_metadata_snap"]], check= action=="reserve")

    def new_vol_entry(self, newname):
        if newname not in (lvols := self.lvols):
            lvols[newname] = vol = self.LVolClass(self, newname)
        else:
            vol = lvols[newname]

        for sv in (vol.snap1, vol.snap2) if vol.snap1 else []:
            if sv and sv not in lvols:   lvols[sv] = self.LVolClass(self, sv)
        return vol

    # Create survey of all interesting volumes
    def update_vol_list(self, arch_vols):
        self.lvols.clear()
        if self.pooltype == "tlvm":   self.update_lvm_list()
        for avol in arch_vols:   self.new_vol_entry(avol)

    # Retrieves survey of all LVM VGs/LVs
    def update_lvm_list(self):
        if not shutil.which(CP.lvm):   sys.stderr.write("LVM not available.\n"); return

        delim   = ":::"            ; colnames = LvmVolume.colnames
        vgs_all = self.vgs_all     ; LvmVol   = LvmVolume

        do_exec([[CP.lvm, "lvs", "--units=b", "--noheadings", "--separator="+delim,
                    "--options=" + ",".join(colnames)]],
                out=tmpdir+"/volumes.lst")

        for ln in open(tmpdir+"/volumes.lst", "r"):
            lv = LvmVol(self, "", members=zip(colnames, ln.strip().split(delim)))
            vgs_all.setdefault(lv.vg_name, {})[lv.name] = lv

        self.lvols = vgs_all[self.vgname]

    # static
    def vg_exists(vgname):
        try:
            do_exec([[CP.lvm, "vgdisplay", vgname]])
        except SPr.CalledProcessError:
            return False
        else:
            return True

    def get_fs_type(path):
        mtab = {x[1]: x[2] for x in map(str.split, open("/etc/mtab","r"))}
        while path not in mtab and path != "/":   path = os.path.dirname(path)
        return mtab[path]

    # accepts either a volgroup/pool or directory path and returns type, tpool, absolute path
    def parse_local_path(localpath):
        lvname, lvpool, vg, lvattr = LocalStorage.get_lv_path_pool(localpath)
        abspath = os.path.abspath(localpath)

        if lvname and not lvpool and lvattr.startswith("t") and exists("/dev/"+vg):
            return ("logical volume", lvname, "/dev/"+vg)
        elif abspath.startswith("/dev"):
        #and stat.S_ISBLK(os.stat(localpath).st_mode):
            return ("block device", "", abspath)
        elif abspath.startswith("/") and exists(abspath) and os.path.isdir(abspath):
            return ("file", "", abspath)
        else:
            return (None, None, None)

    # Converts a non-cannonical LV path to LV name plus pool and vg names.
    def get_lv_path_pool(path):
        try:
            p = SPr.run([CP.lvm, "lvs", "--separator=:::", "--noheadings",
                                "--options=lv_name,pool_lv,vg_name,attr", path], check=True,
                                stdout=SPr.PIPE, stderr=SPr.DEVNULL)
        except:
            return "", "", "", ""
        else:
            return p.stdout.decode("utf-8").strip().split(":::")


# Base class for local volumes; do not instantiate.

class LocalVolume:
    maxname   = 255    ; maxpath = 4096 - maxname
    __slots__ = ("storage","lockfile","pdir","path","name","snap1","snap2")

    def _my_init(self, storage, name):
        self.storage   = storage                        ; self.lockfile = None
        self.pdir      = storage.path.rstrip("/")+"/"   ; self.path     = self.pdir+name
        self.name      = name
        self.snap1 = self.snap2 = None

    def lock(self, mode="r+b"):
        self.lockfile = lf = open(self.path, mode)
        fcntl.lockf(lf, fcntl.LOCK_EX|fcntl.LOCK_NB)
        return lf

    def unlock(self):
        if self.lockfile:   self.lockfile.close()
        return True

    def rotate_snapshots(self, rotate=True, timestamp_path=None):
        lvols = self.storage.lvols
        if rotate:
            lvols[self.snap2].rename(self.snap1)
            os.fsync(open(self.pdir+self.snap1,"rb"))
            t = self.gettime()    ; os.utime(timestamp_path, times=(t,t))
            return t
        else:
            lvols[self.snap2].delete(sync=False)
            return None

    def set_mode_bits(self, modebits):
        raise NotImplementedError()

    def exists(self):
        return exists(self.path)

    def check_pathname(path):
        if not 0 < len(path) <= LocalVolume.maxpath:
            return f"Volume path/name length must be 1 - {LocalVolume.maxpath}."
        if not path.isprintable():
            return "Non-printable characters not allowed in volume names."
        if any([x in (".","..") for x in path.strip().split("/")]):
            return "Bad volume name."


class ReflinkVolume(LocalVolume):

    snap_ext = (".wyng1",".wyng2")

    def __init__(self, storage, name):
        super()._my_init(storage, name)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            fdir, fname  = os.path.split(name)    ; subdir = fdir+"/" if fdir else ""
            idhash = hashlib.blake2b(bytes(self.storage.auuid+name,
                                     encoding="UTF-8"), digest_size=32).hexdigest()
            self.snap1 = subdir + idhash + self.snap_ext[0]
            self.snap2 = subdir + idhash + self.snap_ext[1]

    def rename(self, new_name):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        newvol = self.storage.lvols[new_name]
        newvol.delete()
        if self.exists() and self.unlock():
            if debug:   print("*rename", self.path, newvol.path)
            os.makedirs(newvol.pdir, exist_ok=True)
            os.replace(self.path, newvol.path)

        del(self.storage.lvols[self.name])

    def delete(self, sync=True, check=False):
        assert self.name.endswith(self.snap_ext)
        self.unlock()
        if self.exists():   os.remove(self.path)

    def create(self, size=None, snapshotfrom=None, ro=True):
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        os.makedirs(self.pdir, exist_ok=True)
        if snapshotfrom:
            snap_path = self.storage.lvols[snapshotfrom].path
            do_exec([[CP.cp, "-p", "--reflink=always", snap_path, self.path]])
        else:
            assert size is not None
            open(self.path, "wb").truncate(size)
        self.storage.lvols[self.name] = self

    def gettime(self):
        return float(os.path.getmtime(self.path))

    def resize(self, size):
        if not self.exists():   raise FileNotFoundError(self.path)
        open(self.path, "r+b").truncate(size)

    def getsize(self):
        return os.path.getsize(self.path)

    def is_arch_member(self):
        if not self.name.endswith(self.snap_ext) or not self.exists():
            return "na"
        else:
            return "true"

    def convert_pathname(self, path):
        raise NotImplementedError()


class LvmVolume(LocalVolume):

    snap_ext  = (".tick",".tock")
    maxname   = 112    ; maxpath = maxname    ; alphanumsym = r"^[a-zA-Z0-9\+\._-]+$"
    colnames  = ("vg_name","lv_name","lv_path","lv_attr","lv_size",
                 "lv_time","pool_lv","thin_id","tags")
    __slots__ = LocalVolume.__slots__ + colnames

    def __init__(self, storage, name=None, members=[]):

        self.lv_path = None    ; self.tags = ""
        for attr, val in members:   setattr(self, attr, val)

        super()._my_init(storage, name or self.lv_name)
        name = self.name    ; assert bool(name)

        # assign snapshot names
        if not name.endswith(self.snap_ext):
            self.snap1 = name + self.snap_ext[0]
            self.snap2 = name + self.snap_ext[1]

    def rename(self, new_name):
        assert all((x.endswith(self.snap_ext) for x in (self.name, new_name)))
        storage = self.storage

        m = ((x, getattr(self,x)) for x in set(self.colnames) - {"lv_name","lv_path","thin_id"})
        storage.lvols[new_name] = newvol = LvmVolume(storage, name=new_name, members=m)
        del(storage.lvols[self.name])    ; storage.new_vol_entry(self.name)

        newvol.delete()
        if self.exists() and self.unlock():
            do_exec([[CP.lvm, "lvrename", self.path, new_name]])

    def delete(self, sync=True, check=False):
        # Enh: re-write with asyncio
        sync = (optsync := options.maxsync) or sync    ; clean = options.clean
        assert not (sync == False and check)
        assert self.name.endswith(self.snap_ext)

        if self.exists():
            self.unlock()    ; procs = self.storage.gc_procs    ; maxprocs = 16
            if len(procs) == maxprocs:
                for ii in reversed(range(len(procs))):
                    retcode = procs[ii].returncode
                    if retcode is not None:
                        if clean and retcode != 0:
                            raise CalledProcessError("lvremove failed "+str(retcode))
                        del(procs[ii])

            cmds = [CP.sh, "-c", CP.lvm + " lvchange -p rw " + self.path + " ; "
                 +  CP.lvm + " lvremove -f " + self.path]
            if not (sync or clean):   cmds = [CP.ionice, "-c3"] + cmds
            p = SPr.Popen(cmds, shell=False,
                                stdout=SPr.DEVNULL, stderr=SPr.DEVNULL)
            if not sync and len(procs) < maxprocs:
                procs.append(p)
            else:
                retcode = p.wait()
                if check and retcode != 0:
                    raise CalledProcessError("lvremove failed "+str(retcode))

    def create(self, size=None, snapshotfrom=None, ro=True, addtags=[]):
        vg = self.storage.path    ; rwmode = ["-pr"+("w" if not ro else "")]
        if self.exists():   raise ValueError(f"Volume {self.name} already exists.")
        if snapshotfrom:
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay"] + rwmode + addtags + [
                        "-s", vg+"/"+snapshotfrom, "-n", self.name]])
        else:
            assert size is not None
            do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(size)+"b"] + rwmode
                      + addtags + ["--thin", "-n", self.name, vg+"/"+self.storage.lvpool]])

        self.storage.lvols[self.name] = self

    def gettime(self):
        return float(time.mktime(time.strptime(self.lv_time, r"%Y-%m-%d %H:%M:%S %z")))

    def resize(self, size):
        do_exec([[CP.lvm, "lvresize", "-L", str(size)+"b", "-f", self.path]])

    def getsize(self):
        return int(re.sub("[^0-9]", "", self.lv_size))

    def is_arch_member(self):
        if not self.name.endswith(self.snap_ext) or "wyng" not in self.tags or not self.exists():
            return "na"
        if "arch-"+self.storage.auuid not in self.tags:
            return "false"
        else:
            return "true"

    def check_pathname(path):
        if res := super.check_pathname(path):
            return res
        if re.match(LvmVolume.alphanumsym, path) is None:
            return "Only characters A-Z 0-9 . + _ - are allowed in LVM volume names."
        return ""

    def convert_pathname(self, path):
        raise NotImplementedError()


# Try to sync only selected filesystem
def fssync(path):
    if options.maxsync:   SPr.Popen([CP.sync,"-f",path])


def ask_passphrase(prompt="Enter passphrase: ", verify=False):
    for ii in range(3):
        passphrase  = bytearray(getpass.getpass(prompt), encoding="UTF-8")
        if not verify or len(passphrase) >= 10:   break
        print("Passphrase must be 10 or more characters.")   ; clear_array(passphrase)
        if ii == 2 or options.unattended:   x_it(3, "Passphrase required.")
    if options.unattended or not verify:   return passphrase
    passphrase2 = bytearray(getpass.getpass("Re-enter passphrase: "), encoding="UTF-8")
    if passphrase != passphrase2:
        clear_array(passphrase)   ; clear_array(passphrase2)
        x_it(3, "Entries do not match.")
    clear_array(passphrase2)
    return passphrase


def clear_array(ar):
    for ii in range(len(ar)):   ar[ii] = 0


# Initialize a new ArchiveSet:

def arch_init(aset, opts):
    if not opts.local:
        x_it(1,"--local is required.")

    aset.set_local(opts.local)

    aset.data_cipher = opts.encrypt.lower()
    # Fix: duplicates code in aset... move to aset class.
    if aset.data_cipher in (x[0] for x in DataCryptography.crypto_codes.values()): 
        aset.ci_mode, ci= [(x,y) for x,y in DataCryptography.crypto_codes.items()
                                 if y[0] == aset.data_cipher][0]

        if aset.data_cipher != "off":
            ##if opts.unattended:   x_it(1, "Enter passphrase interactively.")
            # Security Enh: Possibly use mmap+mlock to store passphrase/key values,
            #               and wipe them from RAM after use.
            passphrase      = ask_passphrase(prompt="Enter new encryption passphrase: ",
                                             verify=True)
            aset.mcrypto    = DataCryptography(ci[1], aset.confpath+".salt", slot=1,
                                            passphrase=passphrase[:], init=True)
            aset.datacrypto = DataCryptography(aset.data_cipher, aset.mcrypto.keyfile, slot=0,
                                            passphrase=passphrase, init=True)
    else:
        x_it(1,"Error: Invalid cipher option.")

    print(); print(f"Encryption    : {aset.data_cipher} ({ci[1]})")

    if opts.hashtype:
        if opts.hashtype not in hash_funcs:
            x_it(1, "Hash function '"+opts.hashtype+"' is not available on this system.")
        aset.hashtype = opts.hashtype

    print("Hashing       :", aset.hashtype)

    if opts.compression:
        if ":" in opts.compression:
            compression, compr_level = opts.compression.strip().split(":")
        else:
            compression = opts.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not is_num(compr_level):
            x_it(1, "Invalid compression spec.")
        #if compression == "zstd":
        #    print("Warning: zstd does not support reproducible output; future zstd updates may "
        #          "prevent Wyng from deduplicating data.")
        aset.compression = compression      ; aset.compr_level = compr_level
        compressors[compression][2](b"test", int(compr_level))

    print("Compression   : %s:%s" % (aset.compression, aset.compr_level))

    if opts.chfactor:
        # accepts an exponent from 1 to 6
        if not ( 0 < opts.chfactor < 7 ):
            x_it(1, "Requested chunk size not supported.")
        aset.chunksize = (aset.min_chunksize//2) * (2** opts.chfactor)
        if aset.chunksize > 256 * 1024:
            print("Large chunk size set:", aset.chunksize)

    aset.save_conf()    ; update_dest(aset, pathlist=[aset.confname, aset.confname+".salt"])


# Check/verify an entire archive

def arch_check(storage, aset, vol_list=None, startup=False):
    dest     = aset.dest                   ; attended  = not options.unattended
    gethash  = hash_funcs[aset.hashtype]   ; chunksize = aset.chunksize
    compare_digest = hmac.compare_digest   ; b64enc    = base64.urlsafe_b64encode

    decrypt  = aset.datacrypto.decrypt if aset.datacrypto else None

    vol_list = vol_list or list(aset.vols.keys())
    vol_dirs = set((x.name for x in os.scandir(aset.path) \
                            if x.is_dir() and x.name.startswith("Vol_")))

    vdir_strays = vol_dirs - set(aset.conf["volumes"].keys())
    if vdir_strays:
        print("Stray volume dirs:", vdir_strays)

    # Remove orphan snapshots
    for lv in (x for x in storage.lvols.values() if x.is_arch_member() == "false"):
        if options.clean:
            lv.delete()
            print("Removed orphan snapshot:", lv.name)
    # Fix... Extend to scan of reflink storage

    # Check volume contents at various levels (dirs, metadata, content)
    for volname in vol_list if dest.online else []:
        if volname not in aset.vols:   continue

        vol = aset.vols[volname]
        if not startup or debug:   print("Volume", volname, flush=True)

        # Remove session tmp dirs
        for sdir in os.scandir(vol.path):
            if sdir.name.startswith("S_") and sdir.name.endswith("-tmp"):
                if debug:   print("Removing partial session dir '%s'" % sdir.name)
                dest.run([dest.cd + " && rm -rf " + vol.vid+"/"+sdir.name])
                shutil.rmtree(vol.path+"/"+sdir.name, ignore_errors=True)

        # Check session sequencing
        seslist = vol._seslist
        for si, ses in enumerate(seslist[1:]):
            if ses.previous != seslist[si].name:
                raise ValueError(f"Prev Out of sequence: {ses.name} -> {seslist[si].name}")
            if ses.sequence == seslist[si].sequence:
                raise ValueError(f"Duplicate sequence {ses.sequence} in {ses.name}")
        vol._seslist = None

        if startup:   continue

        # Check all combined manifests are correct
        print("  Checking indexes,", end="", flush=True) ; mset = []
        for ses in vol.sesnames:   mset.append(ses)    ; check_manifest_sequence(vol, mset)

        # Check hashes of each session individually
        print(" data:")
        for sesname in reversed(vol.sesnames):
            if options.session and ((options.session.lower() == "newest" \
            and sesname != vol.sesnames[-1]) or options.session < vol.sessions[sesname].localtime):
                continue # Enh: use seq

            if attended:   print(" ", sesname[2:], end="... ", flush=True)
            bcount = receive_volume(storage, vol, select_ses=sesname[2:], verify_only=2)
            #if attended:   print(bcount, "bytes OK")


def check_manifest_sequence(vol, sesnames):
    volsize = vol.sessions[sesnames[-1]].volsize    ; aset = vol.archive
    with open(merge_manifests(vol, msessions=sesnames, addcol=False), "r") as mrgf:
        for addr in range(0, volsize, aset.chunksize):
            ln = mrgf.readline().strip()
            if not ln:   break
            ln1, ln2 = ln.split()
            if ln1 != "0":
                assert len(ln1) == aset.mhash_sz    ; h1 = base64.urlsafe_b64decode(ln1)
            assert len(ln2) == aset.sesname_sz      ; a1 = int("0"+ln2, 16)
            if addr != a1:
                print(ln); raise("Manifest seq error. Expected %d got %d." % (addr, a1))

    if addr+aset.chunksize != volsize:
        raise ValueError("Manifest range stopped short at", addr)


# Get configuration settings:

def get_configs(opts):
    load_children = 1 if opts.action in ("arch-delete","delete","add") else 2

    dest = Destination(opts.dest, opts.dest_name, ArchiveSet.bkdir)
    if (opts.action not in local_actions and dest.sys is not None) \
    or opts.remap:
        dest.detect_state(opts.dedup)

    # Check online status for certain commands.
    if not dest.online and (opts.remap  \
    or opts.action not in local_actions) and not (opts.action == "delete" and opts.clean):
        x_it(5, "Destination not ready to receive commands.")
    if not dest.writable and opts.action in write_actions \
    and not (opts.action == "delete" and opts.clean):
        x_it(5, "Destination not writable.")
    if dest.archive_ini_hash == "none" and opts.action not in local_actions+("arch-init",):
        x_it(6,"Archive not found at '%s'" % dest.spec)

    #### Test dest.archive_ini_hash here:

    aset = ArchiveSet(metadir, dest, children=load_children, pass_agent=opts.pass_agent)
 
    if opts.action == "arch-init" and aset.updated_at is None:
        arch_init(aset, opts)
    elif opts.action == "arch-init":
        x_it(1, "Archive already initialized for "+dest.spec)
    elif aset.updated_at is None:
        if exists(metadir+ArchiveSet.bkdir+".old"):
            shutil.rmtree(metadir+ArchiveSet.bkdir+".old")
        if exists(metadir+ArchiveSet.bkdir):
            os.replace(metadir+ArchiveSet.bkdir, metadir+ArchiveSet.bkdir+".old")
        aset = get_configs_remote(dest, metadir)

    if aset.updated_at is None:
        x_it(6, "Archive not found.")
    elif opts.local:
        aset.set_local(opts.local)

    return aset


# Fetch copy of archive metadata from remote/dest

def get_configs_remote(dest, arch_dir):

    recv_dir = arch_dir+ArchiveSet.bkdir

    recv_list = [(ArchiveSet.confname,  ArchiveSet.max_conf_sz),
                 (ArchiveSet.confname+".salt", DataCryptography.max_keyfile_sz)]
    fetch_file_blobs(recv_list, recv_dir, dest, ext=".auth_req", skip0=True)

    # Instantiate ArchiveSet to authenticate archive.ini
    # Fix: rename after auth, not before ###
    for fname, fsz in recv_list:   os.replace(recv_dir+"/"+fname+".auth_req", recv_dir+"/"+fname)
    new_aset = ArchiveSet(arch_dir, dest, children=0, pass_agent=options.pass_agent)

    # Initial auth successful! Fetch + auth volume metadata...
    recv_list= [(x+"/volinfo", ArchiveSet.max_volinfosz)
                 for x in new_aset.conf["volumes"].keys()]
    fetch_file_blobs(recv_list, recv_dir, dest)
    do_exec([[CP.chattr, "+c"] + list(new_aset.conf["volumes"].keys())], cwd=arch_dir, check=False)
    new_aset = ArchiveSet(arch_dir, dest, children=1, allvols=True, prior_auth=new_aset)

    # Fetch + auth session metadata... use list flattener
    ses_list =[(x.name, x.volume.name) for sub in
               (vol.sessions.values() for vol in new_aset.vols.values()) for x in sub]
    fetch_file_blobs([(new_aset.vols[y].vid+"/"+x+"/info", ArchiveSet.max_infosz)
                                    for x,y in ses_list], recv_dir, dest)
    new_aset = ArchiveSet(arch_dir, dest, children=2, allvols=True, prior_auth=new_aset)

    fetch_file_blobs([(new_aset.vols[y].vid+"/"+x+"/manifest.z",
                       new_aset.vols[y].sessions[x].manifest_max()) for x,y in ses_list],
                        recv_dir, dest)
    for vol in new_aset.vols.values():   vol.decode_manifests(vol.sesnames, force=True)

    new_aset.just_fetched = True
    return new_aset


class Destination:

    url_types   = ("ssh", "file", "qubes-ssh", "qubes")

    ssh_opts    = ["-x", "-o", "ControlPath=~/.ssh/controlsocket-%r@%h-%p",
                         "-o", "ControlMaster=auto", "-o", "ControlPersist=60",
                         "-o", "ServerAliveInterval=30", "-o", "ConnectTimeout=30",
                         "-o", "Compression=no"]

    magic       = b"\xff\x11\x15"

    def __init__(self, dspec, dname, subdir):

        # fetch locations list, dest spec
        dest_url = dspec    ; locs = self.load_locations()
        if dname:
            if re.match(".*[,=\^]", dname) or any(map(str.isspace, dname)) \
            or not dname.isprintable():
                x_it(1,"Error: [^control], [space], and ',^=' not allowed in dest-name.")
            if not dest_url and dname in locs:
                dest_url = locs[dname]
        elif not dspec and "default" in locs and not options.metadir:  #### options
            dest_url = locs["default"]

        # parse and validate dest spec
        if not dest_url:                 x_it(1,"Error: Missing dest specification.")
        if not dest_url.isprintable():   x_it(1,"Error: [^control] not allowed in dest.")
        dparts      = urlparse(dest_url)    ; self.dtype = dtype = dparts.scheme
        if dtype not in self.url_types:
            x_it(1,"'%s' not an accepted type." % dtype)
        if (dtype == "file" and (dparts.netloc or not dparts.path)) \
        or (dtype in ("ssh","qubes","qubes-ssh") and not dparts.netloc) \
        or (dtype == "qubes-ssh" and not all(dparts.netloc.partition(":"))) :
            x_it(1,"Error: Malformed --dest specification.")

        self.spec   = dest_url
        self.sys    = dparts.netloc
        self.path   = os.path.normpath(dparts.path+"/"+subdir)
        self.cd     = " cd '"+self.path+"'"      ; self.archive_ini_hash = "none"
        self.free   = self.dtmp     =  None      ; self.dname  = dname
        self.online = self.writable =  False

        self.run_map = {"file":       [CP.sh],
                        "ssh":        [CP.ssh] + self.ssh_opts + [self.sys],
                        "qubes":      [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                      "-p", self.sys],
                        "qubes-ssh":  [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                      "-p", self.sys.split(":")[0]]
                        }

        # save locations change
        if dspec and dname:
            print(f"Naming this URL '{dname}'.")
            locs[dname] = dspec
            self.save_locations(locs)    ; del(locs)


    def remove_dtmp(self):
        if self.online:
            self.run([f"cd $(dirname {self.dtmp}) && rm -rf $(basename {self.dtmp})"],
                     timeout=10)

    def get_free(self, fpath): ## Enh: add sanitize
        for ln in open(fpath,"r"):
            if ln.startswith("wyng_check_free"):  self.free = int(ln.split()[1])

    def save_locations(self, locs):
        with open(vardir+"/dests", "w") as fl:
            for x, y in locs.items():   print(x, y, file=fl)

    def load_locations(self):
        if not exists(vardir+"/dests"):   return {}
        with open(vardir+"/dests", "r") as fl:
            loc = { x: y for x, y in (w.strip().split(" ",maxsplit=1) for w in fl) }

        return loc

    # Run system commands on destination

    def run(self, commands, direct=False, timeout=None,
            infile="", inlines=None, out="", check=True, trap=False):

        if direct:
            cmd = self.run_map[self.dtype] + commands
        else:
            cmd = self.run_args(commands, trap=trap)

        return do_exec([cmd], infile=infile, inlines=inlines, out=out,
                       check=check, timeout=timeout)

    # Build command lists that can be shunted to remote systems.
    # The input commands are stored in a temp file and a standard command that
    # runs the temp file is returned.

    def run_args(self, commands, trap=False, dest_type=None):

        dest_type = dest_type or self.dtype    ; tmpprefix = "/tmp/wyngrpc/"
        trapcmd   = "trap '' INT TERM QUIT ABRT ALRM TSTP USR1\n" if trap else ""
        # shunt commands to local tmp file
        with tempfile.NamedTemporaryFile(dir=tmpdir+"/rpc", delete=False) as tmpf:
            cmd = bytes(trapcmd + shell_prefix
                        + " ".join(commands) + "\n", encoding="UTF-8")
            tmpf.write(cmd)
            remotetmp = os.path.basename(tmpf.name)

        if dest_type in {"qubes","qubes-ssh"}:
            do_exec([[CP.qvm_run, "--no-color-stderr", "--no-color-output", "-p",
                      (self.sys if dest_type == "qubes" else self.sys.split(":")[0]),
                      (CP.mkdir+" -p "+tmpprefix+"; " if not self.dtmp else "")
                      +CP.cat+" >"+tmpprefix+remotetmp
                    ]], infile=pjoin(tmpdir,"rpc",remotetmp))
            if dest_type == "qubes":
                add_cmd = [CP.sh+" "+tmpprefix+remotetmp]
            else:
                add_cmd = [CP.ssh+" "+" ".join(self.ssh_opts)+" "+self.sys.split(":")[1]
                        +' "$('+CP.cat+' '+tmpprefix+remotetmp+')"']

        elif dest_type == "ssh":
            #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
            add_cmd = [cmd]

        elif dest_type == "file":
            add_cmd = [pjoin(tmpdir,"rpc",remotetmp)]

        return self.run_map[dest_type] + add_cmd


    def detect_state(self, dedup):

        if self.dtype == "qubes-ssh":
            # fix: possibly remove dargs and use dest.run()
            dargs = self.run_map["qubes"][:-1] + [self.sys.split(":")[0]]

            cmd = dargs + [shell_prefix + "mkdir -p /tmp/wyngrpc"]
            do_exec([cmd])

        tmpprefix = "/tmp/wyngrpc/"    ; tmpdigits = 12
        cmd  = [r"mkdir -p "+self.path+r" && "+self.cd

                # send helper program to remote dest
                +r"  && mkdir -p " +tmpprefix + r" && chmod 777 " + tmpprefix
                +r"  && tdir=$(mktemp -d " + tmpprefix + ("X"*tmpdigits) + r") && echo $tdir"
                +r"  && cat >$tdir/dest_helper.py"

                # check free space and archive.ini status on remote
                +r"  && echo -n 'wyng_check_free ' && stat -f -c '%a %S' ."
                +r"  && echo -n 'wyng_archive_ini '"
                +r"  && { if [ -e archive.ini ]; then sha256sum archive.ini; else echo none; fi }"

                # test write access and hardlinks
                +r"  && touch archive.dat && echo 'wyng_writable'"
                +(r" && ln -f archive.dat .hardlink" if dedup else "")
                ]
        try:
            online = \
                not do_exec([self.run_args(cmd),
                            # sanitize remote output:
                            [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]],
                            pipefail=True,
                            out=tmpdir+"/dest-state.log", infile=tmpdir+"/rpc/dest_helper.py")
        except SPr.CalledProcessError as e:
            online = False    ; err_out(repr(e))

        if online:
            for ln in open(tmpdir+"/dest-state.log","r"):
                if ln.startswith("wyng_archive_ini"):
                    self.archive_ini_hash = ln.split()[1]
                elif ln.startswith("wyng_check_free"):
                    parts = ln.split()    ; self.free = int(parts[1]) * int(parts[2])
                elif ln.startswith("wyng_writable"):
                    self.writable = True
                elif ln.startswith(tmpprefix):
                    self.dtmp = ln.strip()

            if not self.dtmp or len(self.dtmp) != len(tmpprefix)+tmpdigits \
            or not set(self.dtmp[5:]) <= set(string.ascii_letters + string.digits + "/"):
                raise ValueError("Missing or malformed tmp dir: "+repr(self.dtmp))
        else:
            for log in ("/dest-state.log", "/err.log"):
                if exists(tmpdir+log) and verbose:
                    do_exec([ [CP.cat,"-v",tmpdir+log],  [CP.tail,"--bytes=2000"],
                              [CP.tee, "--append", tmpdir+log+"-out" ] ])
                    err_out(open(tmpdir+log+"-out", "r").read())

        self.online = self.free is not None


    def write_helper_program(path):

        dest_program = r'''#  Copyright Christopher Laprise 2018-2023
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, time, signal, shutil, subprocess as SPr, gzip, tarfile

def fssync(path):
    if msync:   SPr.Popen(["sync","-f",path])

def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1"], iflag=False):
    for sval in (getattr(signal,"SIG"+x) for x in sel):
        signal.signal(sval, handle_signal) ; signal.siginterrupt(sval, iflag)

def handle_signal(sig, frame):
    if sig == signal.SIGALRM:   raise IOError("Timeout ALRM")

def helper_send():
    mkdirs = os.makedirs    ; hlink = os.link    ; dirname = os.path.dirname
    with tarfile.open(mode="r|", fileobj=sys.stdin.buffer) as tarf:
        extract = tarf.extract    ; substitutions = {}    ; dirlist = set()
        for member in tarf:   sdir = member.name    ; mkdirs(sdir)    ; print(sdir)    ; break
        for member in tarf:
            if not member.islnk():
                extract(member, set_attrs=False)
            else:
                source = src_orig = member.linkname   ; dest = member.name   ; ddir = dirname(dest)
                if source in substitutions:   source = substitutions[source]
                if ddir not in dirlist:   mkdirs(ddir, exist_ok=True)   ; dirlist.add(ddir)
                try:
                    hlink(source, dest)
                except OSError as err:
                    print(err)    ; print("Substitution:", source, dest)
                    shutil.copyfile(source, dest)    ; substitutions[src_orig] = dest

def helper_receive(lstf):
    stdout_write = sys.stdout.buffer.write   ; exists = os.path.exists   ; getsize= os.path.getsize
    stdout_flush = sys.stdout.buffer.flush   ; magicm = magic
    for line in lstf:
        fname = line.strip()
        if not fname:   break
        fsize = getsize(fname) if exists(fname) else 0
        stdout_write(magicm + fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:   stdout_write(dataf.read(fsize))
        stdout_flush()

def helper_merge():
    exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
    try:
        if resume:
            if exists("merge-init") or not exists("merge"):
                raise RuntimeError("Merge: Init could not complete; Aborting merge.")
        else:
            print("Merge: Initialization.")
            for f in (target+"/info", target+"/manifest.z", "volinfo", "archive.ini"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")    ; fssync(".")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):   replace(i.path, i.name)
        elif not exists("merge"):
            for f in (target+"/info.tmp", target+"/manifest.tmp", "volinfo.tmp", "merge.lst.gz"):
                if exists(f):   os.remove(f)
        fssync(".")    ; print(err)    ; sys.exit(50)
    try:
        os.chdir("merge")  #  CD
        if not resume or not exists("CHECK-mv-rm"):
            print("Merge: remove/replace files.")    ; subdirs = set()
            for src in src_list:  # Enh: replace os.scandir w manifest method
                for i in os.scandir(src):
                    if i.is_dir():   subdirs.add(i.name)
            for sdir in subdirs:   os.makedirs(merge_target+"/"+sdir, exist_ok=True)
            for line in lstf:
                ln = line.split() # default split() does strip()
                if ln[0] == "rename" and (not resume or exists(ln[1])):
                    replace(ln[1], ln[2])
                elif ln[0] == "-rm" and exists(ln[1]):
                    remove(ln[1])
            open("CHECK-mv-rm","w").close()
    except Exception as err:
        print(err)    ; sys.exit(60)

def helper_merge_finalize():
    try:
        print("Merge: Finalize target")
        os.chdir("merge")  #  CD                 ; open("CHECK-start-finalize","w").close()
        for f in ("/info", "/manifest.z"):
            if not resume or exists(target+f+".tmp"):  replace(target+f+".tmp", merge_target+f)
        os.chdir("..")     #  CD
        if not resume or not exists(target):         replace("merge/"+merge_target, target)
        if not resume or exists("volinfo.tmp"):      replace("volinfo.tmp", "volinfo")
        if not resume or exists("archive.ini.tmp"):  replace("archive.ini.tmp", "../archive.ini")
        if not exists(target):   raise FileNotFoundError(target)
        fssync(".")
    except Exception as err:
        print(err)    ; sys.exit(70)
    shutil.rmtree("merge", ignore_errors=True)
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

## MAIN ##
cmd = sys.argv[1]   ; msync = "--sync" in sys.argv   ; magic  = b"\xff\x11\x15"
tmpdir = os.path.dirname(os.path.abspath(sys.argv[0]))  
exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
if "--finalize" in sys.argv:   catch_signals()
if cmd in ("receive","dedup") and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
else:
    lstf = None

if cmd == "merge":
    src_list = []    ; resume = "--resume" in sys.argv    ; mtpath = sys.argv[2]
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    if resume and not (exists("merge.lst.gz") and exists("merge")):
        print("Error: Remote dir not initialized.")
        sys.exit(50 if exists(mtpath) else 40)
    lstf = gzip.open("merge.lst.gz", "rt")
    merge_target, target = lstf.readline().split()
    while True:
        ln = lstf.readline().strip()
        if ln == "###":  break
        src_list.append(ln)
    na = helper_merge_finalize() if "--finalize" in sys.argv else helper_merge()
elif cmd == "receive":
    helper_receive(lstf if lstf else sys.stdin)
elif cmd == "send":
    helper_send()
elif cmd == "dedup":
    ddcount = 0    ; substitutions = {}
    for line in lstf:
        source, dest = line.split()   ; src_orig = source    ; deststat = os.stat(dest)
        ddcount += deststat.st_size
        if source in substitutions:   source = substitutions[source]
        if os.stat(source).st_ino != deststat.st_ino:
            try:
                os.link(source, dest+"-lnk")    ; replace(dest+"-lnk", dest)
            except OSError as err:
                if err.errno == 31:
                    # source has too many links; substitute
                    substitutions[src_orig] = dest   ; ddcount -= deststat.st_size   ; continue
                else:
                    if exists(dest+"-lnk"):   remove(dest+"-lnk")
                    print(err)   ; raise err
    print(ddcount, "bytes reduced.")
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)
'''
        with open(path+"/dest_helper.py", "wb") as progf:
            progf.write(bytes(dest_program, encoding="UTF-8"))

    #####>  End dest_helper program  <#####


# Run system commands with pipes, without shell:
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'. 'inlines' can be a list-like collection of strings
# to be used as input instead of 'infile'.
# List of commands may include 'None' instead of a child list; these will be ignored.

def do_exec(commands, cwd=None, check=True, out="", infile="", inlines=[], text=False,
            pipefail=False, timeout=None):
    ftype   = "t" if text else "b"
    if issubclass(type(out), io.IOBase):
        outf = out
    else:
        outmode = "a" if out.startswith(">>") else "w"    ; out = out.lstrip(">>")
        if cwd and out and out[0] != "/":   out = pjoin(cwd,out)
        outfunc = gzip.open if out.endswith(".gz") else open
        outf    = outfunc(out, outmode+ftype) if out else SPr.DEVNULL

    if inlines:
        inf = SPr.PIPE  #io.StringIO("\n".join(inlines)+"\n")
    else:
        if cwd and infile and infile[0] != "/":   infile = pjoin(cwd,infile)
        infunc  = gzip.open if infile.endswith(".gz") else open
        inf     = infunc(infile, "r"+ftype) if infile else SPr.DEVNULL

    errf = open(tmpdir+"/err.log", "a")  ; print("--+--", file=errf)
    commands = [x for x in commands if x is not None]

    # Start each command, linking them via pipes
    procs = []    ; start_t = time.time()
    for i, clist in enumerate(commands):
        p = SPr.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else SPr.PIPE,
                             stderr=errf)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)

    try:
        if inlines:   procs[0].communicate(("\n".join(inlines)+"\n").encode("UTF-8"),
                                            timeout=timeout)
    except SPr.TimeoutExpired:
        pass

    # Monitor and control processes

    while True:
        err = None    ; finish = to_flag = False
        for p1 in reversed(procs):
            retcode = p1.poll()
            if not finish and retcode is None:
                try:
                    p1.communicate(timeout=2)
                except SPr.TimeoutExpired:
                    to_flag = True
                    continue
                retcode = p1.returncode   ; finish = True
                if check and (retcode != 0):
                    err = p1              ; finish = True
            elif finish and retcode is None:
                p1.terminate()
                continue

        if err or not to_flag or (timeout and time.time() - start_t > timeout):
            break

    for f in [inf, outf, errf]:
        if type(f) is not int: f.close()
    if err and check:
        raise SPr.CalledProcessError(err.returncode, err.args)

    rclist = [0] + list(filter(bool if pipefail else str, ( x.returncode for x in procs )))
    return rclist[-1]


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Session objects.
# Returns False if local and dest hashes match.

def compare_files(arch, pathlist=[], volumes=[], sessions=[], clear=True, manifest=False):
    dest = arch.dest    ; cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions) and not x.meta_checked]
    realses   = [x for x in sessions if not x.meta_checked]
    if len(volumes)+len(sessions)+len(pathlist) == 0:   return False

    with open(cmp_list, "a") as flist:
        for pth in pathlist:
            #if not exists(pth):   raise FileNotFoundError(pth)
            print(pth, file=flist)
        for v in realvols:
            v.meta_checked = True    ; print(v.vid+"/volinfo", file=flist)
        for s in realses:
            s.meta_checked = True
            for sf in ["info"] + ["manifest.z"] if manifest else []:
                print(pjoin(s.volume.vid,s.name,sf), file=flist)

    do_exec([[CP.xargs, CP.sha256sum]], cwd=arch.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest.run([dest.cd + " && xargs sha256sum"],
             infile=cmp_list, out=tmpdir+"/compare-hashes.dest")
    # maybe switch cmp to diff/sha256sum if they are safe enough to read untrusted input
    files  = [tmpdir+"/compare-hashes.local", tmpdir+"/compare-hashes.dest"]
    result = do_exec([[CP.cmp] + files], check=False) > 0

    return result


## Enh: Create aset functions to supply meta filenames for objects

def update_dest(arch, pathlist=[], volumes=[], sessions=[], ext="", delete=False):
    lcd = arch.path    ; dest = arch.dest

    update_list = [x.volume.vid+"/"+x.name+"/manifest.z" for x in sessions] \
                + [x.volume.vid+"/"+x.name+"/info" for x in sessions] \
                + [x.vid+"/volinfo" for x in volumes] + pathlist

    if delete:
        assert not ext;  dest.run([dest.cd + " && xargs rm -f"], inlines=update_list)
        return

    do_exec([[CP.tar,"-cf","-","--no-recursion","--verbatim-files-from","--files-from", "-"],
             dest.run_args([dest.cd + "  && tar -xf -"]
                                + [" && mv '"+x+ext+"' '"+x+"'" for x in update_list if ext])
            ], inlines=[x+ext for x in update_list], cwd=lcd)


def fetch_file_blobs(recv_list, recv_dir, dest, ext="", skip0=False, verifier=None):

    cmd = dest.run_args(
            [dest.cd
             +" && exec 2>>"+dest.dtmp+"/receive.log"
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ])
    recvp = SPr.Popen(cmd, stdout=SPr.PIPE, stdin=SPr.PIPE)
    recvp.stdin.write(("".join((x+"\n" for x,y in recv_list))).encode("UTF-8"))
    recvp.stdin.flush()    ; rc = recvp.poll()
    magic = dest.magic

    for fname, fsz in list(recv_list):
        if rc is not None:   raise RuntimeError("Process terminated early.")
        fpath = recv_dir+"/"+fname+ext
        if exists(fpath):   os.remove(fpath)
        assert recvp.stdout.read(3) == magic
        # Read chunk size
        untrusted_size = int.from_bytes(recvp.stdout.read(4),"big")
        if untrusted_size == 0 and skip0:   continue
        if not fsz >= untrusted_size > 0:
            raise BufferError("Bad file size "+str(untrusted_size))

        # Size is OK.
        size = untrusted_size
        # Read chunk buffer
        untrusted_buf = recvp.stdout.read(size)    ; rc  = recvp.poll()
        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))
        if verifier:   verifier.auth(untrusted_buf)

        os.makedirs(os.path.dirname(fpath), exist_ok=True)
        with open(fpath, "wb") as outf:   outf.write(untrusted_buf)


# Prepare snapshots and check consistency with metadata:
# Normal use will have a snap1 snapshot of the volume already in place.  Here we
# create a fresh snap2 so 'update_delta_digest' can compare it to the older snap1
# and then rotate snap2 -> snap1.

def prepare_snapshots_lvm(storage, aset, datavols, monitor_only):

    print("Preparing snapshots...")
    incr_vols, complete_vols = [], []   ; vgname = aset.vgname      ; lvols = storage.lvols

    for datavol in datavols:

        l_vol = storage.new_vol_entry(datavol)
        if not l_vol.exists():
            print("Warning: Local '%s' does not exist!" % datavol)
            continue

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, options.voldesc)

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        vol = aset.vols[datavol]        ; mapfile  = vol.mapfile
        snap1, snap2 = l_vol.snap1, l_vol.snap2
        if lvols[snap1].is_arch_member() == "false":
            if options.remap:
                l_vol.delete    ; print("  Removed mis-matched snapshot", snap1)
            else:
                print("  Skipping %s; snapshot is from a different archive." % datavol)
                continue

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing snapshot UUIDs; a match means remap is unnecessary.
        if len(vol.sessions):
            s1tags = lvols[snap1].tags.split(",")

            if not lvols[snap1].exists() or not exists(mapfile)    \
            or os.path.getmtime(mapfile) != lvols[snap1].gettime() \
            or vol.last not in s1tags:

                # Handle inadvertant mapfile snapshot mis-match
                vol.init_deltamap(mapfile, vol.mapsize())    ; lvols[snap1].delete()
                if debug:   print("  Removed mis-matched delta for", datavol)

            if not lvols[snap1].exists() and lvols[snap2].exists() \
            and vol.last in lvols[snap2].tags.split(",") and exists(mapfile)  \
            and os.path.getmtime(mapfile) == lvols[snap2].gettime():
                lvols[snap2].rename(snap1)
                if debug:  print("Recovered interrupted snapshot rotation:", snap2)
 
            #if lvols[snap1].exists() and "delta" in lvols[snap1].tags \
            #and (     vol.map_used() == 0 
                  #or (vol.map_used() > 0 and
                      #"delta-"+str(int(os.path.getmtime(mapfile))) not in lvols[snap1].tags
                      #)
                 #):
                ## Handle inadvertant mapfile snapshot mis-match
                #vol.init_deltamap(mapfile, vol.mapsize())
                #lvols[snap1].delete()    ; print("  Removed mis-matched delta", snap1vol)

            if not exists(mapfile) and lvols[snap1].exists() \
            and vol.last in s1tags \
            and "delta" not in lvols[snap1].tags \
            and os.path.getmtime(mapfile) == lvols[snap1].gettime():
                # Latest session matches current snapshot; OK to make blank map.
                vol.init_deltamap(mapfile, vol.mapsize())

        elif monitor_only:
            print("  Skipping %s; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            incr_vols.append(datavol)
        elif not monitor_only:
            print("  Pairing snapshot for", datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping %s; No paired snapshot." % datavol)    ; continue

        # Make fresh snap2vol
        lvols[snap2].delete()
        tagopts = ["--addtag=wyng", "--addtag=arch-"+aset.uuid]
        if monitor_only:   tagopts.append("--addtag=delta")
        lvols[snap2].create(snapshotfrom=datavol, ro=True, addtags=tagopts)

    return incr_vols, complete_vols


def prepare_snapshots_reflink(storage, aset, datavols, monitor_only):

    print("Preparing snapshots...")
    incr_vols, complete_vols = [], []   ; vgname = aset.vgname      ; lvols = storage.lvols

    for datavol in datavols:

        l_vol = storage.new_vol_entry(datavol)
        if not l_vol.exists():
            print("Warning: Local '%s' does not exist!" % datavol)
            continue

        if datavol not in aset.vols and not monitor_only:
            add_volume(aset, datavol, options.voldesc)

        # 'mapfile' is the deltamap file, snap1 holds vol state between send/monitor ops
        vol = aset.vols[datavol]        ; mapfile  = vol.mapfile
        snap1, snap2 = l_vol.snap1, l_vol.snap2

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing snapshot UUIDs; a match means remap is unnecessary.
        if len(vol.sessions):

            if not lvols[snap1].exists() or not exists(mapfile) \
            or os.path.getmtime(mapfile) != lvols[snap1].gettime():

                # Handle inadvertant mapfile snapshot mis-match
                vol.init_deltamap(mapfile, vol.mapsize())
                lvols[snap1].delete()    ; print("  Removed mis-matched delta", snap1)

            if not lvols[snap1].exists() and lvols[snap2].exists() \
            and os.path.getmtime(mapfile) != lvols[snap2].gettime():
                if debug:  print("Recovered interrupted snapshot rotation:", snap2)
                lvols[snap2].rename(snap1)

        elif monitor_only:
            print("  Skipping %s; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but snap1 and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if vol.sessions and exists(mapfile) and lvols[snap1].exists():
            incr_vols.append(datavol)
        elif not monitor_only:
            print("  Pairing snapshot for", datavol)
            complete_vols.append(datavol)
        else:
            print("  Skipping %s; No paired snapshot." % datavol)    ; continue

        # Make fresh snap2vol
        storage.new_vol_entry(datavol)
        lvols[snap2].delete()
        lvols[snap2].create(snapshotfrom=datavol, ro=True)

    return incr_vols, complete_vols


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(storage, aset, datavols):

    #Enh: construct 'poolset' as vg,pool pairs; change vgname handling to track 'apool'
    vgname   = os.path.basename(storage.path).replace("-","--")    ; lvols = storage.lvols
    poolset  = set(lvols[x].pool_lv for x in datavols)
    assert aset.chunksize % storage.block_size == 0

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    catch_signals()
    for apool in sorted(poolset):
        poolname = apool.replace("-","--")
        if not lvols[apool].exists():
            raise RuntimeError("LV pool '%s/%s' does not exist." % (vgname, apool))

        try:
            storage.metadata_lock(apool)
            for datavol in datavols:
                lv = lvols[datavol]
                if lv.pool_lv != apool:   continue
                cmds = [[CP.thin_delta, "-m",   "--thin1=" + lvols[lv.snap1].thin_id,
                                                "--thin2=" + lvols[lv.snap2].thin_id,
                                        "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", r"^\s*<same .*\/>$"]
                        ]
                do_exec(cmds,  out=tmpdir+"/delta."+datavol)
        except Exception as e:
            err_out("ERROR running thin_delta process.")
            raise e
        finally:
            storage.metadata_unlock(apool)

    catch_signals(None)


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest_lvm(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = lvols[vol.name].snap1      ; snap2vol   = lvols[vol.name].snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    assert len(vol.sessions) and exists(vol.mapfile)
    assert storage.block_size == 512

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(tmpdir+"/delta."+datavol).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    dnewchunks  = isnew  = anynew = dfreedblocks = 0

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile, "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // storage.block_size
        chunkblocks  = chunksize // storage.block_size
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)
        tally=0 ####

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                isnew = anynew = 1
                if delta.tag == "right_only":   tally -= blocklen
            elif delta.tag == "left_only":
                isnew = 0    ; dfreedblocks += blockend - blockbegin
                tally += blocklen
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm)
        if dnewchunks+dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())

    catch_signals()
    map_updated = dnewchunks+dfreedblocks+anynew+vol.map_used() > 0
    if monitor_only:
        t1 = lvols[snap2vol].gettime()
        do_exec([[CP.lvm,"lvchange","--addtag="+vol.last, "--addtag=delta-"+str(t1),
                  lvols[snap2vol].path]])
        t2 = lvols[datavol].rotate_snapshots(rotate=map_updated, timestamp_path=vol.mapfile)
        assert t1 == t2
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if map_updated else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    print("***", tally, dnewchunks*chunksize, dfreedblocks) ####
    catch_signals(None)
    return map_updated


def get_reflink_deltas(storage, aset, datavols):

    assert aset.chunksize % storage.block_size == 0
    blksz = str(storage.block_size)
    generation, stpath = storage.metadata_lock()    ; lvols = storage.lvols    ; procs=[]

    for vol in (aset.vols[x] for x in datavols):
        locvol = lvols[vol.name]
        deltapath = tmpdir+"/delta."+vol.vid    ; cmds, devs = [], set()

        for side, snapname in (("11", locvol.snap1), ("22", locvol.snap2)):
            if not exists(stpath+snapname):
                raise RuntimeError("Volume path '%s' does not exist." % fpath)
            # filefrag output: file extent ranges, their 'physical' block ranges, extent length
            # sort -m will merge this output from two vols (file extent #s are pre-sorted)
            # uniq -u will filter-out exact extent/phys/len matches between the two vols
            # The result will be a list of extent ranges that have changed.

            devs.add(os.stat(stpath+snapname).st_dev)
            cmds.append([
              [CP.env, "LC_ALL=''", CP.filefrag, "-vs", "-b"+blksz, stpath+snapname],
              # awk checks the expected filefrag format and removes extra cols & punctuation.
              [CP.awk, r'BEGIN {PAT="^\\s*ext:\\s+logical_offset:\\s+physical_offset:\\s+length:"}'
                       r' NR==1, $0 ~ PAT {HEADER=$0; next}'
                       r' END {if (!(HEADER ~ PAT && $0 ~ /extents? found$/)) exit 2}'
                       r' /delalloc/ {next}'
                       r' /^\s*[0-9]+:/ {gsub(/\.|:/, " ", $0);'
                                        r' print '+side+', $2, $6, $3, $4, $5}'
               ]
            ])
        if debug:   print(devs, "\n", cmds)
        assert len(devs) == 1

        # Save output from first volume to a file
        do_exec(cmds[0], out=deltapath+"11")
        #Enh: maybe use dev+inode as fname

        # Pipe output from second volume to 'sort' and include first file
        do_exec(cmds[1] +
                [ [CP.sort, "-m", "-k2n,2", deltapath+"11", "-"],
                  [CP.uniq, "-u", "-f1"],
                  [CP.gzip, "-2"]
                 ],
                    out=deltapath
                )
        if not debug:   os.remove(deltapath+"11")

    # Check here that Btrfs snapshot gen#s did not change.
    # If changed: retry, x_it(7), use remap mode or pause/cancel the Btrfs op.
    # This could be extended with a fallback mode that checks _get_btrfs_generation() directly
    # while doing one snap pair at a time to increase chances of success.
    if storage.metadata_unlock()[0] != generation:
        x_it(7, "fs gen ids differ.")


def update_delta_digest_reflink(storage, aset, datavol, monitor_only):

    lvols       = storage.lvols              ; l_vol      = lvols[datavol]
    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = l_vol.snap1                ; snap2vol   = l_vol.snap2
    snap1size   = lvols[snap1vol].getsize()  ; snap2size  = lvols[snap2vol].getsize()
    assert len(vol.sessions) and exists(vol.mapfile)

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile, "r+b") as bmapf, gzip.open(tmpdir+"/delta."+vol.vid, "r") as deltaf:
        snap_ceiling = max(snap1size, snap2size) // storage.block_size
        chunkblocks  = chunksize // storage.block_size
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        if bmap_size != os.fstat(bmapf.fileno()).st_size:
            bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)
        dnewchunks   = isnew  = anynew = dfreedblocks = highwater = tally = 0

        # Cycle through the merged 'filefrag' metadata, marking bits in bmap_mm as needed.
        # Entries carry a beginning block position and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for dln in deltaf:
            side, blockbegin, blocklen = map(int, dln.split()[:3])

            tally += blocklen if side == 11 else -blocklen
            blockend     = min(blockbegin + blocklen, snap_ceiling)
            if blockend <= highwater:   continue
            highwater    = blockend

            #if delta.tag in ("different", "right_only"):
            isnew = anynew = 1 #### FIX: reconcile with tally

            # Fix: compare '11' output with final output to find freed blocks

            #elif delta.tag == "left_only":
            #    isnew = 0    ; dfreedblocks += blockend - blockbegin
            #else: # superfluous tag
            #    continue

            # blockpos iterates over disk blocks.
            # block_size (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm)
        if dnewchunks+dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())

    catch_signals()
    map_updated = dnewchunks+dfreedblocks+anynew+vol.map_used() > 0
    if monitor_only:
        t = lvols[datavol].rotate_snapshots(rotate=map_updated, timestamp_path=vol.mapfile)
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if map_updated else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    catch_signals(None)
    return map_updated


# Reads addresses from manifest and marks corresponding chunks in a volume's deltamap.

def manifest_to_deltamap(volume, manifest, mapsize):
    aset = volume.archive
    with open(manifest, "r") as mf, \
         open(volume.mapfile, "r+b") as bmapf:

        bmapf.truncate(mapsize)    ; bmapf.flush()
        bmap_mm = mmap.mmap(bmapf.fileno(), 0)       ; chunksize  = aset.chunksize
        for ln in mf:
            addr = int(ln.split()[1][1:], 16)        ; volsegment = addr // chunksize
            bmap_pos = volsegment // 8               ; bmap_mm[bmap_pos] |= 1 << (volsegment % 8)


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a blank deltamap and .tick snapshot to
# track changes. After an incremental send, snapshots are rotated and the deltamap is reset.
#
# Returns (int, int) representing an estimate of bytes sent and chunks freed.
# Bytes sent will always be >0 if nonzero chunks were added to the manifest.
# A result of (0, 0) means no change was detected or sent.

def send_volume(storage, vol, localtime, ses_tags, send_all):

    aset        = vol.archive                   ; bkdir       = aset.bkdir
    datavol     = vol.name                      ; dedup       = options.dedup
    snap2vol    = storage.lvols[storage.lvols[vol.name].snap2]
    snap2size   = snap2vol.getsize()
    bmap_size   = vol.mapsize(snap2size)        ; chunksize   = aset.chunksize
    chdigits    = max_address.bit_length()//4   ; chformat    = "%0"+str(chdigits)+"x"
    bksession   = "S_"+localtime                ; sdir        = pjoin(vol.vid, bksession)
    prior_size  = vol.volsize()                 ; prior_ses   = vol.last
    verbose     = not options.quiet             ; zeros       = bytes(chunksize)
    addrsplit   = -address_split[1]             ; lchunk_addr = vol.last_chunk_addr(snap2size)
    compress = compressors[aset.compression][2] ; compresslevel = int(aset.compr_level)

    if len(vol.sessions):
        # Our chunks are usually smaller than LVM's, so generate a full manifest to detect
        # significant amount of unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(vol), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None
    fman_hash     = fman_fname = ""

    ses = vol.new_session(bksession, addtags=ses_tags)
    ses.localtime = localtime
    ses.volsize   = snap2size
    ses.path      = vol.path+"/"+bksession+"-tmp"

    # Code from init_dedup_indexN() localized here for efficiency.
    dedup_idx     = dedup_db = None
    if dedup:
        hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w \
                  = aset.dedupindex
        chtree_max= 2**(chtree[0].itemsize*8)
        idxcount  = dataf.seek(0,2) // (ch_w+ses_w)    ; ddblank_ch = bytes(hash_w)
        ddsessions = aset.dedupsessions                ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(metadir+bkdir)    ; os.makedirs(sdir+"-tmp")
    do_exec([[CP.chattr, "+c", sdir+"-tmp"]], check=False)


    if aset.datacrypto:
        crypto  = True
        encrypt = aset.datacrypto.encrypt
    else:
        crypto  = False
        etag    = b''


    # Use tar to stream files to destination
    stream_started = False
    untar_cmd = [dest.cd + " && mkdir -p ./"+sdir
                 + " && exec >>"+dest.dtmp+"/send.log 2>&1"
                 + " && echo "+vol.vid
                 + " && python3 "+dest.dtmp+"/dest_helper.py send"
                 + (" --sync" if options.maxsync else "")]

    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(snap2vol.path,"rb", buffering=chunksize) as vf,    \
         open(sdir+"-tmp/manifest.tmp", "wt") as hashf,            \
         open("/dev/zero" if send_all else vol.mapfile,"rb") as bmapf:

        vf_seek = vf.seek; vf_read = vf.read   ; BytesIO = io.BytesIO
        gethash = hash_funcs[aset.hashtype]    ; b64enc  = base64.urlsafe_b64encode
        b2int   = int.from_bytes               ; islice  = itertools.islice
        compare_digest = hmac.compare_digest


        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        zdelim  = bytes(64)    ; zdlen = len(zdelim)*8      ; minibmap = None   ; bmap_list = []
        addr    = counter = percent = bcount = ddbytes = 0  ; checkpt = 128     ; bmsz = zdlen*50

        while addr < snap2size:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  list(islice(fullmanifest, zdlen))
                addr += chunksize*zdlen
                minibmap = bmap_list.pop(0)
            elif not send_all:
                # Get more: split(zdelim) shows where large unmodified zones exist.
                bmap_list.extend(bmapf.read(bmsz).split(zdelim))
                minibmap = bmap_list.pop(0)

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, snap2size if send_all
                        else min(snap2size, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = "x"+chformat % addr

                if fullmanifest:
                    try:
                        fman_hash, fman_fname = fullmanifest_readline().split()
                    except ValueError: # EOF
                        fullmanifest = None    ; fman_hash = ""
                    else:
                        if fman_fname != destfile:
                            raise ValueError("expected manifest addr %s, got %s"
                                            % (destfile, fman_fname))

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                # Fetch chunk as buf
                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Process checkpoint
                if counter > checkpt:
                    aset.save_conf()    ; tarf.add(aset.confname)
                    # Show progress.
                    if verbose:
                        percent = int(addr/snap2size*1000)
                        print("\x0d  %4.1f%% %7dM" % (percent/10, bcount//1000000),
                              end="", flush=True)
                    counter = 0

                # Compress & write only non-empty chunks
                if buf == zeros:
                    if fman_hash == "0":   continue
                    b64hash = "0"
                else:
                    # Compress chunk and hash it
                    buf    = compress(buf, compresslevel)
                    bhash  = gethash(buf)   ; b64hash = b64enc(bhash.digest()).decode("ascii")

                    # Skip when current and prior chunks are the same
                    if compare_digest(fman_hash, b64hash):  continue

                # Start tar stream
                if not stream_started:
                    untar = SPr.Popen(dest.run_args(untar_cmd),
                            stdin =SPr.PIPE,    stdout=SPr.DEVNULL,
                            stderr=SPr.DEVNULL)
                    tarf = tarfile.open(mode="w|", fileobj=untar.stdin)
                    tarf_addfile = tarf.addfile        ; TarInfo = tarfile.TarInfo
                    stream_started = True              ; LNKTYPE = tarfile.LNKTYPE
                    tar_info = TarInfo(sdir+"-tmp")    ; tar_info.type = tarfile.DIRTYPE
                    tarf_addfile(tarinfo=tar_info)

                # Add entry to new manifest
                print(b64hash, destfile, file=hashf)
                if b64hash == "0":   continue

                # Add buffer to stream
                tar_info = TarInfo("%s-tmp/%s/%s" % (sdir, destfile[1:addrsplit], destfile))

                # If chunk already in archive, link to it
                if dedup:
                    bhashb = bhash.digest()
                    i      = b2int(bhashb[:ht_ksize], "big")
                    pos    = hashtree[i].find(bhashb)     ; ddses = None
                    if pos % hash_w == 0:
                        data_i = chtree[i][pos//hash_w]   ; dataf.seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[b2int(dataf.read(ses_w),"big")]
                        if ddses is None:
                            hashtree[i][pos:pos+hash_w] = ddblank_ch # zero-out obsolete entry
                        else:
                            ddchx = dataf.read(ch_w).hex().zfill(chdigits)
                            tar_info.type = LNKTYPE
                        dataf.seek(0,2)
                    if ddses is None and bhashb != ddblank_ch and idxcount < chtree_max:
                        hashtree[i].extend(bhashb)   ; chtree[i].append(idxcount)   ; idxcount += 1
                        dataf.write(ses_index.to_bytes(ses_w,"big") + addr.to_bytes(ch_w,"big"))

                if tar_info.type == LNKTYPE:
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.vid,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)

                else:
                    # Encrypt the data chunk
                    if crypto:
                        etag, buf = encrypt(buf)

                    # Send data chunk to the archive
                    fileobj = BytesIO()
                    tar_info.size = fileobj.write(etag) + fileobj.write(buf)   ; fileobj.seek(0)
                    tarf_addfile(tarinfo=tar_info, fileobj=fileobj)
                    bcount += len(buf)    ; counter += 1

            # Advance addr, except when minibmap is zero len.
            if minibmap or send_all:  addr += chunksize

    print("\r  100% ", ("%8.1fM  |  %s" % (bcount/1000000, datavol)),
          ("\n  (reduced %0.1fM)" % (ddbytes/1000000)) if ddbytes and options.verbose else "",
          end="")

    # Send session info, end stream and cleanup
    if fullmanifest:   fullmanifest.close()
    if stream_started:
        # Save session info
        if crypto:   aset.datacrypto.save_counter()
        ses.save_info(ext=".tmp")
        for f in ("manifest.z","info"):
            fpath = sdir+"-tmp/"+f         ; tarf.add(fpath+".tmp", arcname=fpath)
        tarf.add(vol.vid+"/volinfo.tmp")   ; tarf.add(aset.confname+".tmp")

        tarf.close()    ; untar.stdin.close()
        try:
            untar.wait(timeout=60)
        except SPr.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 99
            untar.kill()
        else:
            retcode = untar.poll()

        if retcode != 0:
            raise RuntimeError("tar transport failure code %d" % retcode)

        if ses.volsize != prior_size and len(vol.sessions) > 1:
            os.link(ses.path+"/manifest.tmp", ses.path+"/manifest")
            check_manifest_sequence(vol, vol.sesnames)

        # Finalize on VM/remote
        catch_signals()
        dest.run([ dest.cd
                 +" && mv -T "+sdir+"-tmp "+sdir
                 +" && mv "+vol.vid+"/volinfo.tmp "+vol.vid+"/volinfo"
                 +" && mv archive.ini.tmp archive.ini"
                 +(" && ( nohup sync -f . 2&>/dev/null & )" if options.maxsync else "")
                 ], trap=True)

        # Local finalize
        for f in ("manifest.z","manifest","info"):
            os.replace(ses.path+"/"+f+".tmp", ses.path+"/"+f)
        os.replace(vol.path+"/volinfo.tmp", vol.path+"/volinfo")
        os.replace(vol.path+"/vi.dat.tmp", vol.path+"/vi.dat")
        os.replace(aset.path+"/archive.ini.tmp", aset.path+"/archive.ini")
        ses.path = ses.path.rsplit("-tmp",maxsplit=1)[0]   ; os.replace(ses.path+"-tmp", ses.path)
        fssync(vol.path)

    else:
        catch_signals()
        vol.delete_session(bksession)    ; shutil.rmtree(aset.path+"/"+sdir+"-tmp")

    vol.init_deltamap(vol.mapfile, vol.mapsize())
    storage.lvols[vol.name].rotate_snapshots(rotate=True, timestamp_path=vol.mapfile)
    catch_signals(None)
    if dedup and debug:   show_mem_stats()

    return stream_started + bcount


# Build deduplication hash index and list

def init_dedup_index(aset, listfile=""):

    ctime      = time.perf_counter()    ; makelist = bool(listfile)
    addrsplit  = -address_split[1]
    # Define arrays and element widths
    hash_w     = hash_bits // 8
    ht_ksize   = 2 # binary digits for tree key
    hashtree   = [bytearray() for x in range(2**(ht_ksize*8))]
    chtree     = [array("I") for x in range(2**(ht_ksize*8))]
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2

    # Create master session list, limit to ses_w range
    for vol in aset.vols.values():   aset.dedupsessions += vol.sessions.values()
    aset.dedupsessions.sort(key=lambda x: x.localtime, reverse=True)
    ddsessions = aset.dedupsessions[:2**(ses_w*8)-(len(aset.vols))-1]

    dataf  = open(tmpdir+"/hashindex.dat","w+b")
    dataf_read  = dataf.read    ; dataf_seek = dataf.seek      ; int_frbytes = int.from_bytes
    dataf_write = dataf.write   ; bfromhex = bytes().fromhex   ; b64dec = base64.urlsafe_b64decode
    if makelist:   dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = match = 0
    for sesnum, ses in enumerate(ddsessions):
        vol = ses.volume    ; vid = vol.vid    ; sesname = ses.name
        vol.decode_one_manifest(ses)
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":   continue
                bhashb = b64dec(ln1)
                i      = int_frbytes(bhashb[:ht_ksize], "big")
                pos    = hashtree[i].find(bhashb)
                if pos % hash_w == 0:
                    match += 1
                    if makelist:
                        data_i = chtree[i][pos//hash_w]
                        dataf_seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[int_frbytes(
                                 dataf_read(ses_w),"big")]
                        ddchx  = dataf_read(ch_w).hex().zfill(chdigits)
                        print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                            (ddses.volume.vid, ddses.name, ddchx[:addrsplit], ddchx,
                            vid, sesname, ln2[1:addrsplit], ln2),
                            file=dedupf)
                        dataf_seek(0,2)
                elif count < chtree_max:
                    hashtree[i].extend(bhashb)
                    chtree[i].append(count)
                    dataf_write(sesnum.to_bytes(ses_w,"big"))
                    dataf_write(bfromhex(ln2[1:]))  # Enh: scale no. digits to match vol size
                    count += 1

    if listfile:   dedupf.close()

    aset.dedupindex    = (hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w)
    aset.dedupsessions = ddsessions

    if not debug:  return
    print("\nIndexed in %.1f seconds." % (time.perf_counter()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d, matches: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count, match)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


# Deduplicate data already in archive

def dedup_existing(aset):

    print("Building deduplication index...", end="")
    init_dedup_index(aset, "dedup.lst.gz")
    dest = aset.dest

    print(" linking...", end="", flush=True)
    do_exec( [dest.run_args([dest.cd
               +" && /bin/cat >"+dest.dtmp+"/dest.lst.gz"
               +" && /usr/bin/python3 "+dest.dtmp+"/dest_helper.py dedup"
               ]),
              [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]
            ], infile=tmpdir+"/dedup.lst.gz", out=tmpdir+"/arch-dedup.log")
    print(" done.")
    if options.verbose:   print("".join(open(tmpdir+"/arch-dedup.log","r")))


# Controls flow of monitor and send_volume procedures:

def monitor_send(storage, aset, datavols, monitor_only):

    if options.autoprune.lower() == "full":
        for vol in aset.vols:   autoprune(vol, apmode="full")

    #for prg in (CP.lvm, CP.dmsetup, CP.thin_delta ):
        #if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)
    #try:
        #p = SPr.check_output([CP.thin_delta, "-V"])
    #except:
        #p = b""
    #ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
    #if p and ver < target_ver:
        #raise RuntimeError("Thin provisioning tools version >= "+target_ver+" required.")

    dest = aset.dest    ; localtime = time.strftime("%Y%m%d-%H%M%S")

    incrementals, send_alls \
        = storage.prep_snapshots(storage, aset, datavols, monitor_only)
    storage.update_vol_list(aset.vols.keys())

    if monitor_only:   send_alls.clear()

    if len(incrementals)+len(send_alls) == 0:
        x_it(0, "No new data.")

    # Process session tags
    ses_tags = []
    if options.tag and options.tag == [""] and not options.unattended and not monitor_only:
        print("Enter tag info as 'tagID[, tag description]'. Blank to end input.")
        while ans := ask_input("[%d]: " % (len(ses_tags)+1)).strip():
            tag = ArchiveSession.tag_parse(ans, delim=",")
            if not tag:   continue
            ses_tags.append(tag)
            if len(ses_tags) == ArchiveSet.max_tags:   break
        print(len(ses_tags), "tags total.")
    elif options.tag and not monitor_only:
        for tag_opt in options.tag:
            tag = ArchiveSession.tag_parse(tag_opt, delim=",")
            if not tag:   raise ValueError("Invalid tag "+tag_opt)
            ses_tags.append(tag)

    if len(incrementals) > 0:
        if verbose:   print("Acquiring deltas.")
        storage.acquire_deltas(storage, aset, incrementals)

    if not monitor_only:
        cmpvols = [x for x in aset.vols.values()
                         if x.name in incrementals + send_alls and x.sessions]
        cmpses  = [v.sessions[v.sesnames[-1]] for v in cmpvols]
        if compare_files(aset, volumes=cmpvols, sessions=cmpses):
            x_it(1, "Error: Local and archive metadata differ.")

        print("\nSending backup session %s to '%s'." % (localtime, dest.spec))

    for datavol in incrementals + send_alls:
        vol = aset.vols[datavol]    ; updated = False
        print(" ", "Scan" if monitor_only else "Send", "volume      | ", datavol, flush=True,end="")

        if datavol in incrementals:
            updated = storage.process_deltas(storage, aset, datavol, monitor_only)

        if monitor_only:   continue

        if datavol in send_alls or updated:
            if options.dedup and not aset.dedupindex:
                init_dedup_index(aset)

            if vol.changed_bytes > dest.free:
                # Enh: add loop here for all volumes, implement a 'forced' mode
                autoprune(vol, needed_space=vol.changed_bytes, apmode=options.autoprune)
                if vol.changed_bytes > dest.free:
                    print(" %d additional bytes needed." % (vol.changed_bytes-dest.free))
                    print("Insufficient space on destination %d; Skipping." % dest.free)
                    error_cache.append(datavol)
                    continue

            dnew   = send_volume(storage, vol, localtime, ses_tags, send_all=datavol in send_alls)
            dest.free -= int(dnew + (dnew * 0.05))

        else:
            storage.lvols[vol.name].rotate_snapshots(rotate=False)    ; dnew = 0

        print("\r" if dnew else "\r  No changes   ", flush=True)


# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS or ^tagname format.

def prune_sessions(volume, times):

    sessions = volume.sesnames
    t1, t2   = "", ""                ; to_prune  = []

    if len(sessions) < 2:    print("  No extra sessions to prune.")    ; return

    # Validate date-time params
    for pos, dt in enumerate(times[:]):
        if not dt[0].startswith("^"):
            if not dt.startswith("S_"):   times[pos] = "S_"+dt.strip()
            datetime.datetime.strptime(times[pos][2:], "%Y%m%d-%H%M%S")
        elif dt[1:] not in volume.tags:
            print(" No match for", dt)    ; return
        elif pos == 0:
            for sesname in sessions:
                if dt[1:] in volume.sessions[sesname].tags:
                    if len(times) == 1 and not options.allbefore:
                        to_prune.append(sesname)
                    else:
                        t1 = sesname   ; break
        elif pos == 1:
            for sesname in reversed(sessions):
                if dt[1:] in volume.sessions[sesname].tags:   t2 = sesname   ; break

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    if options.allbefore:   t1 = sessions[0]    ; t2 = times[0]
    if not t1:   t1 = times[0]
    if not t2 and len(times) > 1:
        t2 = times[1]
        if t2 <= t1:  x_it(1, "Error: Second date-time must be later than first.")

    # Find specific sessions to prune in contiguous range
    if to_prune:
        pass

    elif t2 == "":
        # find single session
        if t1 in sessions:   to_prune.append(t1)

    else:
        # find sessions in a date-time range
        start = len(sessions)   ; end = 0
        if t1 in sessions:
            start = sessions.index(t1)
        else:
            for ses in sessions:
                if ses > t1:   start = sessions.index(ses)    ; break
        if t2 in sessions:
            end = sessions.index(t2)+1
        else:
            for ses in reversed(sessions):
                if ses < t2:   end = sessions.index(ses)+1    ; break
        to_prune = sessions[start:end]

    if len(to_prune) and to_prune[-1] == sessions[-1]:
        print("  Preserving latest session.")
        del(to_prune[-1])
    if len(to_prune) == 0:
        print("  No selections in this date-time range.")
        return

    autoprune(volume, apmode=options.autoprune, include=set(to_prune))


# Parameters / vars for autoprune:
# oldest (date): date before which all sessions are pruning candidates
# thin_days (int): number of days ago before which the thinning params are applied
# ndays & nsessions: a days/sessions ratio for amount of sessions left after thinning
# nthresh: min number of sessions to prune this time (0 = prune all candidates)
# target_size (0 or MB int): User-selected size cap for archive (future)

def autoprune(vol, needed_space=0, apmode="off", include=set()):

    dtdate = datetime.date
    def to_date(sesdate):
        return dtdate(int(sesdate[2:6]),int(sesdate[6:8]),int(sesdate[8:10]))

    if len(vol.sesnames) < 2:   return False
    datavol   = vol.name                      ; dest    = vol.archive.dest
    apmode    = apmode.lower()                ; exclude = set()
    sessions  = vol.sesnames[:-1]             ; marked  = 0
    today     = dtdate.today()                ; oldest  = today-datetime.timedelta(days=366)
    thin_days = datetime.timedelta(days=32)   ; ndays   = 7      ; nsessions = 2
    startdate = to_date(sessions[0])          ; nthresh = 3 if apmode == "on" else 0
    enddate   = min(to_date(sessions[-2]), today-thin_days) if len(sessions) > 2 else None

    # Make a 2d array of ordinal dates and populate with session id + flag
    apcal = { day: [] for day in range(dtdate(startdate.year,1,1).toordinal(),
                                       to_date(sessions[-1]).toordinal()+ndays) }
    for ses in sessions:  apcal[to_date(ses).toordinal()].append([ses, True])

    # Build set of excluded sessions
    for sx in options.keep:
        if not sx.startswith("^"):
            datetime.datetime.strptime(sx, "%Y%m%d-%H%M%S")    ; exclude.add("S_"+sx)
        else:
            if sx[1:] == "all":
                exclude += {x.name for x in vol.sessions if x.tags}
            else:
                exclude += {x.name for x in vol.sessions if sx[1:] in x.tags}
    include -= exclude

    # Mark all sessions prior to oldest date setting, plus include list
    for ses in sessions:
        sdate = to_date(ses)
        if (apmode != "off" and sdate <= oldest) or ses in include:
            for dses in apcal[sdate.toordinal()]:
                if dses[0] == ses and ses not in exclude:
                    dses[1] = False    ; vol.sessions[ses].toggle = False

    # Mark sessions for thinning-out according to ndays + nsessions
    if apmode != "off" and (needed_space or apmode != "min") and enddate:
        for year in range(startdate.year, enddate.year+1):
            for span in range(dtdate(year,1,1).toordinal(),
                            min(dtdate(year,12,31), enddate).toordinal(), ndays):
                dlist = []    ; offset = 0
                for day in range(span, min(span+ndays, enddate.toordinal())):
                    dlist.append(sum( x[1] for x in apcal[day] ))
                while sum(dlist) > nsessions: ## Enh: Make even distribution
                    bigday = dlist.index(max(dlist[offset:]), offset)
                    offset += (ndays//nsessions)+1    ; offset %= min(ndays, len(dlist))
                    for dses in apcal[span+bigday]:
                        if dses[1]:
                            # always decr bigday, but don't toggle if session is excluded
                            dlist[bigday] -= 1
                            dses[1] = vol.sessions[dses[0]].toggle = dses[0] in exclude
                            break

    # Find contiguous marked ranges and merge/prune them. Repeat until free >= needed space.
    factor = 1    ; sessions.append("End")
    while True:
        to_prune = []    ; removed_ct = 0    ; skipped = False
        for ses in sessions:
            if ses is None:   continue
            if ses == "End" or vol.sessions[ses].toggle :
                if to_prune:
                    # prioritize ranges that overlap with requested includes
                    if include and not (set(to_prune) & include):
                        to_prune.clear()    ; skipped = True    ; continue

                    target_s = vol.sesnames[vol.sesnames.index(to_prune[-1]) + 1]
                    merge_sessions(vol, to_prune, target_s, clear_sources=True)

                    for i in to_prune:   sessions[sessions.index(i)] = None
                    include -= set(to_prune)    ; removed_ct += len(to_prune);   to_prune.clear()
                    if not include and nthresh and removed_ct >= nthresh*factor:  break # for ses
            else:
                to_prune.append(ses)

        if removed_ct:
            if options.verbose:   print(datavol+": Removed", removed_ct)
            dest.get_free(tmpdir+"/merge.log")    ; os.remove(tmpdir+"/merge.log")

        if skipped and apmode != "off":   continue # while
        if removed_ct == 0 or nthresh == 0 or needed_space <= dest.free:
            break # while
        elif factor > 4:
            nthresh = 0
        else:
            factor += 2

    return True


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session dir name.

def merge_manifests(volume, msessions=None, mtarget=None, addcol=False):
    # Enh: implement mtarget to support merge_sessions()
    aset      = volume.archive
    msessions = volume.sesnames if not msessions else msessions
    sespaths  = [ os.path.basename(volume.sessions[x].path) for x in msessions ]
    tmp       = aset.big_tmpdir if volume.volsize() > 128000000000 else tmpdir
    outfile   = tmp+"/manifest.mrg"     ; slist  = []

    if not aset.dedupsessions:   volume.decode_manifests(msessions)
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(sespaths)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir  = tmp+"/m"     ; slsort  = slist[1]
        shutil.rmtree(cdir, ignore_errors=True);   os.makedirs(cdir)

        # fix: extrapolate path with filename
        do_exec([[CP.xargs, "-0", "-a", slist[0],
                  CP.awk, r'{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                            +tmp+'/m/"FILENAME}']], cwd=volume.path)
    else:
        cdir  = volume.path     ; slsort  = slist[0]

    do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=16", "--files0-from="+slsort]],
            out=outfile, cwd=cdir)
    if addcol:  shutil.rmtree(tmp+"/m")

    return outfile


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume, source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(volume, sources, target, clear_sources=False):

    aset       = volume.archive    ; dest = aset.dest    ; resume = bool(aset.in_process)
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    chformat   = "x%0"+str(chdigits)+"x"
    m_tmp      = tmpdir if volume.volsize() < 128000000000 else aset.big_tmpdir

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)         ; destvol       = "/"+volume.vid

    if not resume:
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max(x.volsize for x in volume.sessions.values()
                                    if x.name in sources)
        last_chunk = chformat % volume.last_chunk_addr(volsize)
        lc_filter  = '"'+last_chunk+'"'

        with open("merge.lst", "wt") as lstf:
            print(merge_target, target, file=lstf)
            volume.decode_one_manifest(volume.sessions[merge_target])

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                volume.decode_one_manifest(volume.sessions[ses])
                do_exec([[CP.sed, "-E", r"s|$| "+ses+r"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=16"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]],
                out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, r"$2<="+lc_filter+r" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, r"$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                r"s|^0 x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"-rm " +merge_target+ r"/\1/x\1\2|; t; "

                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|"
                r"rename \3/\1/x\1\2 " +merge_target+ r"/\1/x\1\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.lst")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, r"$2>"+lc_filter, m_tmp+"/manifest.two"],
                     [CP.sed, "-E", r"s|^\S+\s+x(\S{" + str(address_split[0]) + r"})(\S+)|"
                                    r"-rm " + merge_target + r"/\1/x\1\2|"]
                    ], out=">>merge.lst")

        do_exec([[CP.gzip, "-f", "merge.lst"]])

    if not resume:
        # Set archive in_process state to "merge"
        aset.set_in_process(["merge", volume.name, str(clear_sources), target, sources],
                            tmp=False, todest=False)

    # Update & send new metadata and process lists to dest
    if clear_sources:
        for ses in sources:
            if ses in volume.sessions:   volume.delete_session(ses, remove=False)

    if not resume:
        aset.set_in_process(None, tmp=True, todest=False)
        volume.sessions[target].save_info(".tmp")
        do_exec([[CP.tar, "-cf", "-", "../archive.ini", "../archive.ini.tmp", "merge.lst.gz",
                            target+"/manifest.z.tmp", target+"/info.tmp", "volinfo.tmp"],
                 dest.run_args([dest.cd + destvol + " && tar -xmf -"])
                ], cwd=volume.path)

    # Start merge operation on dest
    retcode = dest.run([dest.cd + destvol + " && python3 "
                        + dest.dtmp+"/dest_helper.py merge "+merge_target
                        + (" --resume" if resume else "") + (" --sync" if options.maxsync else "")
                        ],
                        check=False, out=">>"+tmpdir+"/merge.log"
                      )

    catch_signals()
    if retcode == 50:
        # Initialization didn't complete, so reload aset and abort
        aset = ArchiveSet(metadir, aset.dest, prior_auth=aset)
        aset.set_in_process(None)
        for f in ("merge.lst.gz","volinfo.tmp","vi.dat.tmp",target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest.run([dest.cd + destvol + " && rm -rf merge merge.lst.gz"], check=False)
        err_out("Error: Merge could not initialize!")
        if not resume:   sys.exit(1)
    elif retcode != 0:
        x_it(retcode, "Error: Remote exited!")

    # Finalize merge operation on dest
    dest.run([dest.cd + destvol + " && python3 "
                +dest.dtmp+"/dest_helper.py merge --finalize"+(" --resume" if resume else "")
                +" && rm -rf merge merge.lst.gz"
                ],
                trap=True, out=">>"+tmpdir+"/merge.log"
            )

    # Local finalize
    for f in (target+"/info", target+"/manifest.z", target+"/manifest",
              "vi.dat", "volinfo", "../archive.ini"):
        if exists(f+".tmp"):  os.replace(f+".tmp", f)

    aset.set_in_process(None, todest=True)    ; os.remove("merge.lst.gz")
    catch_signals(None)

    # Check consistency after resuming merge
    if resume and compare_files(aset, volumes=[volume], sessions=[volume.sessions[target]],
                                manifest=True):
        x_it(1, "Error: Local and dest metadata differ.")

    for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(storage, vol, select_ses="", save_path="", diff=False, verify_only=0):

    def diff_compare(dbuf,z):
        if dbuf != volf.read(chunksize):
            if remap:
                volsegment = addr // chunksize 
                bmap_pos = volsegment // 8
                bmap_mm[bmap_pos] |= 1 << (volsegment % 8)
            return len(dbuf)
        else:
            return 0

    dest        = (aset := vol.archive).dest
    verbose     = not options.quiet and verify_only != 2    ; debug = options.debug
    attended    = not options.unattended          ; remap     = options.remap
    sparse      = options.sparse                  ; sparse_write = options.sparse_write or sparse
    chunksize   = aset.chunksize                  ; zeros     = bytes(chunksize)
    sessions    = vol.sesnames
    # functions
    compress    = compressors[aset.compression][2]; compresslevel = int(aset.compr_level)
    decompress  = compressors[aset.compression][0].decompress
    decrypt     = aset.datacrypto.decrypt if aset.datacrypto else None
    compare_digest = hmac.compare_digest          ; b64enc    = base64.urlsafe_b64encode
    gethash     = hash_funcs[aset.hashtype]

    if diff or verify_only:
        save_path = ""
    #elif not save_path:
        #save_path = (storage.path, vol.name)

    # Set the session to retrieve
    if select_ses:
        if select_ses[0] == "^":
            # match tag to session id
            tag = select_ses[1:]
            if tag in vol.tags:
                select_ses = sorted(vol.tags[tag])[-1]
                print("Matched tag to", select_ses)
        else:
            # validate date-time input
            datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
            select_ses = "S_"+select_ses

        if select_ses not in sessions:
            err_out(f"No volume {vol.name} in session {select_ses}.")
            return None

    elif len(sessions) > 0:
        # default to last session
        select_ses = sessions[-1]
    else:
        err_out("No sessions available.")
        return None

    if diff and remap and select_ses != sessions[-1]:
        err_out("Cannot use prior session for remap.")
        return None

    ##if save_path and exists(save_path) and attended:
        ##print("\n!! This will", "overwrite" if sparse_write else "erase all",
              ##"existing data in",save_path,"!!")
        ##ans = ask_input("   Are you sure? [y/N]: ")
        ##if ans.lower() not in {"y","yes"}:
            ##return None

    volsize     = vol.sessions[select_ses].volsize   ; magic       = dest.magic
    chdigits    = max_address.bit_length() // 4      ; chformat    = "x%0"+str(chdigits)+"x"
    lchunk_addr = vol.last_chunk_addr(volsize)       ; last_chunkx = chformat % lchunk_addr
    addrsplit   = -address_split[1]                  ; rc = volf = None

    # Collect session manifests
    incl_ses = []
    for ses in sessions if verify_only != 2 else [select_ses]:
        incl_ses.append(ses)
        if ses == select_ses:  break

    # verify metadata
    if not aset.just_fetched \
    and compare_files(aset, volumes=[vol], sessions=[vol.sessions[x] for x in incl_ses],
                      manifest=True):
        err_out("Error: Local and archive metadata differ.")
        return None
    vol.decode_manifests(incl_ses, force=True)

    # Merge manifests and send to archive system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    manifest = merge_manifests(vol, msessions=incl_ses, addcol=True)

    if not sparse:
        cmds = [[CP.sed, "-E", r"/" +last_chunkx+ r"/q", manifest],  ## Enh: detect vol_shrank
                [CP.sed, "-E", r"/^0\s/ d; "
                r"s|^\S+\s+x(\S{" +str(address_split[0])+ r"})(\S+)\s+(S_\S+)|\3/\1/x\1\2|;"],
                [CP.gzip, "-c", "-4"
                ],
                dest.run_args(["cat >"+dest.dtmp+"/dest.lst.gz"]),
            ]   # Enh: replace list with range
        do_exec(cmds)

    # Prepare save volume
    if not (diff or verify_only):

        # Decode save path semantics
        if save_path:
            save_type, lvpool, lpath = LocalStorage.parse_local_path(os.path.dirname(save_path))
            if save_type not in (None,"block device"):
                save_storage = LocalStorage((lvpool, lpath), auuid=aset.uuid)
        else:
            save_storage = storage
            save_type = "logical volume" if storage.pooltype=="tlvm" else "file"
            #, path1, path2 = LocalStorage.parse_local_path(l_vol.pdir)

        # Does save path == original path?
        returned_home = False ## save_path == storage.lvols[vol.name].path

        punch_hole = save_storage.block_discard_chunk

        if save_type == "logical volume":
            l_vol = save_storage.new_vol_entry(vol.name)    ; save_path = l_vol.path
            if not l_vol.exists():
                print("Creating '%s' in thin pool %s[%s]." %
                      (l_vol.name, save_storage.path, save_storage.lvpool))
                # Fix:  translate to safe lvm name
                l_vol.create(volsize, ro=False)

            elif l_vol.getsize() != volsize:
                if debug:   print("Re-sizing LV to %d bytes." % volsize)
                l_vol.resize(volsize)

            volf = open(save_path, "r+b")

        elif save_type == "block device" and exists(save_path):
            if not sparse_write:   do_exec([[CP.blkdiscard, save_path]])
            volf = open(save_path, "r+b")

        elif os.path.abspath(save_path).startswith("/dev/"):
            x_it(1, "Cannot create new volume from ambiguous /dev path.\n"
                    " Please create the volume before using 'receive', or specify"
                    " --save-to=volgroup/pool/volname in case of a thin LVM volume.")
        else: # file
            if not save_path:
                l_vol = save_storage.new_vol_entry(vol.name)    ; save_path = l_vol.path
            save_type = "file"   ; punch_hole = save_storage.file_punch_hole
            if not exists(save_path):
                d = os.path.dirname(save_path)
                if d:   os.makedirs(d, exist_ok=True)
                open(save_path, "wb").close()
            volf = open(save_path, "r+b")   ; volf.truncate(volsize)

    elif diff:
        l_vol = storage.lvols[vol.name]     ; snap1vol = l_vol.snap1
        if not l_vol.exists():
            err_out("Local volume must exist for diff.")
            return None
        if remap:
            raise NotImplementedError()
            #lv_remove(vgname, snap1vol)
            #do_exec([[CP.lvm,"lvcreate", "-pr", "-kn", "-ay", "--addtag=wyng",
                    #"--addtag="+vol.last, "--addtag=delta",
                    #"--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n", snap1vol]])
            #print("  Initial snapshot created for", datavol)
            #get_lvm_vgs(aset.vgname)
            #if not exists(vol.mapfile):
                #vol.init_deltamap(vol.mapfile, vol.mapsize())
            #bmapf = open(vol.mapfile, "r+b")
            #bmapf.truncate(vol.mapsize())    ; bmapf.flush()
            #bmap_mm = mmap.mmap(bmapf.fileno(), 0)
        else:
            if storage.lvols[snap1vol].exists():
                l_vol = storage.lvols[snap1vol]
            else:
                print("Snapshot not available; Comparing with source volume instead.")

            if volsize != l_vol.getsize():
                err_out("Volume sizes differ:\n  Archive = %d \n  Local   = %d"
                        % (volsize, l_vol.getsize()))   ; return None

        volf  = open(l_vol.path, "r+b")


    if volf:
        volf_read = volf.read     ; volf_write = volf.write    ; volf_seek = volf.seek
        volfno    = volf.fileno() ; fcntl.lockf(volf, fcntl.LOCK_EX|fcntl.LOCK_NB)

    if verbose:
        print("\nReceiving" if save_path else "\nVerifying", "volume:", vol.name, select_ses[2:])
        if save_path:    print("Saving to %s '%s'" % (save_type, save_path))

    # Create retriever process using py program
    cmd = dest.run_args(
            [dest.cd + "/"+vol.vid
             +" && exec 2>>"+dest.dtmp+"/receive.log"
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ])
    getvol   = SPr.Popen(cmd, stdout=SPr.PIPE,
                                     stdin =SPr.PIPE if sparse else SPr.DEVNULL)
    gv_stdin = io.TextIOWrapper(getvol.stdin, encoding="utf-8") if sparse else None

    # Open manifest then receive, check and save data
    addr = bcount = diff_count = 0    ; buf = b''
    for mfline in open(manifest, "r"):
        if addr >= lchunk_addr:   break
        cksum, faddr, ses = mfline.split()    ; addr = int(faddr[1:], 16)

        if verbose:   print("%.2f%%" % (addr/volsize*100), end="\x0d")

        # Process zeros quickly
        if cksum == "0":
            if save_path:
                volf_seek(addr)
                if sparse_write and volf_read(chunksize) != zeros:
                    # Note: punch_hole() might not work, write zeros first anyway
                    volf_seek(addr)    ; volf_write(zeros)    ; diff_count += chunksize
                    punch_hole(volfno, addr, chunksize)
            elif diff:
                volf_seek(addr)    ; diff_count += diff_compare(zeros,True)

            continue

        # Request chunks on-demand if local chunk doesn't match cksum
        if sparse and save_path:
            volf_seek(addr)
            if b64enc(gethash(compress(volf_read(chunksize),
                                       compresslevel)).digest()).decode("ascii") == cksum:
                continue
            else:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)

        # Read chunk size
        assert getvol.stdout.read(3) == magic
        untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

        # allow for slight expansion from compression algo
        if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
            raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

        ##  Size is OK  ##
        size = untrusted_size

        # Read chunk buffer
        untrusted_buf = getvol.stdout.read(size)
        rc  = getvol.poll()
        if rc is not None and len(untrusted_buf) == 0:
            break

        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))


        # Decrypt the data chunk
        # Validation MUST be next step!
        if decrypt:
            untrusted_buf = decrypt(untrusted_buf)

        # Validate data chunk
        if not compare_digest(cksum, b64enc(gethash(untrusted_buf).digest()).decode("ascii")):
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            print(size, mfline)
            raise ValueError("Bad hash "+faddr+" :: "+str(b64enc(gethash(untrusted_buf).digest())))

        ##  Buffer is OK  ##
        buf = untrusted_buf   ; bcount += len(buf)

        if verify_only:   continue

        # Proceed with decompress.
        decomp = decompress(buf)
        if len(decomp) != chunksize and addr < lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        if addr == lchunk_addr and len(decomp) != volsize - lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        buf = decomp

        if save_path:
            volf_seek(addr)
            # Don't re-check buffer for sparse mode, check for sparse_write:
            if sparse:
                volf_write(buf)    ; diff_count += len(buf)
            elif sparse_write:
                if volf_read(chunksize) != buf:
                    volf_seek(addr)    ; volf_write(buf)    ; diff_count += len(buf)
            else:
                volf_write(buf)
        elif diff:
            volf_seek(addr)    ; diff_count += diff_compare(buf,False)


    if rc is not None and rc != 0:
        raise RuntimeError("Error code from getvol process: "+str(rc))

    if not sparse and verify_only != 2 and addr+len(decompress(untrusted_buf)) != volsize:
        raise ValueError("Received range %d does not match volume size %d."
                            % (addr+len(decompress(untrusted_buf)), volsize))

    print(f"Data bytes: {bcount} / {addr+len(buf)}", end="")
    print("  OK" if not diff_count else "  Diff bytes: " + str(diff_count))

    if volf:  volf.close()
    if save_path:
        os.sync()
        if returned_home and select_ses == sessions[-1]:
            tags = ["--addtag=wyng", "--addtag=arch-"+aset.uuid]
            save_storage.lvols[l_vol.snap1].create(snapshotfrom=vol.name, addtags=tags)
            if verbose:   print("  Snapshot created for", vol.name)
            vol.init_deltamap(vol.mapfile, vol.mapsize())
    if remap:
        bmapf.close()
        if diff_count > 0 and options.action != "send":
            print("\nNext 'send' will bring this volume into sync.")
    elif diff and diff_count:
        return None

    return bcount


# Rename a volume in the archive

def rename_volume(storage, aset, oldname, newname):

    os.chdir(aset.path)    ; volume   = aset.vols[oldname]
    if not aset.rename_volume_meta(oldname, newname, ext=".tmp"):
        x_it(1, "Error: Cannot rename '%s' to '%s'." % (oldname,newname))

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[volume], ext=".tmp")
    for fl in (aset.confpath, volume.path+"/volinfo", volume.path+"/vi.dat"):
        os.replace(fl+".tmp", fl)
    catch_signals(None)

    # move snapshots to new pathname
    new_lvol = storage.new_vol_entry(newname)    ; old_lvol = storage.lvols[oldname]
    for atr in ("snap1","snap2"):
        if new_lvol.exists():
            storage.lvols[getattr(old_lvol,atr)].rename(getattr(new_lvol,atr))
        else:
            storage.lvols[getattr(old_lvol,atr)].delete()


def add_volume(aset, datavol, desc):
    #if not storage.online or not exists(storage.lvols[datavol].path):
        #print("Warning:", datavol, "not found.")
    vol = aset.add_volume_meta(datavol, desc=desc, ext=".tmp")
    if not vol:   return

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    for fl in (aset.confpath, vol.path+"/volinfo", vol.path+"/vi.dat"):
        os.replace(fl+".tmp", fl)
    catch_signals(None)

    #if lv_exists(aset.vgname, datavol+".tick") \
    #and "arch-"+aset.uuid not in volgroups[aset.vgname].lvs[datavol+".tick"].tags:
        #sys.stderr.write("Warning: Volume '%s' is already tracked"
                            #"by a Wyng snapshot from a different archive!\n" % datavol)


# Remove a volume from the archive

def delete_volume(storage, aset, dv):

    if not options.unattended and not aset.in_process:
        print("Warning! Delete will remove ALL metadata AND archived data",
              "for volume", dv)

        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:   x_it(0,"")
        print()

    print("Deleting volume", dv, "from archive.")
    catch_signals()
    if not aset.in_process:       aset.set_in_process(["delete", dv])
    dvid = aset.delete_volume_meta(dv)
    update_dest(aset, pathlist=[aset.confname])
    aset.set_in_process(None)

    if dvid:
        aset.dest.run([aset.dest.cd + "  && rm -rf '%s'" % dvid])
    catch_signals(None)

    # remove snapshots
    for lvol_name in (storage.lvols[dv].snap1, storage.lvols[dv].snap2):
        storage.lvols[lvol_name].delete()
    return


# Remove Wyng snapshots and metadata from local system

def remove_local_metadata(storage, archive):

    # Remove LVM snapshots
    for lv in (x for vg in storage.vgs_all.values() for x in vg.values()):
        if lv.is_arch_member() == "true" or (not archive and "wyng" in lv.tags.split(",") and
                                             lv.name.endswith(lv.snap_ext)):
            lv.delete()

    # Remove reflink snapshots by scanning local dir
    if storage.pooltype == "rlnk":
        exts = ReflinkVolume.snap_ext    ; lvols = storage.lvols
        if archive:
            archpaths = [lvols[getattr(lvols[x], y)] for x in archive.vols.keys()
                                                      for y in ("snap1","snap2")]
        for ldir, _, files in os.walk(storage.path):
            for fpath in (ldir+"/"+x for x in files if x.endswith(exts)):
                if not archive or fpath not in archpaths:   os.remove(fpath)

    # Remove metadata dir(s)
    shutil.rmtree(metadir+"/"+prog_name, ignore_errors=True)
    shutil.rmtree(archive.path if archive else metadir+"/"+ArchiveSet.bkdir, ignore_errors=True)
    shutil.rmtree(archive.path if archive else metadir+"/"+ArchiveSet.bkdir+".old",
                  ignore_errors=True)


def show_list(aset, selected_vols):

    if aset.dest.archive_ini_hash == "none":   print("(CACHED)")
    if options.verbose:
        print("\nArchive Settings:")
        for key, val in aset.conf["var"].items():
            print(" %-15s = %s" % (key, val))

    # Print list of sessions grouped by tag. First, organize by tag:
    if options.tag:
        print("\nTag Assignments")    ; ltags = {}
        for dv in selected_vols or datavols:
            vol = aset.vols[dv]
            for tag, tses in vol.tags.items():
                #tset = ( dv+" / "+x[2:] for x in tses )
                tset = ( (dv, x[2:], vol.sessions[x].tags[tag] ) for x in tses )
                if tag not in ltags:
                    ltags[tag] = list(tset)
                else:
                    ltags[tag] += tset
        # Print result:
        for tag in sorted(ltags):
            print("\n"+tag+":")
            for ses in sorted(ltags[tag]):   print(" ", ses)
        return

    # Print list of volumes if no volume is selected.
    if not aset.vols:
        print("No volumes.")    ; return
    elif not selected_vols and not len(options.volumes):
        print("\nVolumes:")
        maxwidth = max(len(x.name) for x in aset.vols.values())
        fmt    = "%7.2f GB  %s  %-" + str(maxwidth+2) + "s  %s (%3d)"
        for vname in sorted(x.name for x in aset.vols.values()):
            vol = aset.vols[vname]   ; sname = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            if options.verbose:
                print(fmt % ((vol.volsize() / 1024**3), vol.vid, vname, sname, len(vol.sessions)))
                if vol.desc:   print("  desc:", vol.desc)
            else:
                print("",vname)
        return

    # Print list of sessions grouped by volume.
    # Get the terminal column width and set cols to number of list columns.
    ttycols = os.popen('stty size', 'r').read().split()[1]
    cols = max(4, min(10, int(ttycols)//17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        if not aset.vols[dv].sessions:   print("None.")    ; continue
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for sname in vol.sesnames + [""]:
            month = sname[2:8]    ; ses = vol.sessions[sname] if sname else None
            if options.unattended or options.verbose:
                # plain listing
                print(sname[2:])
                if ses and options.verbose:
                    for tag in sorted(ses.tags.items()):
                        print(" tag:", tag[0]+(", "+tag[1] if tag[1] else ""))
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(sname)
            else:
                # print the month listing: 'rows' is adjusted to carry the remainder on
                # additional lines. 'extra' and 'steps' are used to eliminate ragged
                # column on the right (a ragged row is more pleasing to the eye).
                size  = len(slist)    ; rows = size//cols
                rows += (size%rows)//cols if rows else 0     ; extra = size - cols*rows
                heights = [rows+(x<extra) for x in range(cols) ]

                # output one row at a time
                for ii in range(max(heights)):
                    print("  ".join(  slist[ii+sum(heights[:c])][2:] for c in range(cols)
                                       if ii < heights[c]  ))
                print()

                # start new month list
                lmonth = month    ; slist = [sname]


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def is_num(val):
    try:
        float(val)
    except:
        return False
    else:
        return True


def os_kill(pid, sig=signal.SIGTERM):
    try:
        os.kill(pid, sig)
    except ProcessLookupError:
        pass


def catch_signals(sel=["INT","TERM","QUIT","ABRT","ALRM","TSTP","USR1","USR2"], iflag=False):
    # Remove existing handlers
    for sig in list(signal_handlers):   signal.signal(sig, signal_handlers.pop(sig))
    # Set handler on requested signals
    for sig in (getattr(signal,"SIG"+x) for x in (sel or [])):
        signal_handlers[sig] = signal.getsignal(sig)   ; signal.signal(sig, handle_signal)
        signal.siginterrupt(sig, iflag)

    while not sel and signals_caught:   os.kill(os.getpid(), signals_caught.pop(0))


def handle_signal(signum, frame):
    sys.stderr.write(" *** Caught signal "+str(signum))
    if signum == signal.SIGALRM:  raise TimeoutError("SIGALRM")
    if signum not in signals_caught:   signals_caught.append(signum)


def ask_input(text):
    sys.stderr.write(text)
    return input()


def show_dest_free(dest):
    print("\n%dMB free on destination." % (dest.free//1024//1024))


# Exit with simple message
def x_it(code, text):
    err_out(text)
    if tmpdir:   cleanup()
    sys.exit(code)


def err_out(text):
    sys.stdout.write(text+"\n")


def cleanup():
    if debug:
        shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
        shutil.move(tmpdir, "/tmp/"+prog_name+"-debug")
    if not debug:
        if dest:   dest.remove_dtmp()
        if aset:   shutil.rmtree(aset.big_tmpdir, ignore_errors=True)
        shutil.rmtree(tmpdir, ignore_errors=True)
    if error_cache:
        err_out("Error on volume(s): " + ", ".join(error_cache))
        sys.exit(2)




##  MAIN  #########################################################################################

# Constants / Globals
prog_name             = "wyng"
prog_version          = "0.4wip014"     ; prog_date = "20230321"
format_version        = 3               ; debug     = False       ; tmpdir = None
admin_permission      = os.getuid() == 0
aset = dest = None


if sys.hexversion < 0x3080000:
    x_it(1, "Python ver. 3.8 or greater required.")

# Allow only one instance at a time
lockpath = "/var/lock/"+prog_name
try:
    lockf = open(lockpath, "w")
    fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
except PermissionError:
    x_it(1, "ERROR: No writing permission on %s.  Run %s as root." % (lockpath,prog_name))
except IOError:
    x_it(1, "ERROR: "+prog_name+" is already running.")

cpu_flags = [x for x in open("/proc/cpuinfo") if x.startswith("flags")] [0].split()[1:]

max_address           = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split         = [len(hex(max_address))-2-7, 7]
hash_bits             = 256

os.environ["LC_ALL"]  = "C"    ; shell_prefix   = "set -e && export LC_ALL=C\n"

pjoin       = os.path.join    ; exists = os.path.exists

local_actions         = ("monitor","version")
write_actions         = ("add","send","prune","delete","rename","arch-delete","arch-deduplicate")
dest_actions          = ("list","receive","verify","diff","arch-check","arch-init") + write_actions

# Parse Arguments:
parser = argparse.ArgumentParser(description="")
parser.add_argument("action", choices=local_actions+dest_actions, help="Action to take")
parser.add_argument("-u", "--unattended", action="store_true", default=False,
                    help="Non-interactive, supress prompts")
parser.add_argument("--pass-agent", type=int, default=0,
                    help="Minutes to remember passphrase")
parser.add_argument("--all-before", dest="allbefore", action="store_true", default=False,
                    help="Select all sessions before --session date-time.")
parser.add_argument("--all", action="store_true", default=False)
parser.add_argument("--session", help="YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS] or ^tag_id"
                                 " select session date|tag, singular or range.")
parser.add_argument("--tag", action="append",
                    help="tag_id[,description] Add tags when sending")
parser.add_argument("--keep", action="append", default=[],
                    help="YYYYMMDD-HHMMSS or ^tag_id exclude session by date|tag (prune)") 
parser.add_argument("--volex", action="append", help="Exclude volume")
parser.add_argument("--autoprune", default="off", help="Automatic pruning")
parser.add_argument("--save-to", dest="saveto", default="",help="Path to store volume for receive")
parser.add_argument("--sparse", action="store_true", default=False, help="Retrieve differences only")
parser.add_argument("--sparse-write", action="store_true", default=False, help="Save differences only")
parser.add_argument("--remap", action="store_true", default=False, help="Remap snapshots")
parser.add_argument("--dest", default="",     help="URL to archive")
parser.add_argument("--dest-name", "-n", default="", help="Nickname for dest location")
parser.add_argument("--local", default="",    help="Init: LVM vg/pool containing source volumes")
parser.add_argument("--encrypt", default="xchacha20", help="Init: compression type:level")
parser.add_argument("--compression", default="", help="Init: compression type:level")
parser.add_argument("--hashtype", default="", help="Init: hash function type")
parser.add_argument("--chunk-factor", dest="chfactor", type=int,
                    help="Init: set chunk size to N*64kB")
parser.add_argument("--meta-dir", dest="metadir", default="", help="Use alternate metadata path")
parser.add_argument("--force", action="store_true", default=False, help="For arch-delete")
parser.add_argument("--clean", action="store_true", default=False, help="Clean snapshots, metadata")
parser.add_argument("--maxsync", action="store_true", default=False, help="Use more fs sync")
parser.add_argument("--debug", action="store_true", default=False, help="Debug mode")
parser.add_argument("--quiet", action="store_true", default=False)
parser.add_argument("--verbose", action="store_true", default=False)
parser.add_argument("--dedup", "-d", action="store_true", default=False,
                    help="Data deduplication (send)")
parser.add_argument("--volume-desc", dest="voldesc", default="",
                    help="Set volume description (add, rename, send)")
parser.add_argument("volumes", nargs="*")
options = parser.parse_args()    ; options.action = options.action.lower()

# Set stdout to devnull if --quiet is specified
debug = options.debug    #; assert not (options.quiet and (options.verbose or debug))
verbose = options.verbose or debug
if debug:   options.verbose = True
if options.quiet and options.action not in ("list","version"):
    options.unattended = True
    sys.stdout = open(os.devnull, "w")

print("%s %s release %s" % (prog_name.capitalize(), prog_version, prog_date), flush=True)

if options.action in write_actions+("arch-init","monitor") and not admin_permission:
    x_it(1, "Must be root/admin user for %s." % options.action)
if options.action == "version":   x_it(0,"")
if options.action not in ("send", "arch-deduplicate"):   options.dedup = False
if options.action == "arch-deduplicate":   options.dedup = True


# vardir:  holds data not directly related to an archive, such as nicknames for commonly accessed
# archive URLs.
# metadir: holds cached archive metadata.
# bigtmpdir is used for metadir when an archive's metadata cache may be too big for /tmp.
vardir      = "/var/lib/"+prog_name
metadir     = options.metadir if options.metadir else "/var/lib"
tmpdir_obj  = tempfile.TemporaryDirectory(prefix=prog_name)   ; tmpdir = tmpdir_obj.name

os.makedirs(vardir, exist_ok=True)
shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
os.makedirs(tmpdir+"/rpc")
Destination.write_helper_program(tmpdir+"/rpc")
agent_helper_write(tmpdir)


## General Configuration ##

signal_handlers  = {}    ; signals_caught = []    ; error_cache = []

# Dict of compressors in the form: (library, default_compress_level, compress_func)
compressors      =    {"zlib":   (zlib, 4, zlib.compress),
                       "bz2" :   (bz2,  9, bz2.compress)}
if zstd:   compressors["zstd"] = (zstd, 3, lambda data, lvl: zstd.compress(data, lvl, 3))

hash_funcs       = {"sha256" : hashlib.sha256,
                    "blake2b": functools.partial(hashlib.blake2b, digest_size=hash_bits//8)}


# Create ArchiveSet and Destination objects with get_configs().
# Passphrase input happens here; no stdin before this point!
aset    = get_configs(options)    ; dest = aset.dest
storage = LocalStorage((None, options.local) if options.local else (aset.vgname, aset.poolname),
                       require_online= options.action in ("monitor","send","receive","diff"),
                       arch_vols=aset.vols.keys(),
                       auuid=aset.uuid)

##if options.local:   aset.set_local(opts.local)



# Check if local matches dest
if dest.online and options.action not in ("arch-init","arch-check","arch-delete") + local_actions:
    if not hmac.compare_digest(aset.raw_hashval, dest.archive_ini_hash) and dest.online:
        #if not exists(aset.confpath+".tmp"):
            #fetch_file_blobs([(aset.confname, aset.max_conf_sz)], aset.path, ext=".tmp")
            #test_aset = ArchiveSet(metadir+topdir, ext=".tmp", children=0)
            #if float(aset.updated_at) > float(test_aset.updated_at):
                #update_dest(aset, [aset.confname])
            #else:
        #else:
            #raise NotImplementedError(".tmp recovery")

        # local copy is consistent, so update destination:
        #update_dest(aset, pathlist=[aset.confname]) # Fix: expand checks from here....
        raise RuntimeError("archive.ini mis-match")

    arch_check(storage, aset, [], startup=True)





# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.
if aset.in_process and dest.online  \
    and options.action not in ("arch-init","arch-delete","list"):

    if (options.action == "delete" and aset.in_process[1] == options.volumes[0]) \
        or (options.clean and options.force):
        # user is currently deleting the in_process volume or using --clean --force
        aset.set_in_process(None)
    elif aset.in_process:
        open(aset.path+"/in_process_retry","w").close()
    elif exists(aset.path+"/in_process_retry"):
        x_it(1, "Interrupted process already retried; Exiting.")

    if aset.in_process and aset.in_process[0] in ("delete","merge"):
        print("Completing prior operation in progress:", " ".join(aset.in_process[0:2]))

        if aset.in_process[0] == "delete":
            vname = aset.in_process[1]
            delete_volume(storage, aset, vname)
        elif aset.in_process[0] == "merge":
            vname = aset.in_process[1]
            merge_sessions(aset.vols[vname], aset.in_process[4].split(":|"),
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))
    else:
        raise ValueError("Bad in_process descriptor: "+repr(aset.in_process[0]))

        aset = ArchiveSet(metadir, aset.dest, allvols=True, prior_auth=aset)
        if compare_files(aset, pathlist=[aset.confname], volumes=[aset.vols[vname]]):
            x_it(1, "Error: Local and archive metadata differ.")

# Display archive update time as local time
arch_dt_raw = datetime.datetime.fromtimestamp(float(aset.updated_at), datetime.timezone.utc)
arch_dt     = arch_dt_raw.astimezone().isoformat(sep=u" ")

print("Encrypted" if (aset.mcrypto and aset.datacrypto) else "Un-encrypted",
      "archive:", dest.spec,
      "\nLast updated %s (%s)" % (arch_dt[:-6], arch_dt[-6:]), flush=True)

# Check volume args against config
exclude_vols  = set(options.volex or [])
datavols      = sorted(set(aset.vols.keys())  - exclude_vols)
selected_vols = options.volumes[:]
for vol in selected_vols[:]:
    if selected_vols.count(vol) > 1:
        selected_vols.remove(vol)
    if vol not in aset.vols and options.action not in {"add","delete","rename","send"}:
        print("Volume "+vol+" not configured; Skipping.")
        selected_vols.remove(vol)


# Process Commands:

if options.action   == "monitor":
    monitor_send(storage, aset, selected_vols or datavols, monitor_only=True)


elif options.action == "send":
    monitor_send(storage, aset, selected_vols or datavols, monitor_only=False)


elif options.action == "prune":
    if options.autoprune.lower() == "off" and not options.session:
        x_it(1, "Must specify --autoprune or --session for prune.")
    dvs = selected_vols or datavols

    if not options.unattended and len(dvs):
        print("This operation will delete session(s) from the archive;")
        ans = ask_input("Are you sure? [y/N]: ").strip()
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    for dv in dvs:
        if dv in datavols:
            if options.session:
                prune_sessions(aset.vols[dv], options.session.split(","))
            elif options.autoprune.lower() in ("on","min","full"):
                autoprune(aset.vols[dv], apmode=options.autoprune)


elif options.action == "receive":
    if len(selected_vols) != 1 and options.saveto:
        x_it(1, "Specify one volume for receive --save-to.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify only one session for receive.")

    for dv in selected_vols:
        count = receive_volume(storage, aset.vols[dv], select_ses=options.session or "",
                                        save_path=options.saveto or "")
        if count is None:   error_cache.append(dv)


elif options.action == "verify":
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify one session for verify.")

    for dv in datavols if options.all else selected_vols:
        count = receive_volume(storage, aset.vols[dv],
                    select_ses="" if not options.session else options.session.split(",")[0],
                    verify_only=1, save_path="")
        if count is None:   error_cache.append(dv)


elif options.action == "diff":

    for dv in datavols if options.all else selected_vols:
        count = receive_volume(storage, aset.vols[dv], save_path="", diff=True)
        if count is None:   error_cache.append(dv)


elif options.action == "list":
    show_list(aset, datavols if options.all else selected_vols)


elif options.action == "add":
    if len(options.volumes) < 1:
        x_it(1, "Volume name(s) required for 'add'.")

    for dv in options.volumes:
        if dv not in aset.vols:   add_volume(aset, dv, options.voldesc)


elif options.action == "rename":
    if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
    rename_volume(storage, aset, options.volumes[0], options.volumes[1])


elif options.action == "delete":
    if options.clean:
        print("Remove local Wyng metadata from system for path",
              aset.path if options.all else aset.path)
        if not options.unattended:
            ans = ask_input("Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:
                x_it(0,"")
        elif not options.force:
            x_it(1, "Ignoring --clean without --force.")
        remove_local_metadata(storage, None if options.all else aset)

    else:
        delete_volume(storage, aset, selected_vols[0])


elif options.action == "arch-init":
    pass


elif options.action == "arch-check":
    arch_check(storage, aset, datavols)


elif options.action == "arch-delete":
    desthost = "//"+dest.sys+" " if dest.sys else ""
    print("\nDeleting ALL archived data from '%s' !" % dest.spec)

    if not options.unattended and not options.force:
        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")
    elif options.unattended and not options.force:
        x_it(1,"Error: --force required to delete entire archive.")

    # Remove metadata and snapshots
    for dv in list(aset.vols):   aset.delete_volume_meta(dv)

    # Remove local metadata and stray snapshots associated with archive
    remove_local_metadata(storage, aset)

    # Remove from destination
    print("\nDeleting entire archive...")
    cmd = [dest.cd
          +" && cd .."
          +" && rm -rf default" ####
          +" && { sync -f . || sync; }"
          ]
    dest.run(cmd)


elif options.action == "arch-deduplicate":
    show_dest_free(aset.dest)
    dedup_existing(aset)


cleanup()
# END
