#!/usr/bin/env python3


###  Wyng â€“ Logical volume backup tool
###  Copyright Christopher Laprise 2018-2021 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.



#  editor width: 100  -----------------------------------------------------------------------------
import sys, signal, os, stat, shutil, subprocess, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, hashlib, hmac, functools, uuid
import getpass, base64, platform, resource, itertools, string
import xml.etree.ElementTree
from array import array         ; from urllib.parse import urlparse

try:
    import zstd, warnings
    # Required for crippled zstd library
    warnings.filterwarnings("ignore", category=DeprecationWarning)
except:
    zstd = None

try:
    from Cryptodome.Cipher import AES as Cipher_AES
    from Cryptodome.Cipher import ChaCha20 as Cipher_ChaCha20
    from Cryptodome.Cipher import ChaCha20_Poly1305 as Cipher_ChaCha20_Poly1305
    from Cryptodome.Random import get_random_bytes
    import Cryptodome
except:
    Cipher_AES = Cipher_ChaCha20 = Cipher_ChaCha20_Poly1305 = None


# ArchiveSet manages archive metadata incl. volumes and sessions

class ArchiveSet:

    confname    = "archive.ini"   ; max_volumes  = 4096           ; volname_len = 112
    mhash_sz    = 44              ; max_sessions = 16384          ; comment_len = 100
    sesname_sz  = 17              ; tag_len      = 25             ; max_tags = 5

    max_conf_sz = (volname_len + comment_len + mhash_sz + 25) * max_volumes + 1000
    max_infosz  = (tag_len+comment_len + 10) * max_tags + 1000
    max_volinfosz= (mhash_sz + sesname_sz + 10) * max_sessions + 1000

    attr_ints   = {"format_ver","chunksize","mci_count","dataci_count"}

    def __init__(self, name, top, dest, ext="", allvols=False, children=2,
                 passphrase=None, prior_auth=None):
        self.dest        = dest
        self.name        = name
        self.path        = pjoin(top,name)
        self.confpath    = pjoin(self.path, self.confname)
        self.confprefix  = b"[WYNG%02d]\n" % format_version
        self.modeprefix  = b"ci = "
        self.mcrypto     = mcrypto = None
        self.datacrypto  = datacrypto = None
        self.vols        = {}
        self.dedupindex  = {}
        self.dedupsessions = []
        self.raw_hashval = None
        self.Volume      = ArchiveVolume

        # persisted:
        self.format_ver  = 0
        self.chunksize   = bkchunksize * 2
        self.compression = "zstd" if zstd else "zlib"
        self.compr_level = str(compressors[self.compression][1])
        self.hashtype    = "blake2b"
        self.vgname      = None
        self.poolname    = None
        self.uuid        = None
        self.updated_at  = None
        self.data_cipher = self.ci_mode = None
        self.mci_count   = self.dataci_count  =  0
        self.in_process  = []


        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"], cp["volumes"], cp["in_process"]  =  {},{},{}

        # halt configuration if this is a new or temp config
        if not exists(self.confpath+ext):
            self.uuid = str(uuid.uuid4())
            return

        # decode archive.ini file
        # Format "magic" is "[WYNGvv]\n" where vv = format version: 9 bytes
        # Format mode is next as "ci = m0\n" where m = cipher mode: 8 bytes
        with open(self.confpath+ext, "rb") as f:
            header0 = f.read(9)    ; header1 = f.read(8)    ; buf = f.read(self.max_conf_sz)
        self.raw_hashval = hashlib.sha256(header0 + header1 + buf).hexdigest()
        if not (header0 == self.confprefix and header1.startswith(self.modeprefix)):
            if not header0.startswith(b"[var]\n"):
                raise ValueError("Not a Wyng format.")
            else:
                old_v2_format = True  ; raise NotImplementedError("Old format conversion")
        else:
            old_v2_format = False    #; untrusted_ver = 2

        # use existing auth if specified
        if isinstance(prior_auth, ArchiveSet):
            self.mcrypto    = mcrypto = prior_auth.mcrypto    ; self.ci_mode = prior_auth.ci_mode
            self.datacrypto = datacrypto = prior_auth.datacrypto
            self.data_cipher = prior_auth.data_cipher
        else:
            self.ci_mode = header1[-3:-1]

        # parse metadata crypto mode and instantiate it
        if self.ci_mode != b"00" and not mcrypto:
            ci_types = DataCryptography.crypto_codes[self.ci_mode]
            if debug:   print("metadata cipher =", ci_types[1])
            if not passphrase:   passphrase = ask_passphrase()
            assert passphrase and passphrase != bytes(len(passphrase))
            self.mcrypto = mcrypto = DataCryptography(ci_types[1], self.confpath+".salt",
                                       slot=1, passphrase=passphrase[:])

        # initial decryption + auth
        try:
            if mcrypto:   buf = mcrypto.decrypt(buf)
        except Exception as e:
            sys.stderr.write("Error: Could not decrypt/authenticate archive.\n")
            raise e

        # read attributes from archive.ini
        if debug:   print(gzip.decompress(buf).decode("UTF-8"))
        cp.read_string(gzip.decompress(buf).decode("UTF-8"))
        for key, value in cp["var"].items():
            setattr(self, key, int(value) if key in self.attr_ints else value)
        self.in_process  = [ ln for ln in cp["in_process"].values() ]

        if self.format_ver > format_version or not self.format_ver:
            raise ValueError("Archive format ver = "+str(self.format_ver)+
                                ". Expected = "+str(format_version))

        # init data crypto
        if mcrypto and not self.datacrypto:
            if not float(self.updated_at) < time.time():
                raise ValueError("Current time is less than archive timestamp!")
            assert passphrase and passphrase != bytes(len(passphrase))
            self.datacrypto = datacrypto \
                = DataCryptography(self.data_cipher, mcrypto.keyfile, slot=0,
                                   passphrase=passphrase, cadence=20)

        if mcrypto:
        # use higest known counter values:
        # Enh: Examine imported .salt counters to ensure they are not wildly increased
        # above values in archive.ini (anti-DoS). Possibly warn or prompt user.
            mcrypto.counter    = self.mci_count    = max(mcrypto.counter, self.mci_count)
            datacrypto.counter = self.dataci_count = max(datacrypto.counter, self.dataci_count)

        self.compress    = compressors[self.compression][2]
        self.decompress  = compressors[self.compression][0].decompress
        self.gethash     = hash_funcs[self.hashtype]

        # load volume metadata objects
        convlist = []
        for key, value in cp["volumes"].items() if children else []:
            # conditions when a volume must be loaded:
            # - allvols flag is set
            # - in_process flag from an unfinished action
            # - volume specified on command line
            # - no volumes specified (hence all)
            # - deduplication is in effect
            #if children: ## and (allvols or self.in_process or options.from_arch or \
            ##(len(options.volumes)==0 or key in options.volumes or options.dedup)):
                # instantiate:
            if value.startswith("Vol_") and " " in value.strip(): #### convert older alpha
                vid, hashval = value.strip().split(" ", maxsplit=1)
                vname = key; convlist.append(vname)
            else:
                vid = key; hashval = value; vname = None

            #loadses = allvols or self.in_process or options.from_arch or options.dedup or
            volume  = self.Volume(self, vid, hashval, pjoin(self.path,vid),
                                    self.vgname, name=vname, children=children)
            self.vols[volume.name] = volume

        if convlist and dest.writable: #### convert older alpha
            cp["volumes"].clear()
            for v in self.vols.values():   v.save_volinfo()
            try:
                update_dest(self, pathlist=[self.confname], volumes=list(self.vols.values()))
            except:
                x_it(1,"convert error")


    def save_conf(self, ext=""):
        c = self.conf['var']    ; c.clear()    ; mcrypto = self.mcrypto
        c['uuid']        = self.uuid if self.uuid else str(uuid.uuid4())
        c['updated_at']  = self.updated_at = str(time.time())
        c['format_ver']  = str(format_version)
        c['chunksize']   = str(self.chunksize)
        c['compression'] = self.compression
        c['compr_level'] = self.compr_level
        c['hashtype']    = self.hashtype
        c['vgname']      = self.vgname      # move to volname
        c['poolname']    = self.poolname    # move to ses info
        c['data_cipher'] = self.data_cipher
        if mcrypto:
            if self.datacrypto.counter > self.datacrypto.ctstart:
                self.dataci_count = self.datacrypto.counter
            self.mci_count = mcrypto.counter
            c['dataci_count']= str(self.dataci_count)
            c['mci_count']   = str(self.mci_count)
        self.conf['in_process'] = { x: ln if type(ln) is str else ":|".join(ln)
                                          for x,ln in enumerate(self.in_process) }

        os.makedirs(self.path, exist_ok=True)
        with io.StringIO() as fs:
            self.conf.write(fs)    ; fs.flush()
            buf = gzip.compress(fs.getvalue().encode("UTF-8"), 4)
        if mcrypto:   buf = mcrypto.encrypt(buf)
        with open(self.confpath+ext, "wb") as f:
            f.write(self.confprefix + self.modeprefix + self.ci_mode + b"\n" + buf)

    # Set or clear state for the archive as 'in_process' in case of interruption during write.
    # Format is list containing strings or list of strings. For latter, ':|' is the delimiter.
    def set_in_process(self, outer_list, tmp=False, todest=True):
        fssync(aset.path)
        self.in_process = [] if outer_list is None else outer_list
        self.save_conf(".tmp" if tmp else "")

        #if not tmp:   os.replace(self.confpath+".tmp", self.confpath)
        if todest:      update_dest(aset, pathlist=[self.confname], ext=".tmp" if tmp else "")

    def add_volume(self, datavol, desc="", ext=""):
        errs = []
        if len(self.conf["volumes"]) >= self.max_volumes:   x_it(1, "Too many volumes")
        if datavol in self.vols:
            print(datavol+" is already configured.")    ; return None

        namecheck = self.Volume.volname_check(datavol)
        if namecheck:
            errs.append(namecheck+"\n")
        if len(desc.encode("UTF-8")) > self.comment_len:
            errs.append("Error: Max "+self.comment_len+" size for volume desc.\n")
        if not desc.isprintable():
            errs.append("Error: [^control] not allowed in volume desc.\n")
        if errs:
            sys.stderr.write("".join(errs))    ; error_cache.append(datavol)    ; return None

        while (vid := "Vol_"+os.urandom(2).hex()) in self.conf["volumes"]:   pass

        self.vols[datavol] = vol = self.Volume(self, vid, "0", pjoin(self.path,vid),
                                               self.vgname, name=datavol, children=0)
        vol.save_volinfo(ext)
        return vol

    def delete_volume(self, datavol):
        # Enh: add delete-by-vid
        vid = self.vols[datavol].vid    ; vpath = self.vols[datavol].path
        del(self.vols[datavol], self.conf["volumes"][vid])
        self.save_conf()

        if exists(vpath):    shutil.rmtree(vpath)
        return vid

    def rename_volume(self, datavol, newname, ext=""):
        vol = self.vols[datavol]
        if newname in self.vols or self.Volume.volname_check(newname):   return False

        vol.name = newname
        vol.save_volinfo(ext)
        return True

    def b64hash(self, buf):
        return base64.urlsafe_b64encode(self.gethash(buf).digest()).decode("ascii")

    def encode_file(self, fname, fdest=None, get_digest=True, compress=True):
        # Enh: optimize memory use
        mcrypto = self.mcrypto    ; digest = None
        destname= fdest if fdest else fname+(".z" if compress else "")

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            inbuf = bytes(inmap) if self.compression == "zstd" else inmap
            mbuf  = self.compress(inbuf, int(self.compr_level)) if compress else inbuf
            if get_digest:   digest = self.b64hash(mbuf)
            if mcrypto:      mbuf = mcrypto.encrypt(mbuf)
            open(destname,"wb").write(mbuf)

        return digest

    def decode_file(self, fname, fdest=None, digest=None, max_sz=16000000):
        # Enh: optimize memory use
        destname= fdest if fdest else (fname[:-2] if fname.endswith(".z") else fname)
        mcrypto = self.mcrypto         ; buf_start = mcrypto.buf_start if mcrypto else 0

        with  open(fname,"r+b") as inf,   mmap.mmap(inf.fileno(), 0) as inmap:
            assert buf_start < len(inmap) <= max_sz ## Fix: move to get_configs_remote()
            mbuf = mcrypto.decrypt(inmap) if mcrypto else bytes(inmap)
            assert hmac.compare_digest(digest, self.b64hash(mbuf))
            open(destname,"wb").write(self.decompress(mbuf))


class ArchiveVolume:

    __slots__ = ("vid","name","archive","path","vgname","mapfile","sessions","sesnames",
                 "last","meta_checked","tags","desc","changed_bytes")

    def __init__(self, archive, vid, hashval, path, vgname, name=None, children=2):
        self.vid       = vid                       ; self.tags    = {}
        self.archive   = archive                   ; self.path    = path
        self.vgname    = vgname                    ; self.mapfile = path+"/deltamap"
        self.last      = "None"                    ; self.meta_checked = False
        # persisted here:
        self.name      = name                      ; self.desc      = ""
        self.sessions  = {}                        ; self.sesnames= []
        # other:
        self.changed_bytes = 0

        Ses = ArchiveSession
        if debug:  print("\n", vid, "-", name)
        if path and not exists(path):   os.makedirs(path)
        if exists(path):
            do_exec([[CP.chattr, "-c", path, self.mapfile]], check=False) #### remove
        if path and hashval != "0":
            with open(pjoin(path,"volinfo"), "rb") as f:
                fsize = os.fstat(f.fileno()).st_size
                if fsize > ArchiveSet.max_volinfosz:   raise ValueError("volinfo too large")
                buf = f.read(fsize)
            if archive.mcrypto:   buf = archive.mcrypto.decrypt(buf)
            assert hmac.compare_digest(hashval, archive.b64hash(buf))
            if debug:   print(archive.decompress(buf).decode("UTF-8"))
            with io.StringIO(archive.decompress(buf).decode("UTF-8")) as f:
                for ln in f:
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if vname.startswith("S_"):
                        self.sessions[vname] = Ses(self, vname, value,
                                                   path+"/"+vname if children > 1 else "")
                    else:
                        setattr(self, vname, value)
        if not self.name: raise ValueError("Vol name missing")

        # session name list sorted by sequence field
        seslist = list(self.sessions.values()) if children > 1 else []
        seslist.sort(key=lambda x: x.sequence)
        self.sesnames = sesnames = [y.name for y in seslist]
        if sesnames:   self.last = sesnames[-1]

        if exists(pjoin(path,"volchanged")):
            self.changed_bytes = int(open(pjoin(path,"volchanged"),"r").readlines()[0].strip())


    def save_volinfo(self, ext=""):
        os.makedirs(self.path, exist_ok=True)   ; fname = "volinfo"
        with io.StringIO() as f:
            print("name =", self.name, file=f)
            print("desc =", self.desc, file=f)
            for ses in self.sessions.values():
                if ses.saved:   print(ses.name, "=", ses.hashval, file=f)

            buf = self.archive.compress(f.getvalue().encode("UTF-8"),
                                        int(self.archive.compr_level))
            self.archive.conf["volumes"][self.vid] = self.archive.b64hash(buf)

        if self.archive.mcrypto:   buf = self.archive.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(buf)
            f.flush()    ; os.fsync(f.fileno())
        self.archive.save_conf(ext)

    def volname_check(vname): # Fix: move to LocalStorage class
        check = re.compile(alphanumsym)
        if check.match(vname) is None:
            return "Only characters A-Z 0-9 . + _ - are allowed in volume names."
        if vname in (".",".."):
            return "Bad volume name."
        if len(vname) > ArchiveSet.volname_len:
            return "Volume name must be %d characters or less." % ArchiveSet.volname_len

        return "" # OK

    def volsize(self):
        return self.sessions[self.last].volsize if self.sessions else 0

    def last_chunk_addr(self, vsize=None):
        if vsize is None:  vsize = self.volsize()
        return (vsize-1) - ((vsize-1) % self.archive.chunksize)

    # Based on last session size unless volume_size is specified.
    def mapsize(self, volume_size=None):
        if not volume_size:
            volume_size = self.volsize()
        return (volume_size // self.archive.chunksize // 8) + 1

    def map_used(self, ext=""):
        return os.stat(self.mapfile+ext).st_blocks if exists(self.mapfile+ext) else 0

    def changed_bytes_add(self, amount, reset=False, save=False):
        if reset:
            if exists(self.path+"/volchanged"):   os.remove(self.path+"/volchanged")
            self.changed_bytes = 0  ; return

        self.changed_bytes += amount
        if save:
            with open(self.path+"/volchanged", "w") as f:
                print(self.changed_bytes, file=f)
                f.flush()    ; os.fsync(f.fileno())

    def new_session(self, sname, addtags={}):
        ns = ArchiveSession(self, sname, "0", addtags=addtags)
        ns.path = pjoin(self.path, sname)
        ns.sequence = self.sessions[self.last].sequence + 1 if self.sessions else 0
        ns.previous = self.last

        self.last = sname
        self.sesnames.append(sname)
        self.sessions[sname] = ns
        if self.archive.dedupindex:    self.archive.dedupsessions.append(ns)
        return ns

    def delete_session(self, sname, remove=True, force=False):
        ses     = self.sessions[sname]
        index   = self.sesnames.index(sname)    ; affected = None
        if sname == self.last and ses.saved and not force:
            raise ValueError("Cannot delete last session")

        for tag in list(ses.tags.keys()):   self.sessions[sname].tag_del(tag)
        del(self.sesnames[index], self.sessions[sname])

        if self.archive.dedupsessions:
            indexdd = self.archive.dedupsessions.index(ses)
            self.archive.dedupsessions[indexdd] = None

        # Following condition means:
        #   * sesnames cannot be empty
        #   * ses wasn't deleted from end of list

        if len(self.sesnames) > index:
            affected = self.sesnames[index]
            self.sessions[affected].previous = ses.previous

        self.last  = self.sesnames[-1] if len(self.sesnames) else "None"

        if remove and exists(pjoin(self.path, sname)):   shutil.rmtree(ses.path)
        return affected

    def decode_one_manifest(self, ses, force=False):
        if not exists(ses.path+"/manifest") or force:
            self.archive.decode_file(ses.path+"/manifest.z",
                                        digest=ses.manifesthash, max_sz=ses.manifest_max())

    def decode_manifests(self, sesnames, force=False):
        for ses in (self.sessions[x] for x in sesnames):
            if ses.path and force:
                os.makedirs(ses.path, exist_ok=True)    ; do_exec([[CP.chattr, "+c", ses.path]])
            self.decode_one_manifest(ses, force=force)


class ArchiveSession:

    attr_str  = ("localtime","previous","uuid","manifesthash")
    attr_int  = ("volsize","sequence")
    attr_misc = ("tags","volume","archive","name","path","saved","loaded","toggle",
                 "hashval","meta_checked")
    __slots__ = attr_str + attr_int + attr_misc

    def __init__(self, volume, name, hashval, path="", addtags={}):
        self.volume   = volume;    self.archive = arch = volume.archive
        self.name     = name
        self.path     = path
        self.saved    = self.loaded = False
        self.toggle   = True
        self.hashval  = hashval
        self.meta_checked = False
        # persisted:
        self.uuid     = None
        self.localtime= None
        self.volsize  = None
        self.sequence = None
        self.previous = "None"
        self.tags     = {}
        self.manifesthash = None

        if path and hashval != "0":
            with open(pjoin(path,"info"), "rb") as sf:
                fsize = os.fstat(sf.fileno()).st_size
                if fsize > ArchiveSet.max_infosz:   raise ValueError("info too large")
                buf = sf.read(fsize)
            if arch.mcrypto:   buf = arch.mcrypto.decrypt(buf)
            assert hmac.compare_digest(hashval, arch.b64hash(buf))
            with io.StringIO(arch.decompress(buf).decode("UTF-8")) as sf:
                for ln in sf:
                    if ln.strip() == "uuid =":  continue
                    vname, value = ln.split("=", maxsplit=1)
                    vname = vname.strip()    ; value = value.strip()
                    if value == "none":   value = "None"
                    if vname == "tag":
                        self.tag_add(ArchiveSession.tag_parse(value))
                        continue

                    setattr(self, vname, 
                        int(value) if vname in self.attr_int else value)

            self.saved = self.loaded = True

        for tag in addtags:   self.tag_add(tag)


    def manifest_max(self):
        return self.volsize // self.archive.chunksize * ((256//4) + 20)


    def tag_parse(tag, delim=" "):
        result = tuple()    ; errs = []      ; parts = tag.strip().split(delim, maxsplit=1)
        tag_id = parts[0].strip().lower()    ; comment = parts[1].strip()

        if len(tag_id.encode("UTF-8")) > ArchiveSet.tag_len:
            errs.append("Error: Max "+ArchiveSet.tag_len+" size for tag ID.\n")
        if len(comment.encode("UTF-8")) > ArchiveSet.comment_len:
            errs.append("Error: Max "+ArchiveSet.comment_len+" size for comment.\n")
        if re.match(".*[,=\^]", tag_id) or any(map(str.isspace, tag_id)) \
        or not tag_id.isprintable():
            errs.append("Error: [^control], [space], and ',^=' not allowed in tag ID.\n")
        if not comment.isprintable():
            errs.append("Error: [^control] not allowed in tag comment.\n")
        if tag_id == "all":   errs.append("Error: tag 'all' is reserved.\n")
        sys.stderr.write("".join(errs))

        if not errs:
            result = (tag_id, "" if len(parts) == 1
                                    else parts[1].strip()[:ArchiveSet.comment_len])
        return result

    def tag_add(self, tag):
        if len(self.tags) >= ArchiveSet.max_tags:
            x_it(1, ArchiveSet.max_tags+" maximum tags.")
        self.saved = False    ; voltags = self.volume.tags    ; tid = tag[0]
        if tid not in self.tags:   self.tags[tid] = tag[1]
        if tid in voltags:
            voltags[tid].add(self.name)
        else:
            voltags[tid] = {self.name}
        return True

    def tag_del(self, tag):
        del(self.tags[tag])   ; voltags = self.volume.tags
        if tag in voltags:
            if self.name in voltags[tag]:   voltags[tag].remove(self.name)
            if len(voltags[tag]) == 0:    del(voltags[tag])

    def save_info(self, ext=""):
        assert self.path   ; fname = "info"   ; arch = self.volume.archive
        self.manifesthash = arch.encode_file(self.path+"/manifest"+ext,
                                             fdest=self.path+"/manifest.z"+ext)
        with io.StringIO() as f:
            for attr in self.attr_str+self.attr_int:
                print(attr, "=", getattr(self, attr), file=f)
            for tkey, tdesc in self.tags.items():
                print("tag =",   tkey, tdesc, file=f)
            buf = arch.compress(f.getvalue().encode("UTF-8"),
                                int(arch.compr_level))
            self.hashval = arch.b64hash(buf)

        if arch.mcrypto:   buf = arch.mcrypto.encrypt(buf)
        with open(pjoin(self.path,fname+ext), "wb") as f:
            f.write(buf)
            f.flush()    ; os.fsync(f.fileno())
        self.saved = self.loaded = True
        self.volume.save_volinfo(ext)

# END class ArchiveSet, ArchiveVolume, ArchiveSession


# DataCryptography(): Handle crypto functions and state for volume data

class DataCryptography:

    crypto_key_bits = 256        ; max_ct_bits = 128
    max_keyfile_sz  = ((crypto_key_bits*2) + max_ct_bits) * 2 // 8
    time_headroom   = int(60*60*24*365.25*50)             ; timesz  = 32 // 8

    # Matrix of recommended mode pairs = 'formatcode: (data, metadata)'
    # User selects a data cipher which is automatically paired w a metadata authentication cipher.
    crypto_codes    = {b"00":  ("off",                "off"),
                       b"10":  ("aes-256-siv",        "aes-256-siv"),
                       b"20":  ("aes-256-cbc",        "aes-256-siv"),
                       b"30":  ("xchacha20",          "xchacha20-poly1305"),
                       b"40":  ("xchacha20-poly1305", "xchacha20-poly1305")}


    def __init__(self, ci_type, keyfile, slot, passphrase, cadence=1, init=False):
        assert passphrase is None or type(passphrase) == bytearray
        assert cadence > 0

        if not issubclass(type(keyfile), io.IOBase):
            if not exists(keyfile) and init:   open(keyfile, "wb").close()
            keyfile =  open(keyfile, "r+b")

        self.keyfile = keyfile          ; self.ci_type = ci_type
        self.key     = None             ; kbits = self.crypto_key_bits
        self.slot    = slot             ; self.counter  = self.ctstart = None
        self.ctcadence = cadence        ; self.auth     = False
        self.timer   = time.time        ; self.get_rnd  = get_random_bytes

        if ci_type == "aes-256-cbc":
            # FIX: Probably requires IV encryption with different non-data key
            raise NotImplementedError("Needs work")
            if not (platform.machine() == "x86_64" and "aes" in cpu_flags):
                print("Warning: x86_64 + AES crypto sidechannel resistance not detected.")
            self.key_sz  = kbits//8                ; self.max_count = 2**48-64 # <--blocks
            self.iv_sz   = 16                      ; self.buf_start = self.iv_sz
            self.blk_sz  = Cipher_AES.block_size   ; self.ps_mask = self.blk_sz-1
            self.mode    = Cipher_AES.MODE_CBC     ; self.AES_new = Cipher_AES.new
            self.countsz = self.max_count.bit_length() // 8
            self.rndivsz = self.iv_sz - self.countsz
            self.encrypt = self._enc_aes_256_cbc_rndpad
            self.decrypt = self._dec_aes_256_cbc_rndpad

        elif ci_type == "aes-256-siv":
            if not (platform.machine() == "x86_64" and "aes" in cpu_flags):
                print("Warning: x86_64 + AES crypto sidechannel resistance not detected.")
            self.key_sz  = 2*kbits//8              ; self.max_count = 2**48-64
            self.nonce_sz= 12                      ; self.tag_sz    = 16
            self.buf_start = self.nonce_sz+self.tag_sz
            self.mode    = Cipher_AES.MODE_SIV     ; self.AES_new = Cipher_AES.new
            self.countsz = self.max_count.bit_length() // 8
            self.randomsz=self.nonce_sz - self.countsz
            self.encrypt = self._enc_aes_256_siv
            self.decrypt = self.auth = self._dec_aes_256_siv

        elif ci_type == "xchacha20":
            if Cryptodome.version_info[0:2] < (3,9):
                raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")
            self.key_sz  = kbits//8                ; self.max_count = 2**80-64
            self.nonce_sz= 24                      ; self.buf_start = self.nonce_sz
            self.countsz = self.max_count.bit_length() // 8
            self.randomsz=self.nonce_sz - self.countsz - self.timesz
            self.ChaCha20_new = Cipher_ChaCha20.new
            self.encrypt = self._enc_chacha20
            self.decrypt = self._dec_chacha20

        elif ci_type == "xchacha20-poly1305":
            if Cryptodome.version_info[0:2] < (3,9):
                raise RuntimeError("Cryptodome version >= 3.9 required for xchacha20 cipher.")
            self.key_sz  = kbits//8         ; self.max_count = 2**80-64    ; self.tag_sz = 16
            self.nonce_sz= 24               ; self.buf_start = self.nonce_sz + self.tag_sz
            self.countsz = self.max_count.bit_length() // 8
            self.randomsz=self.nonce_sz - self.countsz - self.timesz
            self.ChaCha20_Poly1305_new = Cipher_ChaCha20_Poly1305.new
            self.encrypt = self._enc_chacha20_poly1305
            self.decrypt = self.auth = self._dec_chacha20_poly1305

        else:
            raise ValueError("Invalid cipher spec "+ci_type)

        self.slot_offset = (kbits*2//8 + self.max_ct_bits//8) * self.slot

        # Load counter and key
        if init:
            self.key     = self.new_key(passphrase)
        else:
            keyfile.seek(self.slot_offset)    ; ctbytes = keyfile.read(self.countsz)
            self.counter = self.ctstart = cadence - 1 + int.from_bytes(ctbytes, "big")
            salt         = keyfile.read(self.key_sz)   ; assert len(salt) == self.key_sz
            self.key     = self.derive_key(salt, passphrase)

    def __del__(self):
        if self.counter and self.counter > self.ctstart:   self.save_counter()
        if self.key:   clear_array(self.key)


    # Key file binary format: counter=8B, key=key_sz
    def new_key(self, passphrase):
        salt = self.get_rnd(self.key_sz)      ; self.counter = self.ctstart = 0
        self.keyfile.seek(self.slot_offset)   ; self.keyfile.write(bytes(self.countsz) + salt)
        return self.derive_key(salt, passphrase)

    def derive_key(self, salt, passphrase):
        key = bytearray(hashlib.scrypt(passphrase, salt=salt, n=2**19, r=8, p=1,
                                        maxmem=640*1024*1024, dklen=self.key_sz))
        clear_array(passphrase)
        return key

    def save_counter(self):
        self.keyfile.seek(self.slot_offset)
        self.keyfile.write(self.counter.to_bytes(self.countsz, "big"))

    # Encrypt aes-256-cbc:
    # A random 'bolster' block is added as a prefix to the plaintext buffer;
    # A random padding is also used and 'buf' must be prefixed by len % blk_sz (1 byte)
    def _enc_aes_256_cbc_rndpad(self, buf):
        blk_sz = self.blk_sz    ; pad_sz = blk_sz - (len(buf) % blk_sz) & self.ps_mask
        if pad_sz:   buf += self.get_rnd(pad_sz)

        plaintxt= self.get_rnd(blk_sz) + buf    ; self.counter += len(plaintxt) // blk_sz
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        iv      = self.get_rnd(self.rndivsz) + self.counter.to_bytes(self.countsz, "big")
        cipher  = self.AES_new(self.key, self.mode, iv=iv)
        buf     = cipher.encrypt(plaintxt) + pad_sz.to_bytes(1,"big")
        self.save_counter()
        return iv + buf

    # Decrypt aes-256-cbc:
    def _dec_aes_256_cbc_rndpad(self, untrusted_buf):
        iv = untrusted_buf[:self.iv_sz]    ; blk_sz = self.blk_sz
        pad_sz = untrusted_buf[-1] & self.ps_mask

        cipher = self.AES_new(self.key, self.mode, iv)
        untrusted_buf = cipher.decrypt(untrusted_buf[self.iv_sz:-1])

        return  untrusted_buf[blk_sz: -pad_sz if pad_sz else None]

    # Encrypt aes-256-siv:
    def _enc_aes_256_siv(self, buf):
        self.counter += 1
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        nonce  = self.get_rnd(self.randomsz) + self.counter.to_bytes(self.countsz, "big")
        cipher = self.AES_new(self.key, self.mode, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        return  nonce + ci_tag + buf

    # Decrypt aes-256-siv:
    def _dec_aes_256_siv(self, untrusted_buf):
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.AES_new(self.key, self.mode, nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)

    # Encrypt [X]ChaCha20:
    def _enc_chacha20(self, buf):
        self.counter += 1
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = (int(self.timer()) - self.time_headroom).to_bytes(self.timesz, "big") \
               + self.get_rnd(self.randomsz) \
               + self.counter.to_bytes(self.countsz, "big")
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        buf    = cipher.encrypt(buf)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        return  nonce + buf

    # Decrypt [X]ChaCha20:
    def _dec_chacha20(self, untrusted_buf):
        nonce  = untrusted_buf[:self.nonce_sz]
        cipher = self.ChaCha20_new(key=self.key, nonce=nonce)
        return cipher.decrypt(untrusted_buf[self.nonce_sz:])

    # Encrypt [X]ChaCha20-Poly1305:
    def _enc_chacha20_poly1305(self, buf):
        self.counter += 1
        if self.counter > self.max_count:   raise ValueError("Key exhaustion.")
        # Nonce composed from: 32bit current time offset + 80bit rnd + 80bit counter
        nonce  = (int(self.timer()) - self.time_headroom).to_bytes(self.timesz, "big") \
               + self.get_rnd(self.randomsz) \
               + self.counter.to_bytes(self.countsz, "big")
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        if self.counter % self.ctcadence == 0:   self.save_counter()
        return  nonce + ci_tag + buf

    # Decrypt [X]ChaCha20-Poly1305:
    def _dec_chacha20_poly1305(self, untrusted_buf):
        nonce  = untrusted_buf[:self.nonce_sz]
        ci_tag = untrusted_buf[self.nonce_sz:self.buf_start]
        cipher = self.ChaCha20_Poly1305_new(key=self.key, nonce=nonce)
        return cipher.decrypt_and_verify(untrusted_buf[self.buf_start:], ci_tag)


# Define absolute paths of commands

class CP:
    awk    = "/usr/bin/awk"     ; sed   = "/bin/sed"        ; sort     = "/usr/bin/sort"
    cat    = "/bin/cat"         ; mkdir = "/bin/mkdir"      ; python   = "/usr/bin/python3"
    mv     = "/bin/mv"          ; grep  = "/bin/grep"       ; ssh      = "/usr/bin/ssh"
    sh     = "/bin/sh"          ; tar   = "/bin/tar"        ; tail     = "/usr/bin/tail"
    rm     = "/bin/rm"          ; lvm   = "/sbin/lvm"       ; qvm_run  = "/usr/bin/qvm-run"
    tee    = "/usr/bin/tee"     ; sync  = "/bin/sync"       ; dmsetup  = "/sbin/dmsetup"
    chattr = "/usr/bin/chattr"  ; xargs = "/usr/bin/xargs"  ; sha256sum= "/usr/bin/sha256sum"
    cmp    = "/usr/bin/cmp"     ; gzip  = "/bin/gzip"       ; diff     = "/usr/bin/diff"
    mountpoint = "/bin/mountpoint"    ; thin_delta = "/usr/sbin/thin_delta"
    blkdiscard = "/sbin/blkdiscard"   ; nice       = "/usr/bin/nice"


class Lvm_VolGroup:
    def __init__(self, name):
        self.name  = name
        self.lvs   = {}
        self.poolnames = set()
        self.gc_procs = []
        self.clean = options.clean

    def __del__(self):
        if self.clean:
            for p in self.gc_procs:   p.wait()


class Lvm_Volume:
    colnames  = ["vg_name","lv_name","lv_attr","lv_size","lv_time","lv_uuid",
                 "pool_lv","thin_id","tags"]
    attr_ints = ["lv_size"]
    __slots__ = colnames

    def __init__(self, members):
        for attr in self.colnames:
            val = members[self.colnames.index(attr)]
            if attr == "tags":
                setattr(self, attr, val.split(","))   ; uuid = None
            else:
                setattr(self, attr, int(re.sub("[^0-9]", "", val)) if attr \
                    in self.attr_ints else val)

        if uuid:   setattr(self, "lv_uuid", uuid)


# Retrieves survey of all LVs as vgs[].lvs[] dicts

def get_lvm_vgs(vg_to_lvols=None):
    global volgroups, l_vols
    l_vols.clear()    ; volgroups.clear()

    if not admin_permission:   return
    if not shutil.which(CP.lvm):   sys.stderr.write("LVM not available.\n"); return
    do_exec([[CP.lvm, "lvs", "--units=b", "--noheadings", "--separator=::",
                "--options=" + ",".join(Lvm_Volume.colnames)]],
            out=tmpdir+"/volumes.lst")

    with open(tmpdir+"/volumes.lst", "r") as vlistf:
        for ln in vlistf:
            members = ln.strip().split("::")
            vgname = members[0] # Fix: use colname index
            lvname = members[1]    ; lv = Lvm_Volume(members)
            if vgname not in volgroups.keys():
                volgroups[vgname] = Lvm_VolGroup(vgname)
            volgroups[vgname].lvs[lvname] = lv
            if lv.pool_lv:   volgroups[vgname].poolnames.add(lv.pool_lv)

    if vg_to_lvols:  l_vols = volgroups[vg_to_lvols].lvs


def lv_exists(vgname, lvname):
    return vgname in volgroups.keys() \
            and lvname in volgroups[vgname].lvs.keys()


def vg_exists(vgname):
    if not volgroups:   return False
    try:
        do_exec([[CP.lvm, "vgdisplay", vgname]])
    except subprocess.CalledProcessError:
        return False
    else:
        return True


# Converts a non-cannonical LV path to LV name plus pool and vg names.
def get_lv_path_pool(path):
    if not volgroups:   return "", "", ""
    try:
        p = subprocess.run([CP.lvm, "lvs", "--separator=::", "--noheadings",
                            "--options=lv_name,pool_lv,vg_name", path], check=True,
                            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
    except:
        return "", "", ""
    else:
        return p.stdout.decode("utf-8").strip().split("::")


def lv_remove(vgname, lvname, sync=True, check=False):
    # Enh: re-write with asyncio
    assert not (sync == False and check)
    if lv_exists(vgname, lvname):
        del(volgroups[vgname].lvs[lvname])
        procs = volgroups[vgname].gc_procs    ; maxprocs = 16
        if len(procs) == maxprocs:
            clean = options.clean
            for ii in reversed(range(len(procs))):
                retcode = procs[ii].returncode
                if retcode is not None:
                    if clean and retcode != 0:
                        raise CalledProcessError("lvremove failed "+str(retcode))
                    del(procs[ii])

        cmds = [CP.sh, "-c", CP.lvm + " lvchange -p rw " + vgname+"/"+lvname + " ; "  \
               + CP.blkdiscard + " /dev/" + vgname+"/"+lvname    + " ; "  \
               + CP.lvm + " lvremove -f " + vgname+"/"+lvname]
        if not (sync or options.clean):   cmds.insert(0, CP.nice)
        p = subprocess.Popen(cmds, shell=False,
                             stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        if not sync and len(procs) < maxprocs:
            procs.append(p)
        else:
            retcode = p.wait()
            if check and retcode != 0:   raise CalledProcessError("lvremove failed "+str(retcode))


def lv_rename(vgname, vol1, vol2):
    do_exec([[CP.lvm, "lvrename", vgname+"/"+vol1, vol2]])    ; lvs = volgroups[vgname].lvs
    lvs[vol2] = lvs[vol1]    ; lvs[vol2].lv_name = vol2    ; del(lvs[vol1])


# Reserve or release lvm thinpool metadata snapshot.
# arch must be a configured ArchiveSet; action must be "reserve" or "release".
# Specifying "release" without "pool=" will attempt to release all pools.
def lvm_meta_snapshot(arch, action, pool=None):
    if arch.vgname not in volgroups:   return
    vgname   = arch.vgname.replace("-","--")      ; checkerr = action == "reserve"
    the_pool = pool if pool else arch.poolname    ; vgpools  = volgroups[arch.vgname].poolnames

    for pool in vgpools & vgpools if not pool and action == "release" else {the_pool}:
        poolname = pool.replace("-","--")
        do_exec([[CP.dmsetup,"message", vgname+"-"+poolname+"-tpool",
                "0", action+"_metadata_snap"]], check=checkerr)


# Try to sync only selected filesystem
def fssync(path):
    if options.maxsync:   subprocess.Popen([CP.sync,"-f",path])


def ask_passphrase(verify=False, recur=0):
    if recur > 5:   x_it(1, "Max retries on input.")
    passphrase  = bytearray(getpass.getpass("Enter passphrase: "), encoding="UTF-8")
    if verify and not options.unattended:
        if len(passphrase) < 10:
            print("Passphrase must be 10 or more characters.")
            clear_array(passphrase)
            passphrase = ask_passphrase(verify=True, recur=recur+1)
        passphrase2 = bytearray(getpass.getpass("Re-enter passphrase: "), encoding="UTF-8")
        if passphrase != passphrase2:
            clear_array(passphrase)    ; passphrase=None
            x_it(1, "Entries do not match.")
        clear_array(passphrase2)
    return passphrase


def clear_array(ar):
    for ii in range(len(ar)):   ar[ii] = 0


# Initialize a new ArchiveSet:

def arch_init(aset, opts):
    if not opts.local or not opts.dest:
        x_it(1,"--local and --dest are required together.")

    if opts.local:
        vgname, poolname = opts.local.split("/")
        aset.vgname   = vgname
        aset.poolname = poolname

    if opts.from_arch:
        return

    os.makedirs(aset.path, exist_ok=True)
    aset.data_cipher = opts.encrypt.lower()
    # Fix: duplicates code in aset... move to aset class.
    if aset.data_cipher in (x[0] for x in DataCryptography.crypto_codes.values()): 
        aset.ci_mode, ci= [(x,y) for x,y in DataCryptography.crypto_codes.items()
                                 if y[0] == aset.data_cipher][0]

        if aset.data_cipher != "off":
            ##if opts.unattended:   x_it(1, "Enter passphrase interactively.")
            # Security Enh: Possibly use mmap+mlock to store passphrase/key values,
            #               and wipe them from RAM after use.
            passphrase      = ask_passphrase(verify=True)
            aset.mcrypto    = DataCryptography(ci[1], aset.confpath+".salt", slot=1,
                                            passphrase=passphrase[:], init=True)
            aset.datacrypto = DataCryptography(aset.data_cipher, aset.mcrypto.keyfile, slot=0,
                                            passphrase=passphrase, init=True)
    else:
        x_it(1,"Error: Invalid cipher option.")

    print(); print("Encryption    :", aset.data_cipher)

    if opts.hashtype:
        if opts.hashtype not in hash_funcs:
            x_it(1, "Hash function '"+opts.hashtype+"' is not available on this system.")
        aset.hashtype = opts.hashtype

    print("Hashing       :", aset.hashtype)

    if opts.compression:
        if ":" in opts.compression:
            compression, compr_level = opts.compression.strip().split(":")
        else:
            compression = opts.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not is_num(compr_level):
            x_it(1, "Invalid compression spec.")
        aset.compression = compression      ; aset.compr_level = compr_level

    print("Compression   : %s:%s" % (aset.compression, aset.compr_level))

    if opts.chfactor:
        # accepts an exponent from 1 to 6
        if not ( 0 < opts.chfactor < 7 ):
            x_it(1, "Requested chunk size not supported.")
        aset.chunksize = (bkchunksize//2) * (2** opts.chfactor)
        if aset.chunksize > 256 * 1024:
            print("Large chunk size set:", aset.chunksize)

    aset.save_conf()


# Check/verify an entire archive

def arch_check(vol_list=None, startup=False):
    gethash  = hash_funcs[aset.hashtype]   ; chunksize = aset.chunksize
    attended = not options.unattended      ; stray_dirs= []
    compare_digest = hmac.compare_digest   ; b64enc    = base64.urlsafe_b64encode

    decrypt  = aset.datacrypto.decrypt if aset.datacrypto else None

    vol_list = vol_list if vol_list else list(aset.vols.keys())
    vol_dirs = set((x.name for x in os.scandir(aset.path) \
                            if x.is_dir() and x.name.startswith("Vol_")))

    vdir_strays = set(aset.conf["volumes"].keys()) - vol_dirs
    if vdir_strays:
        print("Stray volume dirs:", vdir_strays)
        # Enh: offer to remove/recoved strays

    # Remove orphan snapshots
    for lv in list(l_vols.values()):
        if "wyng" in lv.tags and lv.lv_name.endswith((".tick",".tock")):
            basename = lv.lv_name[:-5]
            if options.clean and basename not in l_vols.keys():
                print("Removing orphan snapshot:", lv.lv_name)
                lv_remove(lv.vg_name, lv.lv_name, check=True, sync=True)

    # Check volume contents at various levels (dirs, metadata, content)
    for volname in vol_list if dest.online else []:
        if volname not in aset.vols:   continue

        vol = aset.vols[volname]
        if not startup or debug:   print("Volume", volname, flush=True)

        # Remove session tmp dirs
        for sdir in os.scandir(vol.path):
            if sdir.name.startswith("S_") and sdir.name.endswith("-tmp"):
                if debug:   print("Removing partial session dir '%s'" % sdir.name)
                dest.run([dest.cd+bkdir + " && rm -rf " + vol.vid+"/"+sdir.name])
                shutil.rmtree(vol.path+"/"+sdir.name, ignore_errors=True)

        if startup:   continue

        # Check ses sequence number is unique and combined manifests are correct
        print("  Checking indexes,", end="", flush=True) ; mset = []
        assert len(vol.sessions) == len(set((x.sequence for x in vol.sessions.values())))
        for ses in vol.sesnames:   mset.append(ses)    ; check_manifest_sequence(volname, mset)

        # Check hashes of each session individually
        print(" data:")
        for sesname in reversed(vol.sesnames):
            if options.session and ((options.session.lower() == "newest" \
            and sesname != vol.sesnames[-1]) or options.session < vol.sessions[sesname].localtime):
                continue # Enh: use seq

            if attended:   print(" ", sesname[2:], end="... ", flush=True)
            bcount = receive_volume(vol.name, select_ses=sesname[2:], verify_only=2)
            #if attended:   print(bcount, "bytes OK")


def check_manifest_sequence(datavol, sesnames):
    vol = aset.vols[datavol]     ; volsize = vol.sessions[sesnames[-1]].volsize
    with open(merge_manifests(datavol, msessions=sesnames, addcol=False), "r") as mrgf:
        for addr in range(0, volsize, aset.chunksize):
            ln = mrgf.readline().strip()
            if not ln:   break
            ln1, ln2 = ln.split()
            if ln1 != "0":
                assert len(ln1) == aset.mhash_sz    ; h1 = base64.urlsafe_b64decode(ln1)
            assert len(ln2) == aset.sesname_sz      ; a1 = int("0"+ln2, 16)
            if addr != a1:
                print(ln); raise("Manifest seq error. Expected %d got %d." % (addr, a1))

    if addr+aset.chunksize != volsize:
        raise ValueError("Manifest range stopped short at", addr)


# Get global configuration settings:

def get_configs(opts):
    global dest
    arch_name = "default"    ; bkdir = topdir + "/" + arch_name
    load_children = 1 if opts.action in ("arch-delete","delete","add") else 2

    dspec = opts.from_arch if opts.from_arch else opts.dest
    dest = Destination(dspec, opts.dest_name, opts.dest)
    if (opts.action not in local_actions and dest.sys is not None) \
    or opts.remap or opts.from_arch:
        dest.detect_state(arch_name, opts.dedup)

    # Check online status for certain commands.
    if not dest.online and (opts.remap or opts.from_arch \
    or opts.action not in local_actions) and not (opts.action == "delete" and opts.clean):
        x_it(1, "Destination not ready to receive commands.")
    if not dest.writable and opts.action in write_actions and not opts.from_arch \
    and not (opts.action == "delete" and opts.clean):
        x_it(1, "Destination not writable.")
    if dest.archive_ini_hash == "none" and opts.action not in local_actions+("arch-init",):
        x_it(1,"Archive not found at '%s'" % dest.spec)

    #### Test dest.archive_ini_hash here:

    if opts.from_arch:
        # Prepare a temporary metadata dir and init aset with it
        tmpmeta   = big_tmpdir+"/var"    ; os.makedirs(tmpmeta+bkdir)
        aset_from = ArchiveSet(arch_name, tmpmeta+topdir, dest)
        arch_init(aset_from, opts)
        return aset_from, dest

    aset = ArchiveSet(arch_name, metadir+topdir, dest, children=load_children)
 
    if opts.action == "arch-init" and aset.updated_at is None:
        arch_init(aset, opts)
    elif opts.action == "arch-init":
        x_it(1, "Archive already initialized for "+aset.name)
    if aset.updated_at is None:
        x_it(1,"Archive not found.")

    return aset, dest


def get_configs_remote(dest, aname, recv_dir):
    make_local = options.action == "arch-init"

    recv_list = [(ArchiveSet.confname,  ArchiveSet.max_conf_sz),
                 (ArchiveSet.confname+".salt", DataCryptography.max_keyfile_sz)]
    fetch_file_blobs(recv_list, recv_dir, dest, ext=".auth_req", skip0=True)

    # Instantiate ArchiveSet to authenticate archive.ini
    for fname, fsz in recv_list:   os.replace(recv_dir+"/"+fname+".auth_req", recv_dir+"/"+fname)
    new_aset = ArchiveSet(aname, os.path.dirname(recv_dir), dest, children=0)

    # Initial auth successful! Fetch + auth volume metadata...
    recv_list= [(x+"/volinfo", ArchiveSet.max_volinfosz)
                 for x in new_aset.conf["volumes"].keys()]
    fetch_file_blobs(recv_list, recv_dir, dest)
    do_exec([[CP.chattr, "+c"] + list(new_aset.conf["volumes"].keys())], cwd=recv_dir)
    new_aset = ArchiveSet(aname, os.path.dirname(recv_dir), dest, children=1, allvols=True,
                          prior_auth=new_aset)

    # Fetch + auth session metadata... use list flattener
    ses_list =[(x.name, x.volume.name) for sub in
               (vol.sessions.values() for vol in new_aset.vols.values()) for x in sub]
    fetch_file_blobs([(new_aset.vols[y].vid+"/"+x+"/info",
                       ArchiveSet.max_infosz) for x,y in ses_list], recv_dir, dest)
    new_aset = ArchiveSet(new_aset.name, os.path.dirname(new_aset.path), dest,
                          children=2, allvols=True, prior_auth=new_aset)

    fetch_file_blobs([(new_aset.vols[y].vid+"/"+x+"/manifest.z",
                       new_aset.vols[y].sessions[x].manifest_max()) for x,y in ses_list],
                        recv_dir, dest)
    for vol in new_aset.vols.values():   vol.decode_manifests(vol.sesnames, force=True)

    if make_local:
        if exists(metadir+bkdir) :  shutil.rmtree(metadir+bkdir)
        new_path = metadir+topdir;  shutil.move(recv_dir, new_path)
        new_aset = ArchiveSet(aname, new_path, dest, prior_auth=new_aset)
    else:
        new_path = os.path.dirname(recv_dir)

    # Configure final ArchiveSet object with imported config & current dest+local
    update = False
    if options.local:
        vgname, poolname = options.local.split("/")
        new_aset.vgname   = vgname    ; new_aset.poolname = poolname    ; update = True

    return new_aset


class Destination:

    url_types   = ("ssh", "file", "qubes-ssh", "qubes")

    ssh_opts    = ["-x", "-o", "ControlPath=~/.ssh/controlsocket-%r@%h-%p",
                         "-o", "ControlMaster=auto", "-o", "ControlPersist=60",
                         "-o", "ServerAliveInterval=30", "-o", "ConnectTimeout=30",
                         "-o", "Compression=no"]

    magic       = b"\xff\x11\x15"

    def __init__(self, dspec, dname, dest):

        # fetch locations list, dest spec
        dest_url = dspec    ; locs = self.load_locations()
        if dname:
            if re.match(".*[,=\^]", dname) or any(map(str.isspace, dname)) \
            or not dname.isprintable():
                x_it(1,"Error: [^control], [space], and ',^=' not allowed in dest-name.")
            if not dest_url and dname in locs:
                dest_url = locs[dname]
        elif not dspec and "default" in locs:
            dest_url = locs["default"]

        # parse and validate dest spec
        if not dest_url:                 x_it(1,"Error: Missing dest specification.")
        if not dest_url.isprintable():   x_it(1,"Error: [^control] not allowed in dest.")
        dparts      = urlparse(dest_url)    ; self.dtype = dtype = dparts.scheme
        if dtype not in self.url_types:
            x_it(1,"'%s' not an accepted type." % destsys)
        if (dtype == "file" and (dparts.netloc or not dparts.path)) \
        or (dtype in ("ssh","qubes","qubes-ssh") and not dparts.netloc) \
        or (dtype == "qubes-ssh" and not all(dparts.netloc.partition(":"))) :
            x_it(1,"Error: Malformed --dest specification.")

        self.spec   = dest_url
        self.sys    = dparts.netloc
        self.path   = os.path.normpath(dparts.path)
        self.cd     = " cd '"+self.path+"'"
        self.dtmp   = None
        self.free   = None
        self.online = self.writable  =  False
        self.archive_ini_hash = "none"

        self.run_map = {"file":       [CP.sh],
                        "ssh":        [CP.ssh] + self.ssh_opts + [self.sys],
                        "qubes":      [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                      "-p", self.sys],
                        "qubes-ssh":  [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                      "-p", self.sys.split(":")[0]]
                        }

        # save locations change
        if dspec and dname:
            locs[dname] = dest
            self.save_locations(locs)    ; del(locs)


    def get_free(self, fpath): ## Enh: add sanitize
        for ln in open(fpath,"r").readlines():
            if ln.startswith("wyng_check_free"):  self.free = int(ln.split()[1])

    # Run system commands on destination

    def run(self, commands, dest_type=None,
            infile="", inlines=None, out="", check=True, trap=False):

        if dest_type is None:   dest_type = self.dtype

        cmd = self.run_args(commands, trap=trap)
        return do_exec([cmd], infile=infile, inlines=inlines, out=out, check=check)

    def load_locations(self):
        if not exists(vardir+"/dests"):   return {}
        with open(vardir+"/dests", "r") as fl:
            loc = { x: y for x, y in (w.strip().split(" ",maxsplit=1) for w in fl.readlines()) }

        return loc

    def save_locations(self, locs):
        with open(vardir+"/dests", "w") as fl:
            for x, y in locs.items():   print(x, y, file=fl)

    # Build command lists that can be shunted to remote systems.
    # The input commands are stored in a temp file and a standard command that
    # runs the temp file is returned.

    def run_args(self, commands, trap=False):
        trapcmd = "trap '' INT TERM QUIT ABRT ALRM TSTP USR1\n" if trap else ""
        dest_type = self.dtype
        # shunt commands to tmp file
        with tempfile.NamedTemporaryFile(dir=tmpdir, delete=False) as tmpf:
            cmd = bytes(trapcmd + shell_prefix
                        + " ".join(commands) + "\n", encoding="UTF-8")
            tmpf.write(cmd)
            remotetmp = os.path.basename(tmpf.name)

        if dest_type in {"qubes","qubes-ssh"}:
            do_exec([[CP.qvm_run, "--no-color-stderr", "--no-color-output", "-p",
                    (self.sys if dest_type == "qubes" else self.sys.split(":")[0]),
                    CP.mkdir+" -p "+tmpdir
                    +" && "+CP.cat+" >"+pjoin(tmpdir,remotetmp)
                    ]], infile=pjoin(tmpdir,remotetmp))
            if dest_type == "qubes":
                add_cmd = [CP.sh+" "+pjoin(tmpdir,remotetmp)]
            else:
                add_cmd = [CP.ssh+" "+" ".join(self.ssh_opts)+" "+self.sys.split(":")[1]
                        +' "$('+CP.cat+' '+pjoin(tmpdir,remotetmp)+')"']

        elif dest_type == "ssh":
            #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
            add_cmd = [cmd]

        elif dest_type == "file":
            add_cmd = [pjoin(tmpdir,remotetmp)]

        return self.run_map[dest_type] + add_cmd


    def write_helper_program(self):

        dest_program = \
'''#  Copyright Christopher Laprise 2018-2022
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, signal, shutil, subprocess, gzip, tarfile
cmd = sys.argv[1]   ; msync = "--sync" in sys.argv
tmpdir = os.path.dirname(os.path.abspath(sys.argv[0]))  
exists = os.path.exists    ; replace = os.replace    ; remove = os.remove

def fssync(path):
    if msync:   subprocess.Popen(["sync","-f",path])

def catch_signals():
    for sig in (signal.SIGINT, signal.SIGTERM, signal.SIGQUIT, signal.SIGABRT, signal.SIGALRM,
                signal.SIGTSTP, signal.SIGUSR1):
        signal.signal(sig, signal.SIG_IGN)    ; signal.siginterrupt(sig, False)

def helper_send():
    mkdirs = os.makedirs    ; hlink = os.link    ; dirname = os.path.dirname
    with tarfile.open(mode="r|", fileobj=sys.stdin.buffer) as tarf:
        extract = tarf.extract    ; substitutions = {}    ; dirlist = set()
        for member in tarf:   sdir = member.name    ; mkdirs(sdir)    ; print(sdir)    ; break
        for member in tarf:
            if not member.islnk():
                extract(member, set_attrs=False)
            else:
                source = src_orig = member.linkname   ; dest = member.name   ; ddir = dirname(dest)
                if source in substitutions:   source = substitutions[source]
                if ddir not in dirlist:   mkdirs(ddir, exist_ok=True)   ; dirlist.add(ddir)
                try:
                    hlink(source, dest)
                except OSError as err:
                    print(err)    ; print("Substitution:", source, dest)
                    shutil.copyfile(source, dest)    ; substitutions[src_orig] = dest

def helper_receive(lstf):
    stdout_write = sys.stdout.buffer.write   ; exists = os.path.exists   ; getsize= os.path.getsize
    stdout_flush = sys.stdout.flush          ; magic  = b"\\xff\\x11\\x15"
    for line in lstf:
        fname = line.strip()
        if not fname:   break
        fsize = getsize(fname) if exists(fname) else 0
        stdout_write(magic + fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:   stdout_write(dataf.read(fsize))
        stdout_flush()

def helper_merge():
    exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
    try:
        if resume:
            if exists("merge-init") or not exists("merge"):
                raise RuntimeError("Merge: Init could not complete; Aborting merge.")
        else:
            print("Merge: Initialization.")
            for f in (target+"/info", target+"/manifest.z", "volinfo", "archive.ini"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")    ; fssync(".")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):   replace(i.path, i.name)
        fssync(".")    ; print(err)    ; sys.exit(50)
    try:
        os.chdir("merge")  #  CD
        if not resume or not exists("CHECK-mv-rm"):
            print("Merge: remove/replace files.")    ; subdirs = set()
            for src in src_list:  # Enh: replace os.scandir w manifest method
                for i in os.scandir(src):
                    if i.is_dir():   subdirs.add(i.name)
            for sdir in subdirs:   os.makedirs(merge_target+"/"+sdir, exist_ok=True)
            for line in lstf:
                ln = line.split() # default split() does strip()
                if ln[0] == "rename" and (not resume or exists(ln[1])):
                    replace(ln[1], ln[2])
                elif ln[0] == "-rm" and exists(ln[1]):
                    remove(ln[1])
            open("CHECK-mv-rm","w").close()
    except Exception as err:
        print(err)    ; sys.exit(60)

def helper_merge_finalize():
    try:
        print("Merge: Finalize target")
        os.chdir("merge")  #  CD                 ; open("CHECK-start-finalize","w").close()
        for f in ("/info", "/manifest.z"):
            if not resume or exists(target+f+".tmp"):  replace(target+f+".tmp", merge_target+f)
        os.chdir("..")     #  CD
        if not resume or not exists(target):         replace("merge/"+merge_target, target)
        if not resume or exists("volinfo.tmp"):      replace("volinfo.tmp", "volinfo")
        if not resume or exists("archive.ini.tmp"):  replace("archive.ini.tmp", "../archive.ini")
        if not exists(target):   raise FileNotFoundError(target)
        fssync(".")
    except Exception as err:
        print(err)    ; sys.exit(70)
    shutil.rmtree("merge", ignore_errors=True)
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

## MAIN ##
if "--finalize" in sys.argv:   catch_signals()
if cmd not in ("merge","merge-finalize","send","rename") and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
else:
    lstf = None
if cmd == "merge":
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    src_list = []    ; resume = "--resume" in sys.argv
    lstf = gzip.open("merge.lst.gz", "rt")
    merge_target, target = lstf.readline().split()
    while True:
        ln = lstf.readline().strip()
        if ln == "###":  break
        src_list.append(ln)
    na = helper_merge_finalize() if "--finalize" in sys.argv else helper_merge()
elif cmd == "receive":
    helper_receive(lstf if lstf else sys.stdin)
elif cmd == "send":
    helper_send()
elif cmd == "dedup":
    ddcount = 0    ; substitutions = {}
    for line in lstf:
        source, dest = line.split()   ; src_orig = source    ; deststat = os.stat(dest)
        ddcount += deststat.st_size
        if source in substitutions:   source = substitutions[source]
        if os.stat(source).st_ino != deststat.st_ino:
            try:
                os.link(source, dest+"-lnk")    ; replace(dest+"-lnk", dest)
            except OSError as err:
                if err.errno == 31:
                    # source has too many links; substitute
                    substitutions[src_orig] = dest   ; ddcount -= deststat.st_size   ; continue
                else:
                    if exists(dest+"-lnk"):   remove(dest+"-lnk")
                    print(err)   ; raise err
    print(ddcount, "bytes reduced.")
    '''
        with open(tmpdir +"/rpc/dest_helper.py", "wb") as progf:
            progf.write(bytes(dest_program, encoding="UTF-8"))

    #####>  End helper program  <#####


    def detect_state(self, aset_name, dedup):

        tmpdigits = 12      ; tmpprefix = "/tmp/wyngrpc-"
        self.write_helper_program()

        if self.dtype == "qubes-ssh":
            # fix: possibly remove dargs and use dest.run()
            dargs = self.run_map["qubes"][:-1] + [self.sys.split(":")[0]]

            cmd = dargs + [shell_prefix \
                +CP.rm+" -rf "+tmpdir+"  &&  mkdir -p "+tmpdir
                ]
            do_exec([cmd])

        rdir = "'"+self.path+topdir+"/"+aset_name+"'"
        cmd = ["mkdir -p "+rdir+" && cd "+rdir

                # send helper program to remote dest
                +"  && tdir=$(mktemp -d " + tmpprefix + ("X"*tmpdigits) + ") && echo $tdir"
                +"  && cat >$tdir/dest_helper.py"

                # check free space and archive.ini status on remote
                +"  && echo -n 'wyng_check_free ' && stat -f -c '%a %S' ."
                +"  && echo -n 'wyng_archive_ini '"
                +"  && { if [ -e archive.ini ]; then sha256sum archive.ini; else echo none; fi }"

                # test write access and hardlinks
                +"  && touch archive.dat && echo 'wyng_writable'"
                +(" && ln -f archive.dat .hardlink" if dedup else "")
                ]
        try:
            do_exec([self.run_args(cmd),
                    # sanitize remote output:
                    [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]
                    ],
                    out=tmpdir+"/dest-state.log", infile=tmpdir+"/rpc/dest_helper.py")
            online = True
        except subprocess.CalledProcessError:
            online = False

        if online:
            with open(tmpdir+"/dest-state.log","r") as logf:
                for ln in logf:
                    if ln.startswith("wyng_archive_ini"):
                        self.archive_ini_hash = ln.split()[1]
                    elif ln.startswith("wyng_check_free"):
                        parts = ln.split()    ; self.free = int(parts[1]) * int(parts[2])
                    elif ln.startswith("wyng_writable"):
                        self.writable = True
                    elif ln.startswith(tmpprefix):
                        self.dtmp = ln.strip()
            if not self.dtmp or len(self.dtmp) != len(tmpprefix)+tmpdigits \
            or not set(self.dtmp[5:]) <= set(string.ascii_letters + string.digits + "-"):
                raise ValueError("Missing or malformed tmp dir: "+repr(self.dtmp))

        self.online = self.free is not None


# Run system commands with pipes, without shell:
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'. 'inlines' can be a list-like collection of strings
# to be used as input instead of 'infile'.
# List of commands may include 'None' instead of a child list; these will be ignored.

def do_exec(commands, cwd=None, check=True, out="", infile="", inlines=[], text=False):
    ftype   = "t" if text else "b"
    outmode = "a" if out.startswith(">>") else "w"    ; out = out.lstrip(">>")
    if cwd and out and out[0] != "/":   out = pjoin(cwd,out)
    outfunc = gzip.open if out.endswith(".gz") else open
    outf    = outfunc(out, outmode+ftype) if out else subprocess.DEVNULL

    if inlines:
        inf = subprocess.PIPE  #io.StringIO("\n".join(inlines)+"\n")
    else:
        if cwd and infile and infile[0] != "/":   infile = pjoin(cwd,infile)
        infunc  = gzip.open if infile.endswith(".gz") else open
        inf     = infunc(infile, "r"+ftype) if infile else subprocess.DEVNULL

    errf = open(tmpdir+"/err.log", "a")  ; print("--+--", file=errf)
    commands = [x for x in commands if x is not None]

    # Start each command, linking them via pipes
    procs = []
    for i, clist in enumerate(commands):
        p = subprocess.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else subprocess.PIPE,
                             stderr=errf)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)

    if inlines:   procs[0].communicate(("\n".join(inlines)+"\n").encode("UTF-8"))
    # Monitor and control processes
    while True:
        err = None    ; finish = timeout = False
        for p1 in reversed(procs):
            retcode = p1.poll()
            if not finish and retcode is None:
                try:
                    p1.wait(timeout=2)
                except subprocess.TimeoutExpired:
                    timeout = True
                    continue
                retcode = p1.returncode   ; finish = True
                if check and (retcode != 0):
                    err = p1              ; finish = True
            elif finish and retcode is None:
                p1.terminate()
                continue

        if err or not timeout:
            break

    for f in [inf, outf, errf]:
        if type(f) is not int: f.close()
    if err and check:
        raise subprocess.CalledProcessError(err.returncode, err.args)

    return procs[-1].returncode


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Session objects.
# Returns False if local and dest hashes match.

def compare_files(arch, pathlist=[], volumes=[], sessions=[], clear=True, manifest=False):
    cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions) and not x.meta_checked]
    realses   = [x for x in sessions if not x.meta_checked]
    if len(volumes)+len(sessions)+len(pathlist) == 0:   return False

    with open(cmp_list, "a") as flist:
        for pth in pathlist:
            #if not exists(pth):   raise FileNotFoundError(pth)
            print(pth, file=flist)
        for v in realvols:
            v.meta_checked = True    ; print(v.vid+"/volinfo", file=flist)
        for s in realses:
            s.meta_checked = True
            for sf in ["info"] + ["manifest.z"] if manifest else []:
                print(pjoin(s.volume.vid,s.name,sf), file=flist)

    do_exec([[CP.xargs, CP.sha256sum]], cwd=arch.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest.run([dest.cd + bkdir +" && xargs sha256sum"],
             infile=cmp_list, out=tmpdir+"/compare-hashes.dest")
    # maybe switch cmp to diff/sha256sum if they are safe enough to read untrusted input
    files  = [tmpdir+"/compare-hashes.local", tmpdir+"/compare-hashes.dest"]
    result = do_exec([[CP.cmp] + files], check=False) > 0
    if result and debug:
        print("'diff %s %s' output:" % tuple(files))
        subprocess.call([CP.diff] + files)

    return result


def update_dest(arch, pathlist=[], volumes=[], sessions=[], ext="", delete=False):
    lcd = arch.path    ; dest = arch.dest
    dcd = " cd '"+dest.path+topdir+"/"+arch.name+"'"

    update_list = [x.volume.vid+"/"+x.name+"/manifest.z" for x in sessions] \
                + [x.volume.vid+"/"+x.name+"/info" for x in sessions] \
                + [x.vid+"/volinfo" for x in volumes] + pathlist

    if delete:
        assert not ext;  dest.run([dcd + " && xargs rm -f"], inlines=update_list)
        return

    do_exec([[CP.tar,"-cf","-","--no-recursion","--verbatim-files-from","--files-from", "-"],
             dest.run_args([dcd + "  && tar -xf -"]
                                + [" && mv '"+x+ext+"' '"+x+"'" for x in update_list if ext])
            ], inlines=[x+ext for x in update_list], cwd=lcd)


def fetch_file_blobs(recv_list, recv_dir, dest, ext="", skip0=False, verifier=None):
    cmd = dest.run_args(
            [dest.cd + bkdir
             +" && exec 2>>"+dest.dtmp+"/receive.log"
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ])
    recvp = subprocess.Popen(cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE)
    recvp.stdin.write(("".join((x+"\n" for x,y in recv_list))).encode("UTF-8"))
    recvp.stdin.flush()    ; rc = recvp.poll()
    magic = dest.magic

    for fname, fsz in list(recv_list):
        if rc is not None:   raise RuntimeError("Process terminated early.")
        fpath = recv_dir+"/"+fname+ext
        if exists(fpath):   os.remove(fpath)
        assert recvp.stdout.read(3) == magic
        # Read chunk size
        untrusted_size = int.from_bytes(recvp.stdout.read(4),"big")
        if untrusted_size == 0 and skip0:   continue
        if not fsz >= untrusted_size > 0:
            raise BufferError("Bad file size "+str(untrusted_size))

        # Size is OK.
        size = untrusted_size
        # Read chunk buffer
        untrusted_buf = recvp.stdout.read(size)    ; rc  = recvp.poll()
        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))
        if verifier:   verifier.auth(untrusted_buf)

        os.makedirs(os.path.dirname(fpath), exist_ok=True)
        with open(fpath, "wb") as outf:   outf.write(untrusted_buf)


# Prepare snapshots and check consistency with metadata.
# Must run get_lvm_vgs() again after this.

def prepare_snapshots(datavols):

    ''' Normal precondition will have a snap1vol already in existence in addition
    to the local datavol. Here we create a fresh snap2vol so we can compare
    it to the older snap1vol. Then, depending on monitor or backup mode, we'll
    accumulate delta info and possibly use snap2vol as source for a
    backup session.
    '''

    print("Preparing snapshots...")
    incr_vols, complete_vols = set(), set()    ; vgname = aset.vgname
    if not vg_exists(vgname):
        print("Warning: Volume group '%s' does not exist." % vgname)
    for datavol in datavols:
        if not lv_exists(vgname, datavol):
            print("Warning: Local '%s' does not exist!" % datavol)
            continue

        if datavol not in aset.vols and not monitor_only:   add_volume(datavol, options.voldesc)

        # 'mapfile' is the deltamap file, 'snapXvol' are the .tick and .tock snapshots.
        # .tick holds vol state between send/monitor ops;
        # .tock is new snapshot which is compared w .tick and then replaces it (send/monitor).
        vol      = aset.vols[datavol]   ; mapfile  = vol.mapfile
        snap1vol = datavol + ".tick"    ; snap2vol = datavol + ".tock"

        if lv_exists(vgname, snap1vol) and "arch-"+aset.uuid not in l_vols[snap1vol].tags \
        and "arch-" in " ".join(l_vols[snap1vol].tags):
            if options.remap:
                lv_remove(vgname, snap1vol)    ; print("  Removed mis-matched snapshot", snap1vol)
            else:
                print("  Skipping %s; LV snapshot is from a different archive." % datavol)
                continue

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing snapshot UUIDs; a match means remap is unnecessary.
        if len(vol.sessions):
            if lv_exists(vgname, snap1vol) and vol.last not in l_vols[snap1vol].tags \
            and l_vols[snap1vol].lv_uuid != vol.sessions[vol.last].uuid:
                lv_remove(vgname, snap1vol)    ; print("  Removed mis-matched snapshot", snap1vol)

            if not lv_exists(vgname, snap1vol) and lv_exists(vgname, snap2vol) \
            and vol.last in l_vols[snap2vol].tags and "arch-"+aset.uuid in l_vols[snap2vol].tags:
                # Recover interrupted snapshot rotation
                lv_rename(vgname, snap2vol, snap1vol)

            if lv_exists(vgname, snap1vol) \
            and (    (vol.map_used() == 0 and "delta" in " ".join(l_vols[snap1vol].tags))
                  or (vol.map_used() > 0 and
                      "delta-"+str(os.path.getmtime(vol.mapfile)) not in l_vols[snap1vol].tags
                      )
                 ):
                # Handle inadvertant mapfile snapshot mis-match
                init_deltamap(vol, mapfile, vol.mapsize())
                lv_remove(vgname, snap1vol)    ; print("  Removed mis-matched delta", snap1vol)

            if not exists(mapfile) and lv_exists(vgname, snap1vol) \
            and vol.last in l_vols[snap1vol].tags \
            and "delta" not in " ".join(l_vols[snap1vol].tags) \
            and l_vols[snap1vol].lv_uuid == vol.sessions[vol.last].uuid: ####
                # Latest session matches current snapshot; OK to make blank map.
                init_deltamap(vol, mapfile, vol.mapsize())

        elif monitor_only:
            print("  Skipping %s; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but .tick and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if not exists(mapfile) or not lv_exists(vgname, snap1vol):
            if not monitor_only:
                print("  Pairing snapshot for", datavol)
                complete_vols.add(datavol)
            else:
                print("  Skipping %s; No paired snapshot." % datavol)    ; continue

        # Make fresh snap2vol
        lv_remove(vgname, snap2vol)
        tags =["--addtag=delta"] if monitor_only else []
        do_exec([[CP.lvm, "lvcreate", "-pr", "-kn", "-ay", "--addtag=wyng"] + tags
                        + ["--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n",snap2vol]])

        # Volume is OK, add to list of vols.
        if datavol not in complete_vols and vol.sessions:
            incr_vols.add(datavol)
        else:
            complete_vols.add(datavol)

    return incr_vols, complete_vols


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(datavols):
    vgname   = aset.vgname.replace("-","--")
    poolset  = set(l_vols[x].pool_lv for x in datavols)
    if options.verbose:   print("Acquiring deltas.")

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    catch_signals()
    for apool in sorted(poolset):
        poolname = apool.replace("-","--")
        if not lv_exists(vgname, apool):
            raise RuntimeError("LV pool '%s/%s' does not exist." % (vgname, apool))

        try:
            lvm_meta_snapshot(aset, "reserve", pool=apool)
            for datavol in datavols:
                if l_vols[datavol].pool_lv != apool:   continue
                snap1vol = datavol + ".tick"    ; snap2vol = datavol + ".tock"
                cmds = [[CP.thin_delta, "-m",   "--thin1=" + l_vols[snap1vol].thin_id,
                                                "--thin2=" + l_vols[snap2vol].thin_id,
                                        "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", "^\s*<same .*\/>$"]
                        ]
                do_exec(cmds,  out=tmpdir+"/delta."+datavol)
        except Exception as e:
            sys.stderr.write("ERROR running thin_delta process.\n")
            raise e
        finally:
            lvm_meta_snapshot(aset, "release", pool=apool)

    catch_signals(None)


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest(datavol):

    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = vol.name + ".tick"         ; snap2vol   = vol.name + ".tock"
    snap1size   = l_vols[snap1vol].lv_size   ; snap2size = l_vols[snap2vol].lv_size
    assert len(vol.sessions) and exists(vol.mapfile)

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(tmpdir+"/delta."+datavol).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    dnewchunks  = isnew  = anynew = dfreedblocks = 0

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile, "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // bs    ; chunkblocks = chunksize // bs
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                isnew = anynew = 1
            elif delta.tag == "left_only":
                isnew = 0    ; dfreedblocks += blockend - blockbegin
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        del(bmap_mm)
        if dnewchunks+dfreedblocks:   bmapf.flush()    ; os.fsync(bmapf.fileno())

    catch_signals()
    map_updated = dnewchunks+dfreedblocks+anynew+vol.map_used() > 0
    if monitor_only:
        rotate_snapshots(vol, rotate=map_updated,
                              delta_time=str(os.path.getmtime(vol.mapfile)))
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if map_updated else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*chunksize, save=True)
    catch_signals(None)
    return map_updated


# Reads addresses from manifest and marks corresponding chunks in a volume's deltamap.

def manifest_to_deltamap(datavol, manifest, mapsize):
    with open(manifest, "r") as mf, \
         open(aset.vols[datavol].mapfile, "r+b") as bmapf:

        bmapf.truncate(mapsize)    ; bmapf.flush()
        bmap_mm = mmap.mmap(bmapf.fileno(), 0)       ; chunksize  = aset.chunksize
        for ln in mf:
            addr = int(ln.split()[1][1:], 16)        ; volsegment = addr // chunksize
            bmap_pos = volsegment // 8               ; bmap_mm[bmap_pos] |= 1 << (volsegment % 8)


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a blank deltamap and .tick snapshot to
# track changes. After an incremental send, snapshots are rotated and the deltamap is reset.
#
# Returns (int, int) representing an estimate of bytes sent and chunks freed.
# Bytes sent will always be >0 if nonzero chunks were added to the manifest.
# A result of (0, 0) means no change was detected or sent.

def send_volume(datavol, localtime, ses_tags, send_all):

    vol         = aset.vols[datavol]            ; dedup       = options.dedup
    snap2vol    = vol.name + ".tock"            ; snap2size   = l_vols[snap2vol].lv_size
    bmap_size   = vol.mapsize(snap2size)        ; chunksize   = aset.chunksize
    chdigits    = max_address.bit_length()//4   ; chformat    = "%0"+str(chdigits)+"x"
    bksession   = "S_"+localtime                ; sdir        = pjoin(vol.vid, bksession)
    prior_size  = vol.volsize()                 ; prior_ses   = vol.last
    verbose     = not options.quiet             ; zeros       = bytes(chunksize)
    addrsplit   = -address_split[1]             ; lchunk_addr = vol.last_chunk_addr(snap2size)
    compress = compressors[aset.compression][2] ; compresslevel = int(aset.compr_level)

    if len(vol.sessions):
        # Our chunks are usually smaller than LVM's, so generate a full manifest to detect
        # significant amount of unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(datavol), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None
    fman_hash     = fman_fname = ""

    ses = vol.new_session(bksession, addtags=ses_tags)
    ses.localtime = localtime
    ses.uuid      = l_vols[snap2vol].lv_uuid
    ses.volsize   = snap2size
    ses.path      = vol.path+"/"+bksession+"-tmp"

    # Code from init_dedup_indexN() localized here for efficiency.
    dedup_idx     = dedup_db = None
    if dedup:
        hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w \
                  = aset.dedupindex
        chtree_max= 2**(chtree[0].itemsize*8)
        idxcount  = dataf.seek(0,2) // (ch_w+ses_w)    ; ddblank_ch = bytes(hash_w)
        ddsessions = aset.dedupsessions                ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(metadir+bkdir)    ; os.makedirs(sdir+"-tmp")
    do_exec([[CP.chattr, "+c", sdir+"-tmp"]])


    if aset.datacrypto:
        crypto  = True
        encrypt = aset.datacrypto.encrypt
    else:
        crypto  = False


    # Use tar to stream files to destination
    stream_started = False
    untar_cmd = [dest.cd + " && mkdir -p ."+bkdir
                 + " && exec >>"+dest.dtmp+"/send.log 2>&1"
                 + " && "+dest.cd+bkdir + " && echo "+vol.vid
                 + " && python3 "+dest.dtmp+"/dest_helper.py send"
                 + (" --sync" if options.maxsync else "")]

    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(pjoin("/dev",aset.vgname,snap2vol),"rb", buffering=chunksize) as vf, \
         open(sdir+"-tmp/manifest.tmp", "wt") as hashf,                             \
         open("/dev/zero" if send_all else vol.mapfile,"rb") as bmapf:

        vf_seek = vf.seek; vf_read = vf.read   ; BytesIO = io.BytesIO
        gethash = hash_funcs[aset.hashtype]    ; b64enc  = base64.urlsafe_b64encode
        b2int   = int.from_bytes               ; islice  = itertools.islice


        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        zdelim  = bytes(64)    ; zdlen = len(zdelim)*8      ; minibmap = None   ; bmap_list = []
        addr    = counter = percent = bcount = ddbytes = 0  ; checkpt = 128

        while addr < snap2size:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  list(islice(fullmanifest, zdlen))
                addr += chunksize*zdlen
                minibmap = bmap_list.pop(0)
            elif not send_all:
                # Get more: split(zdelim) shows where large unmodified zones exist.
                bmap_list.extend(bmapf.read(25000).split(zdelim))
                minibmap = bmap_list.pop(0)

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, snap2size if send_all
                        else min(snap2size, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = "x"+chformat % addr

                if fullmanifest:
                    try:
                        fman_hash, fman_fname = fullmanifest_readline().split()
                    except ValueError: # EOF
                        fullmanifest = None    ; fman_hash = ""
                    else:
                        if fman_fname != destfile:
                            raise ValueError("expected manifest addr %s, got %s"
                                            % (destfile, fman_fname))

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                # Fetch chunk as buf
                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Process checkpoint
                if counter > checkpt:
                    aset.save_conf()    ; tarf.add(aset.confname)
                    # Show progress.
                    if verbose:
                        percent = int(addr/snap2size*1000)
                        print("\x0d  %4.1f%% %7dM" % (percent/10, bcount//1000000),
                              end="", flush=True)
                    counter = 0

                # Compress & write only non-empty chunks
                if buf == zeros:
                    if fman_hash == "0":   continue
                    b64hash = "0"
                else:
                    # Compress chunk and hash it
                    buf    = compress(buf, compresslevel)
                    bhash  = gethash(buf)   ; b64hash = b64enc(bhash.digest()).decode("ascii")

                    # Skip when current and prior chunks are the same
                    if fman_hash == b64hash:  continue

                # Start tar stream
                if not stream_started:
                    untar = subprocess.Popen(dest.run_args(untar_cmd),
                            stdin =subprocess.PIPE,    stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)
                    tarf = tarfile.open(mode="w|", fileobj=untar.stdin)
                    tarf_addfile = tarf.addfile        ; TarInfo = tarfile.TarInfo
                    stream_started = True              ; LNKTYPE = tarfile.LNKTYPE
                    tar_info = TarInfo(sdir+"-tmp")    ; tar_info.type = tarfile.DIRTYPE
                    tarf_addfile(tarinfo=tar_info)

                # Add entry to new manifest
                print(b64hash, destfile, file=hashf)
                if b64hash == "0":   continue

                # Add buffer to stream
                tar_info = TarInfo("%s-tmp/%s/%s" % (sdir, destfile[1:addrsplit], destfile))

                # If chunk already in archive, link to it
                if dedup:
                    bhashb = bhash.digest()
                    i      = b2int(bhashb[:ht_ksize], "big")
                    pos    = hashtree[i].find(bhashb)     ; ddses = None
                    if pos % hash_w == 0:
                        data_i = chtree[i][pos//hash_w]   ; dataf.seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[b2int(dataf.read(ses_w),"big")]
                        if ddses is None:
                            hashtree[i][pos:pos+hash_w] = ddblank_ch # zero-out obsolete entry
                        else:
                            ddchx = dataf.read(ch_w).hex().zfill(chdigits)
                            tar_info.type = LNKTYPE
                        dataf.seek(0,2)
                    if ddses is None and bhashb != ddblank_ch and idxcount < chtree_max:
                        hashtree[i].extend(bhashb)   ; chtree[i].append(idxcount)   ; idxcount += 1
                        dataf.write(ses_index.to_bytes(ses_w,"big") + addr.to_bytes(ch_w,"big"))

                if tar_info.type == LNKTYPE:
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.vid,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)

                else:
                    # Encrypt the data chunk
                    if crypto:
                        buf = encrypt(buf)

                    # Send data chunk to the archive
                    tar_info.size = len(buf)
                    tarf_addfile(tarinfo=tar_info, fileobj=BytesIO(buf))
                    bcount += len(buf)    ; counter += 1

            # Advance addr, except when minibmap is zero len.
            if minibmap or send_all:  addr += chunksize

    print("\r  100% ", ("%8.1fM  |  %s" % (bcount/1000000, datavol)),
          ("\n  (reduced %0.1fM)" % (ddbytes/1000000)) if ddbytes and options.verbose else "",
          end="")

    # Send session info, end stream and cleanup
    if fullmanifest:   fullmanifest.close()
    if stream_started:
        # Save session info
        if crypto:   aset.datacrypto.save_counter()
        ses.save_info(ext=".tmp")
        for f in ("manifest.z","info"):
            fpath = sdir+"-tmp/"+f         ; tarf.add(fpath+".tmp", arcname=fpath)
        tarf.add(vol.vid+"/volinfo.tmp")   ; tarf.add(aset.confname+".tmp")

        tarf.close()    ; untar.stdin.close()
        try:
            untar.wait(timeout=60)
        except subprocess.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 99
            untar.kill()
        else:
            retcode = untar.poll()

        if retcode != 0:
            raise RuntimeError("tar transport failure code %d" % retcode)

        if ses.volsize != prior_size and len(vol.sessions) > 1:
            os.link(ses.path+"/manifest.tmp", ses.path+"/manifest")
            check_manifest_sequence(datavol, vol.sesnames)

        # Finalize on VM/remote
        catch_signals()
        dest.run([ dest.cd + bkdir
                 +" && mv -T "+sdir+"-tmp "+sdir
                 +" && mv "+vol.vid+"/volinfo.tmp "+vol.vid+"/volinfo"
                 +" && mv archive.ini.tmp archive.ini"
                 +(" && ( nohup sync -f . 2&>/dev/null & )" if options.maxsync else "")
                 ], trap=True)

        # Local finalize
        for f in ("manifest.z","manifest","info"):
            os.replace(ses.path+"/"+f+".tmp", ses.path+"/"+f)
        os.replace(vol.path+"/volinfo.tmp", vol.path+"/volinfo")
        os.replace(aset.path+"/archive.ini.tmp", aset.path+"/archive.ini")
        ses.path = ses.path.rsplit("-tmp",maxsplit=1)[0]   ; os.replace(ses.path+"-tmp", ses.path)
        fssync(vol.path)

    else:
        catch_signals()
        vol.delete_session(bksession)    ; shutil.rmtree(aset.path+"/"+sdir+"-tmp")

    rotate_snapshots(vol, rotate=True)
    init_deltamap(vol, vol.mapfile, vol.mapsize())
    catch_signals(None)
    if dedup and debug:   show_mem_stats()

    return stream_started + bcount


# Build deduplication hash index and list

def init_dedup_index(listfile=""):

    ctime      = time.perf_counter()    ; makelist = bool(listfile)
    addrsplit  = -address_split[1]
    # Define arrays and element widths
    hash_w     = hash_bits // 8
    ht_ksize   = 2 # binary digits for tree key
    hashtree   = [bytearray() for x in range(2**(ht_ksize*8))]
    chtree     = [array("I") for x in range(2**(ht_ksize*8))]
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2

    # Create master session list, limit to ses_w range
    for vol in aset.vols.values():   aset.dedupsessions += vol.sessions.values()
    aset.dedupsessions.sort(key=lambda x: x.localtime, reverse=True)
    ddsessions = aset.dedupsessions[:2**(ses_w*8)-(len(aset.vols))-1]

    dataf  = open(tmpdir+"/hashindex.dat","w+b")
    dataf_read  = dataf.read    ; dataf_seek = dataf.seek      ; int_frbytes = int.from_bytes
    dataf_write = dataf.write   ; bfromhex = bytes().fromhex   ; b64dec = base64.urlsafe_b64decode
    if makelist:   dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = match = 0
    for sesnum, ses in enumerate(ddsessions):
        vol = ses.volume    ; vid = vol.vid    ; sesname = ses.name
        vol.decode_one_manifest(ses)
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":   continue
                bhashb = b64dec(ln1)
                i      = int_frbytes(bhashb[:ht_ksize], "big")
                pos    = hashtree[i].find(bhashb)
                if pos % hash_w == 0:
                    match += 1
                    if makelist:
                        data_i = chtree[i][pos//hash_w]
                        dataf_seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[int_frbytes(
                                 dataf_read(ses_w),"big")]
                        ddchx  = dataf_read(ch_w).hex().zfill(chdigits)
                        print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                            (ddses.volume.vid, ddses.name, ddchx[:addrsplit], ddchx,
                            vid, sesname, ln2[1:addrsplit], ln2),
                            file=dedupf)
                        dataf_seek(0,2)
                elif count < chtree_max:
                    hashtree[i].extend(bhashb)
                    chtree[i].append(count)
                    dataf_write(sesnum.to_bytes(ses_w,"big"))
                    dataf_write(bfromhex(ln2[1:]))  # Enh: scale no. digits to match vol size
                    count += 1

    if listfile:   dedupf.close()

    aset.dedupindex    = (hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w)
    aset.dedupsessions = ddsessions

    if not debug:  return
    print("\nIndexed in %.1f seconds." % (time.perf_counter()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d, matches: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count, match)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


# Deduplicate data already in archive

def dedup_existing():

    print("Building deduplication index...", end="")
    init_dedup_index("dedup.lst.gz")

    print(" linking...", end="", flush=True)
    do_exec( [dest.run_args([dest.cd + bkdir
               +" && /bin/cat >"+dest.dtmp+"/dest.lst.gz"
               +" && /usr/bin/python3 "+dest.dtmp+"/dest_helper.py dedup"
               ]),
              [CP.cat,"-v"],  [CP.tail,"--bytes=2000"]
            ], infile=tmpdir+"/dedup.lst.gz", out=tmpdir+"/arch-dedup.log")
    print(" done.")
    if options.verbose:   print("".join(open(tmpdir+"/arch-dedup.log","r")))


# Controls flow of monitor and send_volume procedures:

def monitor_send(datavols, monitor_only=True):

    if options.autoprune.lower() == "full":
        for dv in aset.vols:   autoprune(dv, apmode="full")

    for prg in (CP.lvm, CP.dmsetup, CP.thin_delta ):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)
    try:
        p = subprocess.check_output([CP.thin_delta, "-V"])
    except:
        p = b""
    ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
    if p and ver < target_ver:
        raise RuntimeError("Thin provisioning tools version >= "+target_ver+" required.")

    localtime = time.strftime("%Y%m%d-%H%M%S")

    incrementals, send_alls \
        = prepare_snapshots(datavols)

    get_lvm_vgs(aset.vgname)
    if aset.vgname not in volgroups.keys():
        raise ValueError("Volume group "+aset.vgname+" not present.")

    if monitor_only:   send_alls.clear()

    if len(incrementals)+len(send_alls) == 0:
        x_it(0, "No new data.")

    # Process session tags
    ses_tags = []
    if options.tag and options.tag == [""] and not options.unattended and not monitor_only:
        print("Enter tag info as 'tagID[, tag description]'. Blank to end input.")
        while ans := ask_input("[%d]: " % (len(ses_tags)+1)).strip():
            tag = ArchiveSession.tag_parse(ans, delim=",")
            if not tag:   continue
            ses_tags.append(tag)
            if len(ses_tags) == ArchiveSet.max_tags:   break
        print(len(ses_tags), "tags total.")
    elif options.tag and not monitor_only:
        for tag_opt in options.tag:
            tag = ArchiveSession.tag_parse(tag_opt, delim=",")
            if not tag:   raise ValueError("Invalid tag "+tag_opt)
            ses_tags.append(tag)

    if len(incrementals) > 0:
        get_lvm_deltas(incrementals)

    if options.dedup:
        init_dedup_index()

    if not monitor_only:
        cmpvols = [x for x in aset.vols.values()
                         if x.name in incrementals | send_alls and x.sessions]
        cmpses  = [v.sessions[v.sesnames[-1]] for v in cmpvols]
        if compare_files(aset, volumes=cmpvols, sessions=cmpses):
            x_it(1, "Error: Local and archive metadata differ.")

        print("\nSending backup session %s to '%s'." % (localtime, dest.spec))

    for datavol in sorted(incrementals | send_alls):
        vol = aset.vols[datavol]    ; updated = False
        print(" ", "Scan" if monitor_only else "Send", "volume      | ", datavol, flush=True,end="")

        if datavol in incrementals:   updated = update_delta_digest(datavol)
        if monitor_only:   continue

        if datavol in send_alls or updated:
            if vol.changed_bytes > dest.free:
                # Enh: add loop here for all volumes, implement a 'forced' mode
                autoprune(datavol, needed_space=vol.changed_bytes, apmode=options.autoprune)
                if vol.changed_bytes > dest.free:
                    print(" %d additional bytes needed." % (vol.changed_bytes-dest.free))
                    print("Insufficient space on destination %d; Skipping." % dest.free)
                    error_cache.append(datavol)
                    continue

            dnew       = send_volume(datavol, localtime, ses_tags, send_all=datavol in send_alls)
            dest.free -= int(dnew + (dnew * 0.05))

        else:
            rotate_snapshots(vol, rotate=False)    ; dnew = 0

        print("\r" if dnew else "\r  No changes   ", flush=True)


def init_deltamap(vol, bmfile, bmsize):
    vol.changed_bytes_add(0, reset=True)
    if exists(bmfile):
        os.remove(bmfile)
    with open(bmfile, "wb") as bmapf:
        bmapf.truncate(bmsize)    ; bmapf.flush()


def rotate_snapshots(vol, rotate=True, delta_time=""):
    snap1vol = vol.name+".tick"   ; snap2vol = vol.name+".tock"
    if rotate:
        tags =["--addtag=delta-"+delta_time] if delta_time else []
        do_exec([[CP.lvm,"lvchange","--addtag="+vol.last]+tags+[aset.vgname+"/"+snap2vol]])
        lv_remove(aset.vgname, snap1vol)    ; lv_rename(aset.vgname, snap2vol, snap1vol)
    else:
        lv_remove(aset.vgname, snap2vol, sync=False)



# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS or ^tagname format.

def prune_sessions(datavol, times):

    volume = aset.vols[datavol]    ; sessions = volume.sesnames
    t1, t2 = "", ""                ; to_prune  = []

    if len(sessions) < 2:    print("  No extra sessions to prune.")    ; return

    # Validate date-time params
    for pos, dt in enumerate(times[:]):
        if not dt[0].startswith("^"):
            if not dt.startswith("S_"):   times[pos] = "S_"+dt.strip()
            datetime.datetime.strptime(times[pos][2:], "%Y%m%d-%H%M%S")
        elif dt[1:] not in volume.tags:
            print(" No match for", dt)    ; return
        elif pos == 0:
            for sesname in sessions:
                if dt[1:] in volume.sessions[sesname].tags:
                    if len(times) == 1 and not options.allbefore:
                        to_prune.append(sesname)
                    else:
                        t1 = sesname   ; break
        elif pos == 1:
            for sesname in reversed(sessions):
                if dt[1:] in volume.sessions[sesname].tags:   t2 = sesname   ; break

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    if options.allbefore:   t1 = sessions[0]    ; t2 = times[0]
    if not t1:   t1 = times[0]
    if not t2 and len(times) > 1:
        t2 = times[1]
        if t2 <= t1:  x_it(1, "Error: Second date-time must be later than first.")

    # Find specific sessions to prune in contiguous range
    if to_prune:
        pass

    elif t2 == "":
        # find single session
        if t1 in sessions:   to_prune.append(t1)

    else:
        # find sessions in a date-time range
        start = len(sessions)   ; end = 0
        if t1 in sessions:
            start = sessions.index(t1)
        else:
            for ses in sessions:
                if ses > t1:   start = sessions.index(ses)    ; break
        if t2 in sessions:
            end = sessions.index(t2)+1
        else:
            for ses in reversed(sessions):
                if ses < t2:   end = sessions.index(ses)+1    ; break
        to_prune = sessions[start:end]

    if len(to_prune) and to_prune[-1] == sessions[-1]:
        print("  Preserving latest session.")
        del(to_prune[-1])
    if len(to_prune) == 0:
        print("  No selections in this date-time range.")
        return

    autoprune(datavol, apmode=options.autoprune, include=set(to_prune))


# Parameters / vars for autoprune:
# oldest (date): date before which all sessions are pruning candidates
# thin_days (int): number of days ago before which the thinning params are applied
# ndays & nsessions: a days/sessions ratio for amount of sessions left after thinning
# nthresh: min number of sessions to prune this time (0 = prune all candidates)
# target_size (0 or MB int): User-selected size cap for archive (future)

def autoprune(datavol, needed_space=0, apmode="off", include=set()):

    dtdate = datetime.date
    def to_date(sesdate):
        return dtdate(int(sesdate[2:6]),int(sesdate[6:8]),int(sesdate[8:10]))

    vol       = aset.vols[datavol]
    if len(vol.sesnames) < 2:   return False
    apmode    = apmode.lower()                ; exclude = set()
    sessions  = vol.sesnames[:-1]             ; marked  = 0
    today     = dtdate.today()                ; oldest  = today-datetime.timedelta(days=366)
    thin_days = datetime.timedelta(days=32)   ; ndays   = 7      ; nsessions = 2
    startdate = to_date(sessions[0])          ; nthresh = 3 if apmode == "on" else 0
    enddate   = min(to_date(sessions[-2]), today-thin_days) if len(sessions) > 2 else None

    # Make a 2d array of ordinal dates and populate with session id + flag
    apcal = { day: [] for day in range(dtdate(startdate.year,1,1).toordinal(),
                                       to_date(sessions[-1]).toordinal()+ndays) }
    for ses in sessions:  apcal[to_date(ses).toordinal()].append([ses, True])

    # Build set of excluded sessions
    for sx in options.keep:
        if not sx.startswith("^"):
            datetime.datetime.strptime(sx, "%Y%m%d-%H%M%S")    ; exclude.add("S_"+sx)
        else:
            if sx[1:] == "all":
                exclude += {x.name for x in vol.sessions if x.tags}
            else:
                exclude += {x.name for x in vol.sessions if sx[1:] in x.tags}
    include -= exclude

    # Mark all sessions prior to oldest date setting, plus include list
    for ses in sessions:
        sdate = to_date(ses)
        if (apmode != "off" and sdate <= oldest) or ses in include:
            for dses in apcal[sdate.toordinal()]:
                if dses[0] == ses and ses not in exclude:
                    dses[1] = False    ; vol.sessions[ses].toggle = False

    # Mark sessions for thinning-out according to ndays + nsessions
    if apmode != "off" and (needed_space or apmode != "min") and enddate:
        for year in range(startdate.year, enddate.year+1):
            for span in range(dtdate(year,1,1).toordinal(),
                            min(dtdate(year,12,31), enddate).toordinal(), ndays):
                dlist = []    ; offset = 0
                for day in range(span, min(span+ndays, enddate.toordinal())):
                    dlist.append(sum( x[1] for x in apcal[day] ))
                while sum(dlist) > nsessions: ## Enh: Make even distribution
                    bigday = dlist.index(max(dlist[offset:]), offset)
                    offset += (ndays//nsessions)+1    ; offset %= min(ndays, len(dlist))
                    for dses in apcal[span+bigday]:
                        if dses[1]:
                            # always decr bigday, but don't toggle if session is excluded
                            dlist[bigday] -= 1
                            dses[1] = vol.sessions[dses[0]].toggle = dses[0] in exclude
                            break

    # Find contiguous marked ranges and merge/prune them. Repeat until free >= needed space.
    factor = 1    ; sessions.append("End")
    while True:
        to_prune = []    ; removed_ct = 0    ; skipped = False
        for ses in sessions:
            if ses is None:   continue
            if ses == "End" or vol.sessions[ses].toggle :
                if to_prune:
                    # prioritize ranges that overlap with requested includes
                    if include and not (set(to_prune) & include):
                        to_prune.clear()    ; skipped = True    ; continue

                    target_s = vol.sesnames[vol.sesnames.index(to_prune[-1]) + 1]
                    merge_sessions(datavol, to_prune, target_s, clear_sources=True)

                    for i in to_prune:   sessions[sessions.index(i)] = None
                    include -= set(to_prune)    ; removed_ct += len(to_prune);   to_prune.clear()
                    if not include and nthresh and removed_ct >= nthresh*factor:  break # for ses
            else:
                to_prune.append(ses)

        if removed_ct:
            print(" Removed", removed_ct)
            dest.get_free(tmpdir+"/merge.log")    ; os.remove(tmpdir+"/merge.log")

        if skipped and apmode != "off":   continue # while
        if removed_ct == 0 or nthresh == 0 or needed_space <= dest.free:
            break # while
        elif factor > 4:
            nthresh = 0
        else:
            factor += 2

    return True


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session dir name.

def merge_manifests(datavol, msessions=None, mtarget=None, addcol=False):
    # Enh: implement mtarget to support merge_sessions()
    volume    = aset.vols[datavol]
    msessions = volume.sesnames if not msessions else msessions
    sespaths  = [ os.path.basename(volume.sessions[x].path) for x in msessions ]
    tmp       = big_tmpdir if volume.volsize() > 128000000000 else tmpdir
    outfile   = tmp+"/manifest.mrg"     ; slist  = []

    if not aset.dedupsessions:   volume.decode_manifests(msessions)
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(sespaths)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir  = tmp+"/m"     ; slsort  = slist[1]
        shutil.rmtree(cdir, ignore_errors=True);   os.makedirs(cdir)

        # fix: extrapolate path with filename
        do_exec([[CP.xargs, "-0", "-a", slist[0],
                  CP.awk, '{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                            +tmp+'/m/"FILENAME}']], cwd=volume.path)
    else:
        cdir  = volume.path     ; slsort  = slist[0]

    do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=16", "--files0-from="+slsort]],
            out=outfile, cwd=cdir)
    if addcol:  shutil.rmtree(tmp+"/m")

    return outfile


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume (datavol), source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(datavol, sources, target, clear_sources=False):
    global aset

    volume     = aset.vols[datavol]    ; resume = bool(aset.in_process)
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    chformat   = "x%0"+str(chdigits)+"x"
    m_tmp      = tmpdir if volume.volsize() < 128000000000 else big_tmpdir

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)         ; destvol = bkdir+"/"+volume.vid

    if not resume:
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max(x.volsize for x in volume.sessions.values()
                                    if x.name in sources)
        last_chunk = chformat % volume.last_chunk_addr(volsize)
        lc_filter  = '"'+last_chunk+'"'

        with open("merge.lst", "wt") as lstf:
            print(merge_target, target, file=lstf)
            volume.decode_one_manifest(volume.sessions[merge_target])

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                volume.decode_one_manifest(volume.sessions[ses])
                do_exec([[CP.sed, "-E", "s|$| "+ses+"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=16"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]],
                out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, "$2<="+lc_filter+" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, "$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                "s|^0 x(\S{" + str(address_split[0]) + "})(\S+)\s+(S_\S+)|"
                "-rm "+merge_target+"/\\1/x\\1\\2|; t; "

                "s|^\S+\s+x(\S{" + str(address_split[0]) + "})(\S+)\s+(S_\S+)|"
                "rename \\3/\\1/x\\1\\2 "+merge_target+"/\\1/x\\1\\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.lst")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, "$2>"+lc_filter, m_tmp+"/manifest.two"],
                     [CP.sed, "-E", "s|^\S+\s+x(\S{" + str(address_split[0]) + "})(\S+)|"
                                    "-rm "+merge_target+"/\\1/x\\1\\2|"]
                    ], out=">>merge.lst")

        do_exec([[CP.gzip, "-f", "merge.lst"]])

    if not resume:
        # Set archive in_process state to "merge"
        aset.set_in_process(["merge", datavol, str(clear_sources), target, sources],
                            tmp=False, todest=False)

    # Update & send new metadata and process lists to dest
    if clear_sources:
        for ses in sources:
            if ses in volume.sessions:   volume.delete_session(ses, remove=False)

    if not resume:
        aset.set_in_process(None, tmp=True, todest=False)
        volume.sessions[target].save_info(".tmp")
        do_exec([[CP.tar, "-cf", "-", "../archive.ini", "../archive.ini.tmp", "merge.lst.gz",
                            target+"/manifest.z.tmp", target+"/info.tmp", "volinfo.tmp"],
                 dest.run_args([dest.cd + destvol + " && tar -xmf -"])
                ], cwd=volume.path)

    # Start merge operation on dest
    retcode = dest.run([dest.cd + destvol + " && python3 "
                        + dest.dtmp+"/dest_helper.py merge"
                        + (" --resume" if resume else "") + (" --sync" if options.maxsync else "")
                        ],
                        check=False, out=">>"+tmpdir+"/merge.log"
                      )

    catch_signals()
    if retcode == 50:
        # Initialization didn't complete, so reload aset and abort
        aset = get_configs(options)
        aset.set_in_process(None)
        for f in ("merge.lst.gz","volinfo.tmp",target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest.run([dest.cd + destvol + " && rm -rf merge merge.lst.gz"], check=False)
        x_it(retcode, "Error: Merge could not initialize!")
    elif retcode != 0:
        x_it(retcode, "Error: Remote exited!")

    # Finalize merge operation on dest
    dest.run([dest.cd + destvol + " && python3 "
                +dest.dtmp+"/dest_helper.py merge --finalize"+(" --resume" if resume else "")
                +" && rm -rf merge merge.lst.gz"
                ],
                trap=True, out=">>"+tmpdir+"/merge.log"
            )

    # Local finalize
    for f in (target+"/info", target+"/manifest.z", target+"/manifest",
              "volinfo", "../archive.ini"):
        if exists(f+".tmp"):  os.replace(f+".tmp", f)

    aset.set_in_process(None, todest=True)    ; os.remove("merge.lst.gz")
    catch_signals(None)

    # Check consistency after resuming merge
    if resume and compare_files(aset, volumes=[volume], sessions=[volume.sessions[target]]):
        x_it(1, "Error: Local and dest metadata differ.")

    for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(datavol, select_ses="", save_path="", diff=False, verify_only=0):

    def diff_compare(dbuf,z):
        if dbuf != volf.read(chunksize):
            if remap:
                volsegment = addr // chunksize 
                bmap_pos = volsegment // 8
                bmap_mm[bmap_pos] |= 1 << (volsegment % 8)
            return len(dbuf)
        else:
            return 0

    verbose     = not options.quiet and verify_only != 2
    attended    = not options.unattended          ; remap     = options.remap
    sparse      = options.sparse                  ; chunksize = aset.chunksize
    sparse_write= options.sparse_write or sparse  ; debug     = options.debug
    vgname      = aset.vgname                     ; zeros     = bytes(chunksize)
    vol         = aset.vols[datavol]              ; snap1vol  = datavol+".tick"
    gethash     = hash_funcs[aset.hashtype]       ; sessions  = vol.sesnames
    compress    = compressors[aset.compression][2]  ; compresslevel = int(aset.compr_level)
    decompress  = compressors[aset.compression][0].decompress
    decrypt     = aset.datacrypto.decrypt if aset.datacrypto else None
    compare_digest = hmac.compare_digest          ; b64enc    = base64.urlsafe_b64encode


    if diff or verify_only:
        save_path = ""
    elif not save_path:
        save_path = pjoin("/dev",vgname,datavol)

    # Set the session to retrieve
    if select_ses:
        if select_ses[0] == "^":
            # match tag to session id
            tag = select_ses[1:]
            if tag in vol.tags:
                select_ses = sorted(vol.tags[tag])[-1]
                print("Matched tag to", select_ses)
        else:
            # validate date-time input
            datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
            select_ses = "S_"+select_ses

        if select_ses not in sessions:
            x_it(1, "No match for specified session criteria.")

    elif len(sessions) > 0:
        select_ses = sessions[-1]
    else:
        x_it(1, "No sessions available.")

    if diff and remap and select_ses != sessions[-1]:
        x_it(1, "Cannot use prior session for remap.")

    if save_path and exists(save_path) and attended:
        print("\n!! This will", "overwrite" if sparse_write else "erase all",
              "existing data in",save_path,"!!")
        ans = ask_input("   Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    volsize     = vol.sessions[select_ses].volsize   ; magic       = dest.magic
    chdigits    = max_address.bit_length() // 4      ; chformat    = "x%0"+str(chdigits)+"x"
    lchunk_addr = vol.last_chunk_addr(volsize)       ; last_chunkx = chformat % lchunk_addr
    addrsplit   = -address_split[1]                  ; rc = volf = None

    # Collect session manifests
    incl_ses = []
    for ses in sessions if verify_only != 2 else [select_ses]:
        incl_ses.append(ses)
        if ses == select_ses:  break

    # verify metadata
    if not options.from_arch \
    and compare_files(aset, volumes=[vol], sessions=[vol.sessions[x] for x in incl_ses],
                      manifest=True):
        x_it(1, "Error: Local and archive metadata differ.")
    vol.decode_manifests(incl_ses, force=True)

    # Merge manifests and send to archive system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    manifest = merge_manifests(datavol, msessions=incl_ses, addcol=True)

    if not sparse:
        cmds = [[CP.sed, "-E", "/"+last_chunkx+"/q", manifest],  ## Enh: detect vol_shrank
                [CP.sed, "-E", "/^0\s/ d; "
                "s|^\S+\s+x(\S{" +str(address_split[0])+ "})(\S+)\s+(S_\S+)|\\3/\\1/x\\1\\2|;"],
                [CP.gzip, "-c", "-4"
                ],
                dest.run_args(["cat >"+dest.dtmp+"/dest.lst.gz"]),
            ]   # Enh: replace list with range
        do_exec(cmds)

    # Prepare save volume
    if save_path:
        # Decode dev path semantics and match to vg/lv if possible. Otherwise, open
        # simple block dev or file.
        save_type = "block device"
        returned_home = False    ; lv, pool, vg = get_lv_path_pool(save_path)
        if not lv and vg_exists(os.path.dirname(save_path)):
            # Got vg path, lv does not exist
            lv = os.path.basename(save_path)
            vg = os.path.basename(os.path.dirname(save_path))
        if vg:
            # Does save path == original path?
            returned_home = (lv== datavol) and (vg== aset.vgname) and not options.from_arch
            save_type = "logical volume"
            if not lv_exists(vg,lv):
                if vg != vgname:
                    x_it(1, "Cannot auto-create volume: Volume group does not match config.")
                print("Creating '%s' in thin pool [%s/%s]." % (lv, vg, aset.poolname))
                do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(volsize)+"b",
                          "--thin", "-n", lv, vg+"/"+aset.poolname]])
            elif l_vols[lv].lv_size != volsize:
                print("Re-sizing LV to %d bytes." % volsize)
                do_exec([[CP.lvm, "lvresize", "-L", str(volsize)+"b", "-f", save_path]])

        if exists(save_path) and stat.S_ISBLK(os.stat(save_path).st_mode):
            if not sparse_write:   do_exec([[CP.blkdiscard, save_path]])
            volf = open(save_path, "w+b")
        elif save_path.startswith("/dev/"):
            x_it(1,"Cannot create new volume from ambiguous /dev path."
                 " Please create the volume before using 'receive', or specify"
                 " --save-to=/dev/volgroup/lv in case of a thin LV.")
        else: # file
            save_type = "file"
            volf = open(save_path, "w+b")
            if not sparse_write:   volf.truncate(0)   ; volf.flush()
            volf.truncate(volsize)    ; volf.flush()

    elif diff:
        if not lv_exists(vgname, datavol):
            x_it(1, "Local volume must exist for diff.")
        if remap:
            lv_remove(vgname, snap1vol)
            do_exec([[CP.lvm,"lvcreate", "-pr", "-kn", "-ay", "--addtag=wyng",
                    "--addtag="+vol.last, "--addtag=delta",
                    "--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n", snap1vol]])
            print("  Initial snapshot created for", datavol)
            get_lvm_vgs(aset.vgname)
            if not exists(vol.mapfile):
                init_deltamap(vol, vol.mapfile, vol.mapsize())
            bmapf = open(vol.mapfile, "r+b")
            bmapf.truncate(vol.mapsize())    ; bmapf.flush()
            bmap_mm = mmap.mmap(bmapf.fileno(), 0)
        else:
            if not lv_exists(vgname, snap1vol):
                print("Snapshot '.tick' not available; Comparing with source volume instead.")
                snap1vol = datavol

            if volsize != l_vols[snap1vol].lv_size:
                x_it(1, "Volume sizes differ:"
                    "\n  Archive = %d \n  Local   = %d" % (volsize, l_vols[snap1vol].lv_size))

        volf  = open(pjoin("/dev",vgname,snap1vol), "rb")


    if volf:   volf_read = volf.read    ; volf_write = volf.write    ; volf_seek = volf.seek

    if verbose:
        print("Receiving" if save_path else "Scanning", "volume:", datavol, select_ses[2:])
        if save_path:    print("Saving to %s '%s'" % (save_type, save_path))

    # Create retriever process using py program
    cmd = dest.run_args(
            [dest.cd + bkdir+"/"+vol.vid
             +" && exec 2>>"+dest.dtmp+"/receive.log"
             +" && python3 "+dest.dtmp+"/dest_helper.py receive"
            ])
    getvol   = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                     stdin =subprocess.PIPE if sparse else subprocess.DEVNULL)
    gv_stdin = io.TextIOWrapper(getvol.stdin, encoding="utf-8") if sparse else None

    # Open manifest then receive, check and save data
    addr = bcount = diff_count = 0
    for mfline in open(manifest, "r"):
        if addr >= lchunk_addr:   break
        cksum, faddr, ses = mfline.split()    ; addr = int(faddr[1:], 16)

        if verbose:   print("%.2f%%" % (addr/volsize*100), end="\x0d")

        # Process zeros quickly
        if cksum.strip() == "0":
            if save_path:
                volf_seek(addr)
                if sparse_write and volf_read(chunksize) != zeros:
                    ## Enh:  Use hole punch when available...
                    volf_seek(addr)    ; volf_write(zeros)    ; diff_count += chunksize
            elif diff:
                volf_seek(addr)    ; diff_count += diff_compare(zeros,True)

            continue

        # Request chunks on-demand if local chunk doesn't match cksum
        if sparse and save_path:
            volf_seek(addr)
            if b64enc(gethash(compress(volf_read(chunksize),
                                       compresslevel)).digest()).decode("ascii") == cksum:
                continue
            else:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)

        # Read chunk size
        assert getvol.stdout.read(3) == magic
        untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

        # allow for slight expansion from compression algo
        if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
            raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

        ##  Size is OK  ##
        size = untrusted_size

        # Read chunk buffer
        untrusted_buf = getvol.stdout.read(size)
        rc  = getvol.poll()
        if rc is not None and len(untrusted_buf) == 0:
            break

        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))


        # Decrypt the data chunk
        # Validation MUST be next step!
        if decrypt:
            untrusted_buf = decrypt(untrusted_buf)

        # Validate data chunk
        if not compare_digest(cksum, b64enc(gethash(untrusted_buf).digest()).decode("ascii")):
            with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
            print(size, mfline)
            raise ValueError("Bad hash "+faddr+" :: "+str(b64enc(gethash(untrusted_buf).digest())))

        ##  Buffer is OK  ##
        buf = untrusted_buf   ; bcount += len(buf)

        if verify_only:   continue

        # Proceed with decompress.
        decomp = decompress(buf)
        if len(decomp) != chunksize and addr < lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        if addr == lchunk_addr and len(decomp) != volsize - lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        buf = decomp

        if save_path:
            volf_seek(addr)
            # Don't re-check buffer for sparse mode, check for sparse_write:
            if sparse:
                volf_write(buf)    ; diff_count += len(buf)
            elif sparse_write:
                if volf_read(chunksize) != buf:
                    volf_seek(addr)    ; volf_write(buf)    ; diff_count += len(buf)
            else:
                volf_write(buf)
        elif diff:
            volf_seek(addr)    ; diff_count += diff_compare(buf,False)


    if rc is not None and rc != 0:
        raise RuntimeError("Error code from getvol process: "+str(rc))

    if not sparse and verify_only != 2 and addr+len(decompress(untrusted_buf)) != volsize:
        raise ValueError("Received range %d does not match volume size %d."
                            % (addr+len(decompress(untrusted_buf)), volsize))

    print("Data bytes:", bcount, end="")
    print("  OK" if not diff_count else "  Diff bytes: " + str(diff_count))
    if verbose:   print("Received range:", addr+len(buf))

    if volf:  volf.close()
    if save_path:
        os.sync()
        if returned_home and select_ses == sessions[-1]:
            lv_remove(vgname, snap1vol)
            do_exec([[CP.lvm, "lvcreate", "-pr", "-kn", "-ay",
                "--addtag=wyng", "--addtag="+vol.last,
                "--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n", snap1vol]])
            print("  Initial snapshot created for", datavol)
            init_deltamap(vol, vol.mapfile, vol.mapsize())
        elif returned_home:
            print("Restored from older session: Volume may be out of"
                " sync with archive until '%s --remap diff %s' is run!"
                % (prog_name, datavol))
    if remap:
        bmapf.close()
        if diff_count > 0 and options.action != "send":
            print("\nNext 'send' will bring this volume into sync.")
    elif diff_count:
        error_cache.append(datavol)

    return bcount


# Rename a volume in the archive

def rename_volume(oldname, newname):
    for prg in (CP.lvm, CP.blkdiscard):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    os.chdir(aset.path)    ; volume   = aset.vols[oldname]
    if not aset.rename_volume(oldname, newname, ext=".tmp"):
        x_it(1, "Error: Cannot rename '%s' to '%s'." % (oldname,newname))

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[volume], ext=".tmp")
    for fl in (aset.confpath, volume.path+"/volinfo"):
        os.replace(fl+".tmp", fl)
    catch_signals(None)

    for ext in (".tick",".tock"):
        vg = aset.vgname    ; lv_remove(vg, newname+ext)
        if lv_exists(vg, oldname+ext):   lv_rename(vg, oldname+ext, newname+ext)

    full_aset = ArchiveSet(aset.name, aset.path, aset.dest, allvols=True, prior_auth=aset)
    if len(full_aset.vols[newname].sessions) \
    and compare_files(full_aset, pathlist=[full_aset.confname], volumes=[full_aset.vols[newname]]):
        x_it(1, "Error: Local and archive metadata differ.")

    return full_aset


def add_volume(datavol, desc):
    if not lv_exists(aset.vgname, datavol):   print("Warning:", datavol, "does not exist.")
    vol = aset.add_volume(datavol, desc=desc, ext=".tmp")
    if not vol:   return

    catch_signals()
    update_dest(aset, pathlist=[aset.confname], volumes=[vol], ext=".tmp")
    os.replace(pjoin(vol.path, "volinfo.tmp"), pjoin(vol.path, "volinfo"))
    os.replace(aset.confpath+".tmp", aset.confpath)
    catch_signals(None)

    if lv_exists(aset.vgname, datavol+".tick") \
    and "arch-"+aset.uuid not in volgroups[aset.vgname].lvs[datavol+".tick"].tags:
        sys.stderr.write("Warning: Volume '%s' is already tracked"
                            "by a Wyng snapshot from a different archive!\n" % datavol)


# Remove a volume from the archive

def delete_volume(dv):
    for prg in (CP.lvm, CP.blkdiscard):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    if not options.unattended and not aset.in_process:
        print("Warning! Delete will remove ALL metadata AND archived data",
              "for volume", dv)

        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    print("\nDeleting volume", dv, "from archive.")
    catch_signals()
    if not aset.in_process:       aset.set_in_process(["delete", dv])
    dvid = aset.delete_volume(dv)
    update_dest(aset, pathlist=[aset.confname])
    aset.set_in_process(None)

    if dvid:
        dest.run([dest.cd + bkdir + "  && rm -rf '%s'" % dvid])
    catch_signals(None)

    for ext in {".tick",".tock"}:
        if lv_exists(aset.vgname, dv+ext) and (options.clean \
        or "arch-"+aset.uuid in volgroups[aset.vgname].lvs[dv+ext].tags):
            lv_remove(aset.vgname, dv+ext, check=True)
    return


# Remove Wyng metadata from local system

def remove_local_metadata(archive):
    for vg in volgroups.values():
        for lv in list(vg.lvs.values()):
            if lv.lv_name.endswith((".tick",".tock")) and "wyng" in lv.tags \
            and (not archive or "arch-"+archive.uuid in lv.tags):
                lv_remove(vg.name, lv.lv_name, check=True)

    # Remove metadata dir(s)
    shutil.rmtree(archive.path if archive else metadir+topdir,
                  ignore_errors=True)


def show_list(selected_vols):

    if dest.archive_ini_hash == "none":   print("(CACHED)")
    if options.verbose:
        print("\nArchive Settings:")
        for key, val in aset.conf["var"].items():
            print(" %-15s = %s" % (key, val))

    # Print list of sessions grouped by tag. First, organize by tag:
    if options.tag:
        print("\nTag Assignments")    ; ltags = {}
        for dv in selected_vols if selected_vols else datavols:
            vol = aset.vols[dv]
            for tag, tses in vol.tags.items():
                #tset = ( dv+" / "+x[2:] for x in tses )
                tset = ( (dv, x[2:], vol.sessions[x].tags[tag] ) for x in tses )
                if tag not in ltags:
                    ltags[tag] = list(tset)
                else:
                    ltags[tag] += tset
        # Print result:
        for tag in sorted(ltags):
            print("\n"+tag+":")
            for ses in sorted(ltags[tag]):   print(" ", ses)
        return

    # Print list of volumes if no volume is selected.
    if not aset.vols:
        print("No volumes.")    ; return
    elif not selected_vols and not len(options.volumes):
        print("\nConfigured Volumes [%s/%s]\n" % (aset.vgname, aset.poolname))
        maxwidth = max(len(x.name) for x in aset.vols.values())
        fmt    = "%7.2f GB  %s  %-" + str(maxwidth+5) + "s  %s"
        for vname in sorted(x.name for x in aset.vols.values()):
            vol = aset.vols[vname]   ; sname = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            if options.verbose:
                print(fmt % ((vol.volsize() / 1024**3), vol.vid, vname, sname))
                if vol.desc:   print("  desc:", vol.desc)
            else:
                print(vname)
        return

    # Print list of sessions grouped by volume.
    # Get the terminal column width and set cols to number of list columns.
    ttycols = os.popen('stty size', 'r').read().split()[1]
    cols = max(4, min(10, int(ttycols)//17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        if not aset.vols[dv].sessions:   print("None.")    ; continue
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for sname in vol.sesnames + [""]:
            month = sname[2:8]    ; ses = vol.sessions[sname] if sname else None
            if options.unattended or options.verbose:
                # plain listing
                print(sname[2:])
                if ses and options.verbose:
                    for tag in sorted(ses.tags.items()):
                        print(" tag:", tag[0]+(", "+tag[1] if tag[1] else ""))
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(sname)
            else:
                # print the month listing: 'rows' is adjusted to carry the remainder on
                # additional lines. 'extra' and 'steps' are used to eliminate ragged
                # column on the right (a ragged row is more pleasing to the eye).
                size  = len(slist)    ; rows = size//cols
                rows += (size%rows)//cols if rows else 0     ; extra = size - cols*rows
                heights = [rows+(x<extra) for x in range(cols) ]

                # output one row at a time
                for ii in range(max(heights)):
                    print("  ".join(  slist[ii+sum(heights[:c])][2:] for c in range(cols)
                                       if ii < heights[c]  ))
                print()

                # start new month list
                lmonth = month    ; slist = [sname]


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def is_num(val):
    try:
        float(val)
    except:
        return False
    else:
        return True


def catch_signals(action="do"):
    if action is not None and len(signal_handlers):   print("Already catching")   ; return
    for sig in (signal.SIGINT, signal.SIGTERM, signal.SIGQUIT, signal.SIGABRT, signal.SIGALRM,
                signal.SIGHUP, signal.SIGTSTP, signal.SIGUSR1):
        if action is None:
            signal.signal(sig, signal_handlers[sig])       ; del(signal_handlers[sig])
        else:
            signal_handlers[sig] = signal.getsignal(sig)   ; signal.signal(sig, handle_signal)
            signal.siginterrupt(sig, False)

    if action is None:
        while len(signals_caught):   os.kill(os.getpid(), signals_caught.pop(0))


def handle_signal(signum, frame):
    sys.stderr.write(" *** Caught signal "+str(signum))
    if signum not in signals_caught:   signals_caught.append(signum)


def ask_input(text):
    sys.stderr.write(text)
    return input()


# Exit with simple message
def x_it(code, text):
    sys.stderr.write(text+"\n")
    if tmpdir:   cleanup()
    sys.exit(code)


def cleanup():
    if debug:
        shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
        shutil.move(tmpdir, "/tmp/"+prog_name+"-debug")
    if not debug:
        shutil.rmtree(big_tmpdir, ignore_errors=True)
        for f in (big_tmpdir, tmpdir):
            shutil.rmtree(f, ignore_errors=True)
    if error_cache:
        sys.stderr.write("Error on volume(s): " + ", ".join(error_cache) + "\n")
        sys.exit(2)




##  MAIN  #########################################################################################

# Constants / Globals
prog_name             = "wyng"
prog_version          = "0.4.0alpha2"   ; prog_date = "20220903"
format_version        = 3               ; debug     = False       ; tmpdir = None
admin_permission      = os.getuid() == 0


if sys.hexversion < 0x3080000:
    x_it(1, "Python ver. 3.8 or greater required.")

if tuple(time.gmtime(0))[:6] != (1970, 1, 1, 0, 0, 0):
    x_it(1, "System time epoch is not 1970-01-01.")

# Allow only one instance at a time
lockpath = "/var/lock/"+prog_name
try:
    lockf = open(lockpath, "w")
    fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
except PermissionError:
    x_it(1, "ERROR: No writting permission on /var/lock/ . Run wyng as root.")
except IOError:
    x_it(1, "ERROR: "+prog_name+" is already running.")

cpu_flags = [x for x in open("/proc/cpuinfo") if x.startswith("flags")] [0].split()[1:]

# Disk block size:
bs                    = 512
# LVM min blocks = 128 = 64kBytes:
lvm_block_factor      = 128
# Smallest archive chunk size = 64kBytes:
bkchunksize           = 1 * lvm_block_factor * bs
assert bkchunksize % (lvm_block_factor * bs) == 0
max_address           = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split         = [len(hex(max_address))-2-7, 7]
hash_bits             = 256

os.environ["LC_ALL"]  = "C"    ; shell_prefix   = "set -e && export LC_ALL=C\n"

alphanumsym = "^[a-zA-Z0-9\+\._-]+$"
pjoin       = os.path.join    ; exists = os.path.exists

local_actions         = ("monitor","list","version")
write_actions         = ("add","send","prune","delete","rename","arch-delete","arch-deduplicate")
dest_actions          = ("receive","verify","diff","arch-check","arch-init") + write_actions

# Parse Arguments:
parser = argparse.ArgumentParser(description="")
parser.add_argument("action", choices=local_actions+dest_actions, help="Action to take")
parser.add_argument("-u", "--unattended", action="store_true", default=False,
                    help="Non-interactive, supress prompts")
parser.add_argument("--all-before", dest="allbefore", action="store_true", default=False,
                    help="Select all sessions before --session date-time.")
parser.add_argument("--all", action="store_true", default=False)
parser.add_argument("--session", help="YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS] or ^tag_id"
                                 " select session date|tag, singular or range.")
parser.add_argument("--tag", action="append",
                    help="tag_id[,description] Add tags when sending")
parser.add_argument("--keep", action="append", default=[],
                    help="YYYYMMDD-HHMMSS or ^tag_id exclude session by date|tag (prune)") 
parser.add_argument("--volex", default="", help="Exclude volume(s)")
parser.add_argument("--autoprune", default="off", help="Automatic pruning")
parser.add_argument("--from", dest="from_arch", default="",
                    help="Address+Path of other non-configured archive (receive, verify)") ####
parser.add_argument("--save-to", dest="saveto", default="",help="Path to store volume for receive")
parser.add_argument("--sparse", action="store_true", default=False, help="Retrieve differences only")
parser.add_argument("--sparse-write", action="store_true", default=False, help="Save differences only")
parser.add_argument("--remap", action="store_true", default=False, help="Remap snapshots")
parser.add_argument("--dest", default="",     help="URL to archive")
parser.add_argument("--dest-name", "-n", default="", help="Nickname for dest location")
parser.add_argument("--local", default="",    help="Init: LVM vg/pool containing source volumes")
parser.add_argument("--encrypt", default="xchacha20", help="Init: compression type:level")
parser.add_argument("--compression", default="", help="Init: compression type:level")
parser.add_argument("--hashtype", default="", help="Init: hash function type")
parser.add_argument("--chunk-factor", dest="chfactor", type=int,
                    help="Init: set chunk size to N*64kB")
parser.add_argument("--meta-dir", dest="metadir", default="", help="Use alternate metadata path")
parser.add_argument("--force", action="store_true", default=False, help="For arch-delete")
parser.add_argument("--clean", action="store_true", default=False, help="Clean snapshots, metadata")
parser.add_argument("--maxsync", action="store_true", default=False, help="Use more fs sync")
parser.add_argument("--debug", action="store_true", default=False, help="Debug mode")
parser.add_argument("--quiet", action="store_true", default=False)
parser.add_argument("--verbose", action="store_true", default=False)
parser.add_argument("--dedup", "-d", action="store_true", default=False,
                    help="Data deduplication (send)")
parser.add_argument("--volume-desc", dest="voldesc", default="",
                    help="Set volume description (add, rename, send)")
parser.add_argument("volumes", nargs="*")
options = parser.parse_args()    ; options.action = options.action.lower()

# Set stdout to devnull if --quiet is specified
debug = options.debug    ; assert not (options.quiet and (options.verbose or debug))
if debug:   options.verbose = True
if options.quiet and options.action not in ("list","version"):
    options.unattended = True
    sys.stdout = open(os.devnull, "w")

print("%s %s release %s" % (prog_name.capitalize(), prog_version, prog_date))

if options.action in write_actions+("arch-init","monitor") and not admin_permission:
    x_it(1, "Must be root/admin user for %s." % options.action)
if options.action == "version":   x_it(0,"")
if options.action not in ("send", "arch-deduplicate"):   options.dedup = False
if options.action == "arch-deduplicate":   options.dedup = True
if options.from_arch and options.action not in ("receive","verify","list","arch-init"):
    x_it(1,"--from option can be used only with: receive, verify, list, arch-init")


# Setup var, tmp and metadata dirs
vardir      = "/var/lib/"+prog_name
metadir     = options.metadir if options.metadir else "/var/lib"
topdir      = "/"+prog_name+".backup040" ####
tmpdir_obj  = tempfile.TemporaryDirectory(prefix=prog_name)   ; tmpdir = tmpdir_obj.name
big_tmpdir  = metadir+topdir+"/.tmp"

os.makedirs(vardir, exist_ok=True)
shutil.rmtree("/tmp/"+prog_name+"-debug", ignore_errors=True)
shutil.rmtree(big_tmpdir, ignore_errors=True)
os.makedirs(tmpdir+"/rpc")
if admin_permission:   os.makedirs(big_tmpdir, exist_ok=True)


## General Configuration ##

signal_handlers  = {}    ; signals_caught = []    ; error_cache = []

# Dict of compressors in the form: (library, default_compress_level, compress_func)
compressors      =    {"zlib":   (zlib, 4, zlib.compress),
                       "bz2" :   (bz2,  9, bz2.compress)}
if zstd:   compressors["zstd"] = (zstd, 3, lambda data, lvl: zstd.compress(data, lvl, 3))

hash_funcs       = {"sha256" : hashlib.sha256,
                    "blake2b": functools.partial(hashlib.blake2b, digest_size=hash_bits//8)}

monitor_only     = options.action == "monitor" # gather metadata without backing up

# Create ArchiveSet and Destination objects with get_configs().
# Passphrase input happens here; no stdin before this point!
aset = dest = None
aset, dest  = get_configs(options)
bkdir       = topdir+"/"+aset.name

if options.from_arch:
    aset         = get_configs_remote(dest, aset.name, aset.path)

# Get LVM listings
volgroups, l_vols= {}, {}    ; get_lvm_vgs()
if aset.vgname in volgroups:   l_vols = volgroups[aset.vgname].lvs
lvm_meta_snapshot(aset, "release")





# Check if local matches dest
if dest.online and options.action not in ("arch-init","arch-check","arch-delete") + local_actions:
    if not hmac.compare_digest(aset.raw_hashval, dest.archive_ini_hash) and dest.online:
        #if not exists(aset.confpath+".tmp"):
            #fetch_file_blobs([(aset.confname, aset.max_conf_sz)], aset.path, ext=".tmp")
            #test_aset = ArchiveSet(aset.name, metadir+topdir, ext=".tmp", children=0)
            #if float(aset.updated_at) > float(test_aset.updated_at):
                #update_dest(aset, [aset.confname])
            #else:
        #else:
            #raise NotImplementedError(".tmp recovery")

        # local copy is consistent, so update destination:
        update_dest(aset, pathlist=[aset.confname]) # Fix: expand checks from here....

    arch_check([], startup=True)





# Display archive update time as local time
arch_dt_raw = datetime.datetime.fromtimestamp(float(aset.updated_at), datetime.timezone.utc)
arch_dt     = arch_dt_raw.astimezone().isoformat(sep=u" ")
sys.stderr.write("Encrypted " if (aset.mcrypto and aset.datacrypto) else "Unencrypted ")
sys.stderr.write("archive last updated %s (%s)\n\n" % (arch_dt[:-6], arch_dt[-6:]))
if options.verbose:   sys.stderr.write("Dest location: %s\n" % dest.spec)

# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.
if aset.in_process and dest.online and not options.from_arch \
    and options.action not in ("arch-init","arch-delete","list"):

    if (options.action == "delete" and aset.in_process[1] == options.volumes[0]) \
        or (options.clean and options.force):
        # user is currently deleting the in_process volume or using --clean --force
        aset.set_in_process(None)
    elif aset.in_process:
        open(aset.path+"/in_process_retry","w").close()
    elif exists(aset.path+"/in_process_retry"):
        x_it(1, "Interrupted process already retried; Exiting.")

    if aset.in_process and aset.in_process[0] in ("delete","merge"):
        print("Completing prior operation in progress:", " ".join(aset.in_process[0:2]))

        if aset.in_process[0] == "delete":
            delete_volume(aset.in_process[1])
        elif aset.in_process[0] == "merge":
            merge_sessions(aset.in_process[1], aset.in_process[4].split(":|"),
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))

# Check volume args against config
exclude_vols  = set(options.volex.split(","))
datavols      = sorted(set(aset.vols.keys())  - exclude_vols)
selected_vols = sorted(set(options.volumes[:]))
for vol in selected_vols[:]:
    if vol not in aset.vols and options.action not in {"add","delete","rename","send"}:
        print("Volume "+vol+" not configured; Skipping.")
        del(selected_vols[selected_vols.index(vol)])


# Process Commands:

if options.action   == "monitor":
    monitor_send(selected_vols if selected_vols else datavols, monitor_only=True)


elif options.action == "send":
    monitor_send(selected_vols if selected_vols else datavols, monitor_only=False)


elif options.action == "prune":
    if options.autoprune.lower() == "off" and not options.session:
        x_it(1, "Must specify --autoprune or --session for prune.")
    dvs = selected_vols if selected_vols else datavols

    if not options.unattended and len(dvs):
        print("This operation will delete session(s) from the archive;")
        ans = ask_input("Are you sure? [y/N]: ").strip()
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    for dv in dvs:
        if dv in datavols:
            print("Pruning", dv)
            if options.session:
                prune_sessions(dv, options.session.split(","))
            elif options.autoprune.lower() in ("on","min","full"):
                autoprune(dv, apmode=options.autoprune)


elif options.action == "receive":
    if len(selected_vols) != 1:
        x_it(1, "Specify one volume for receive.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify only one session for receive.")
    receive_volume(selected_vols[0], select_ses="" if not options.session else options.session,
                   save_path=options.saveto if options.saveto else "")


elif options.action == "verify":
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify one session for verify.")

    for dv in selected_vols:
        receive_volume(dv,
                    select_ses="" if not options.session else options.session.split(",")[0],
                    verify_only=1, save_path="")


elif options.action == "diff":
    if len(selected_vols) != 1:
        x_it(1, "Specify one volume for diff.")
    if selected_vols:
        receive_volume(selected_vols[0], save_path="", diff=True)


elif options.action == "list":
    show_list(selected_vols)


elif options.action == "add":
    if len(options.volumes) < 1:
        x_it(1, "Volume name(s) required for 'add'.")

    for dv in options.volumes:
        if dv not in aset.vols:   add_volume(dv, options.voldesc)


elif options.action == "rename":
    if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
    rename_volume(options.volumes[0], options.volumes[1])


elif options.action == "delete":
    if options.clean:
        print("Remove local Wyng metadata from system for path",
              metadir+topdir if options.all else aset.path)
        if not options.unattended:
            ans = ask_input("Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:
                x_it(0,"")
        elif not options.force:
            x_it(1, "Ignoring --clean without --force.")
        remove_local_metadata(None if options.all else aset)

    else:
        delete_volume(selected_vols[0])


elif options.action == "arch-init":
    if not options.from_arch:
        update_dest(aset, pathlist=[aset.confname, aset.confname+".salt"])


elif options.action == "arch-check":
    arch_check(options.volumes)


elif options.action == "arch-delete":
    desthost = "//"+dest.sys+" " if dest.sys else ""
    print("\nDeleting ALL archived data from '%s' !" % dest.spec)

    if not options.unattended and not options.force:
        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")
    elif options.unattended and not options.force:
        x_it(1,"Error: --force required to delete entire archive.")

    # Remove metadata and snapshots
    for dv in list(aset.vols):   aset.delete_volume(dv)

    # Remove local metadata and stray snapshots associated with archive
    remove_local_metadata(aset)

    # Remove from destination
    print("\nDeleting entire archive...")
    cmd = [dest.cd
          +" && rm -rf ."+bkdir
          +" && { sync -f . || sync; }"
          ]
    dest.run(cmd)


elif options.action == "arch-deduplicate":
    dedup_existing()


cleanup()
# END
