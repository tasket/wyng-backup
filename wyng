#!/usr/bin/env python3


###  Wyng â€“ Logical volume backup tool
###  Copyright Christopher Laprise 2018-2021 / tasket@protonmail.com
###  Licensed under GNU General Public License v3. See file 'LICENSE'.



#  editor width: 100  -----------------------------------------------------------------------------
import sys, signal, os, stat, shutil, subprocess, time, datetime
import re, mmap, bz2, zlib, gzip, tarfile, io, fcntl, tempfile
import argparse, configparser, hashlib, hmac, functools, uuid
import xml.etree.ElementTree    ; from itertools import islice
from array import array         ; import resource

try:
    import zstd, warnings
except:
    zstd = None

try:
    from Cryptodome.Cipher import AES as Cipher_AES
    from Cryptodome.Random import get_random_bytes
    import Cryptodome.Util.Padding
except:
    Cipher_AES = None


# ArchiveSet manages configuration and configured volume info

class ArchiveSet:
    def __init__(self, name, top, allvols=False):
        self.name        = name
        self.path        = pjoin(top,name)
        self.confpath    = pjoin(self.path,"archive.ini")
        self.vols        = {}
        self.dedupindex  = {}
        self.dedupsessions = []
        self.in_process  = get_in_process(self.path)
        # persisted:
        self.chunksize   = bkchunksize
        self.compression = "zstd" if zstd else "zlib"
        self.compr_level = str(compressors[self.compression][1])
        self.hashtype    = "blake2b" if "blake2b" in hash_funcs else "sha256"
        self.vgname      = None
        self.poolname    = None
        self.destsys     = None
        self.destdir     = "."
        self.destmountpoint = None
        self.uuid        = None
        self.key         = ""
        self.cipher      = ""

        # parser for the .ini formatted configuration
        self.conf = cp   = configparser.ConfigParser()
        cp.optionxform   = lambda option: option
        cp["var"]        = {}
        cp["volumes"]    = {}

        # halt configuration if this is a new or temp config
        if not exists(self.confpath):
            self.uuid = str(uuid.uuid4())
            return

        cp.read(self.confpath)
        self.a_ints      = {"chunksize"}
        for name in cp["var"].keys():
            setattr(self, name, int(cp["var"][name]) \
                                if name in self.a_ints else cp["var"][name])

        # Fix: Remove filterwarnings when upstream fixes their code...
        if zstd and self.compression == "zstd":
            warnings.filterwarnings("ignore", category=DeprecationWarning)

        # load volume metadata objects
        for key in cp["volumes"]:
            # conditions when a volume must be loaded:
            # - allvols flag is set
            # - in_process flag from an unfinished action
            # - volume specified on command line
            # - no volumes specified (hence all)
            # - deduplication is in effect
            if allvols or self.in_process or options.from_arch or \
            (cp["volumes"][key] != "disable" and \
            (len(options.volumes)==0 or key in options.volumes or options.dedup)):
                os.makedirs(pjoin(self.path,key), exist_ok=True)
                # instantiate:
                self.vols[key] = self.Volume(self, key, pjoin(self.path,key), self.vgname)
                self.vols[key].enabled = cp["volumes"][key] != "disable"
                if options.dedup:   self.dedupsessions += self.vols[key].sessions.values()

        # Create master session list sorted for deduplication
        self.dedupsessions.sort(key=lambda x: x.localtime, reverse=True)


    def save_conf(self, fname=""):
        c = self.conf['var']
        c['chunksize']   = str(self.chunksize)
        c['compression'] = self.compression
        c['compr_level'] = self.compr_level
        c['hashtype']    = self.hashtype
        c['vgname']      = self.vgname
        c['poolname']    = self.poolname
        c['destsys']     = self.destsys
        c['destdir']     = self.destdir
        c['destmountpoint'] = self.destmountpoint
        c['uuid']        = self.uuid if self.uuid else str(uuid.uuid4())
        c['key']         = self.key
        c['cipher']      = self.cipher
        confdir  = os.path.dirname(self.confpath)   ; os.makedirs(confdir, exist_ok=True)
        confname = fname if fname else os.path.basename(self.confpath)
        with open(confdir+"/"+confname, "w") as f:
            self.conf.write(f)
            f.flush()    ; os.fsync(f.fileno())

    def add_volume(self, datavol):
        if datavol in self.conf["volumes"]:
            print(datavol+" is already configured.")    ; return True

        namecheck = self.Volume.volname_check(datavol)
        if namecheck:
            sys.stderr.write(namecheck+"\n")   ; error_cache.append(datavol)  ; return False

        if lv_exists(self.vgname, datavol+".tick") \
        and "arch-"+self.uuid not in volgroups[self.vgname].lvs[datavol+".tick"].tags:
            sys.stderr.write("Warning: Volume '%s' is already tracked"
                             "by a Wyng snapshot from a different archive!\n" % datavol)
        self.conf["volumes"][datavol] = "enable"
        self.save_conf()
        vol = self.vols[datavol] = self.Volume(self,datavol, pjoin(self.path,datavol), self.vgname)
        vol.save_volinfo()    ; print("Volume", datavol, "added to archive config.")
        return True


    def delete_volume(self, datavol):
        if datavol in self.conf["volumes"]:   del(self.conf["volumes"][datavol])
        self.save_conf()

        if exists(pjoin(self.path,datavol)):      shutil.rmtree(pjoin(self.path,datavol))

        for ext in {".tick",".tock"}:          lv_remove(self.vgname, datavol+ext, check=True)

    def rename_volume(self, datavol, newname, confname=""):
        if newname in self.conf["volumes"] or newname in os.listdir(self.path) \
        or self.Volume.volname_check(newname):
            return False

        if datavol in self.conf["volumes"] and datavol in self.vols:
            self.vols[datavol].save_volinfo()
            self.conf["volumes"][newname] = self.conf["volumes"][datavol]
            del(self.conf["volumes"][datavol])    ; self.save_conf(fname=confname)
        else:
            print("Config for %s not present." % datavol)
            return False

        return True


    class Volume:
        def __init__(self, archive, name, path, vgname):
            self.name    = name
            self.archive = archive
            self.path    = path
            self.vgname  = vgname
            self.enabled = False
            self.error   = False
            self.mapfile = path+"/deltamap"
            self.sessions= {}
            self.sesnames= []
            self.tags    = {}
            self.meta_checked = False
            # persisted:
            self.format_ver = "0"
            self.uuid    = None
            self.first   = "None"
            self.last    = "None"
            self.desc    = ""
            self.que_meta_update = "false"
            # other:
            self.changed_bytes = 0

            # load volume info
            if exists(pjoin(path,"volinfo")):
                with open(pjoin(path,"volinfo"), "r") as f:
                    for ln in f:
                        vname, value = ln.strip().split("=", maxsplit=1)
                        setattr(self, vname.strip(), value.strip())

            if exists(pjoin(path,"volchanged")):
                self.changed_bytes = int(open(pjoin(path,"volchanged"),"r").readlines()[0].strip())

            # load sessions as reverse-linked list, starting with the last;
            # store as dict and sequential list of session names
            if debug:  print("\nVOLUME",self.name, self.first, self.last)
            sprev = self.last ### if self.last else "None"
            while sprev != "None":
                if debug:  print(sprev, end="  ")
                # instantiate:
                s = self.sessions[sprev] = self.Ses(self, sprev, path+"/"+sprev)
                sprev = s.previous    ; self.sesnames.insert(0, s.name)
                if sprev == "None" and self.first != s.name:
                    raise ValueError("PREVIOUS MISMATCH: %s/%s, EXPECTED %s" 
                                     % (self.name, s.name, self.first))

            if int(self.format_ver) > format_version:
                raise ValueError("Archive format ver = "+self.format_ver+
                                 ". Expected = "+format_version)
            elif len(self.sessions) and self.format_ver == "0":
                raise ValueError("No format_version; Alpha archive?")


        def volname_check(vname):
            check = re.compile(alphanumsym)
            if check.match(vname) is None:
                return "Only characters A-Z 0-9 . + _ - are allowed in volume names."
            if vname in (".",".."):
                return "Bad volume name."
            if len(vname) > volname_len:
                return "Volume name must be %d characters or less." % volname_len

            return "" # OK

        def volsize(self):
            return self.sessions[self.last].volsize if self.sessions else 0

        def last_chunk_addr(self, vsize=None):
            if vsize is None:  vsize = self.volsize()
            return (vsize-1) - ((vsize-1) % self.archive.chunksize)

        #def map_used(self):
            ## fix: account for -tmp filename
            #return os.stat(self.mapfile).st_blocks

        # Based on last session size unless volume_size is specified.
        def mapsize(self, volume_size=None):
            if not volume_size:
                volume_size = self.volsize()
            return (volume_size // self.archive.chunksize // 8) + 1

        def save_volinfo(self, fname="volinfo", prune=False):
            os.makedirs(self.path, exist_ok=True)
            with open(pjoin(self.path,fname), "w") as f:
                print("format_ver =", format_version, file=f)
                print("uuid =", self.uuid if self.uuid else str(uuid.uuid4()), file=f)
                print("first =", self.first, file=f)
                print("last =", self.last, file=f)

                if not prune:   self.desc = options.voldesc.strip()[:comment_len]
                print("desc =", self.desc, file=f)
                print("que_meta_update =", self.que_meta_update, file=f)
                f.flush()    ; os.fsync(f.fileno())

        def changed_bytes_add(self, amount, reset=False, save=False):
            if reset:
                if exists(self.path+"/volchanged"):   os.remove(self.path+"/volchanged")
                self.changed_bytes = 0  ; return

            self.changed_bytes += amount
            if save:
                with open(self.path+"/volchanged", "w") as f:
                    print(self.changed_bytes, file=f)
                    f.flush()    ; os.fsync(f.fileno())

        def new_session(self, sname, addtags={}):
            ns = self.Ses(self, sname, addtags=addtags)
            ns.path = pjoin(self.path, sname)
            if self.first == "None":
                ns.sequence = 0
                self.first = sname
            else:
                ns.previous = self.last
                ns.sequence = self.sessions[self.last].sequence + 1

            self.last = sname
            self.sesnames.append(sname)
            self.sessions[sname] = ns
            if options.dedup:    self.archive.dedupsessions.append(ns)
            #os.makedirs(pjoin(self.path, sname))
            return ns

        def delete_session(self, sname, remove=True, force=False):
            ses     = self.sessions[sname]
            index   = self.sesnames.index(sname)    ; affected = None
            if sname == self.last and ses.saved and not force:
                raise ValueError("Cannot delete last session")

            for tag in list(ses.tags.keys()):   self.sessions[sname].tag_del(tag)
            del(self.sesnames[index], self.sessions[sname])

            if self.archive.dedupsessions:
                indexdd = self.archive.dedupsessions.index(ses)
                self.archive.dedupsessions[indexdd] = None

            # Following condition means:
            #   * sesnames cannot be empty
            #   * ses wasn't deleted from end of list

            if len(self.sesnames) > index:
                affected = self.sesnames[index]
                self.sessions[affected].previous = ses.previous

            self.first = self.sesnames[0] if len(self.sesnames) else "None"
            self.last  = self.sesnames[-1] if len(self.sesnames) else "None"

            if remove and exists(pjoin(self.path, sname)):   shutil.rmtree(ses.path)
            return affected


        class Ses:
            def __init__(self, volume, name, path="", addtags={}):
                self.name     = name
                self.path     = path
                self.saved    = False
                self.volume   = volume
                self.toggle   = True
                self.meta_checked = False
                # persisted:
                self.uuid     = None
                self.localtime= None
                self.volsize  = None
                self.format   = None
                self.sequence = None
                self.previous = "None"
                self.tags     = {}
                attr_str      = ("localtime","format","previous","uuid")
                attr_int      = ("volsize","sequence")

                if path:
                    with open(pjoin(path,"info"), "r") as sf:
                        for ln in sf:
                            if ln.strip() == "uuid =":  continue
                            vname, value = ln.split("=", maxsplit=1)
                            vname = vname.strip()    ; value = value.strip()
                            if value == "none":   value = "None"
                            if vname == "tag":
                                self.tag_add(self.tag_parse(value))
                                continue

                            setattr(self, vname, 
                                int(value) if vname in attr_int else value)

                    if not exists(pjoin(path,"manifest")):
                        raise FileNotFoundError("ERROR: Manifest does not exist for "+name)
                    self.saved = True

                for tag in addtags:   self.tag_add(tag)


            def tag_parse(self, tag, delim=" "):
                parts = tag.strip().split(delim, maxsplit=1)    ; tag_id = parts[0].strip().lower()
                result = tuple()
                if len(tag_id) > tag_len:
                    sys.stderr.write("Error: Max "+tag_len+" alphanumeric chars for tag ID.\n")
                elif not re.match(alphanumsym, tag_id):
                    sys.stderr.write("Error: Allowed tag ID chars are "+alphanumsym+"\n")
                elif tag_id == "all":
                    sys.stderr.write("Error: tag 'all' is reserved.")
                else:
                    result = (tag_id, "" if len(parts) == 1 else parts[1].strip())
                return result

            def tag_add(self, tag):
                self.saved = False    ; voltags = self.volume.tags    ; tid = tag[0]
                if tid not in self.tags:   self.tags[tid] = tag[1]
                if tid in voltags:
                    voltags[tid].add(self.name)
                else:
                    voltags[tid] = {self.name}
                return True

            def tag_del(self, tag):
                del(self.tags[tag])       ; voltags = self.volume.tags
                if tag in voltags:
                    if self.name in voltags[tag]:   voltags[tag].remove(self.name)
                    if len(voltags[tag]) == 0:    del(voltags[tag])

            def save_info(self, fname="info"):
                if not self.path:
                    raise ValueError("Path not set for save_info")
                self.saved = True
                with open(pjoin(self.path,fname), "w") as f:
                    print("uuid =",      self.uuid, file=f)
                    print("localtime =", self.localtime, file=f)
                    print("volsize =",   self.volsize, file=f)
                    print("format =",    self.format, file=f)
                    print("sequence =",  self.sequence, file=f)
                    print("previous =",  self.previous, file=f)
                    for tkey, tdesc in self.tags.items():
                        print("tag =",   tkey, tdesc, file=f)
                    f.flush()    ; os.fsync(f.fileno())

# END class ArchiveSet, Volume, Ses


# DataCryptography(): Handle crypto functions and state for volume data

class DataCryptography():

    crypto_key_bits  =  256    ; crypto_ciphers  =  {"aes-256-siv", "aes-256-cbc"}

    def __init__(self, ci_type, key=None):
        self.get_rnd = get_random_bytes
        self.ci_type = ci_type

        if ci_type == "aes-256-cbc":
            self.key     = key if key else self.get_rnd(self.crypto_key_bits//8)
            self.iv_sz   = 16
            self.blk_sz  = Cipher_AES.block_size   ; self.ps_mask = self.blk_sz-1
            self.mode    = Cipher_AES.MODE_CBC     ; self.AES_new = Cipher_AES.new
            self.encrypt = self._enc_aes_256_cbc_rndpad
            self.decrypt = self._dec_aes_256_cbc_rndpad

        elif ci_type == "aes-256-siv":
            self.key     = key if key else self.get_rnd(2*self.crypto_key_bits//8)
            self.blk_sz  = Cipher_AES.block_size
            self.nonce_sz= 16                      ; self.tag_sz  = 16
            self.mode    = Cipher_AES.MODE_SIV     ; self.AES_new = Cipher_AES.new
            self.encrypt = self._enc_aes_256_siv
            self.decrypt = self._dec_aes_256_siv


    # Encrypt aes-256-cbc:
    # A random 'bolster' block is added as a prefix to the plaintext buffer;
    # This might be enhanced by using a counter instead of rnd.
    # A random padding is also used and 'buf' must be prefixed by len % blk_sz (1 byte)
    def _enc_aes_256_cbc_rndpad(self, buf):
        #### ADD UNIQUE IV
        blk_sz = self.blk_sz
        cipher = self.AES_new(self.key, self.mode)
        pad_sz = blk_sz-(len(buf)%blk_sz) & self.ps_mask # buf[0]
        if pad_sz:   buf += self.get_rnd(pad_sz)

        buf    = cipher.encrypt( self.get_rnd(blk_sz)
                                + buf ) + pad_sz.to_bytes(length=1, byteorder="little")
        return cipher.iv + buf

    # Decrypt aes-256-cbc:
    def _dec_aes_256_cbc_rndpad(self, untrusted_buf):
        iv = untrusted_buf[:self.iv_sz]    ; blk_sz = self.blk_sz
        pad_sz = untrusted_buf[-1] & self.ps_mask

        cipher = self.AES_new(self.key, self.mode, iv)
        untrusted_buf = cipher.decrypt(untrusted_buf[self.iv_sz:-1])

        return  untrusted_buf[blk_sz: -pad_sz if pad_sz else None]

    # Encrypt aes-256-siv:
    def _enc_aes_256_siv(self, buf):
        #### ADD UNIQUE IV
        nonce  = self.get_rnd(self.nonce_sz)
        cipher = self.AES_new(self.key, self.mode, nonce=nonce)
        buf, ci_tag = cipher.encrypt_and_digest(buf)
        return  nonce + ci_tag + buf

    # Decrypt aes-256-siv:
    def _dec_aes_256_siv(self, untrusted_buf):
        nonce  = untrusted_buf[:self.nonce_sz]    ; buf_start = self.nonce_sz+self.tag_sz
        ci_tag = untrusted_buf[self.nonce_sz:buf_start]
        cipher = self.AES_new(self.key, self.mode, nonce)
        return cipher.decrypt_and_verify(untrusted_buf[buf_start:], ci_tag)


# Define absolute paths of commands

class CP:
    awk  = "/usr/bin/awk"    ; sed   = "/bin/sed"        ; sort     = "/usr/bin/sort"
    cat  = "/bin/cat"        ; mkdir = "/bin/mkdir"      ; python   = "/usr/bin/python3"
    mv   = "/bin/mv"         ; grep  = "/bin/grep"       ; ssh      = "/usr/bin/ssh"
    sh   = "/bin/sh"         ; tar   = "/bin/tar"        ; touch    = "/usr/bin/touch"
    rm   = "/bin/rm"         ; lvm   = "/sbin/lvm"       ; qvm_run  = "/usr/bin/qvm-run"
    tee  = "/usr/bin/tee"    ; sync  = "/bin/sync"       ; dmsetup  = "/sbin/dmsetup"
    find = "/usr/bin/find"   ; xargs = "/usr/bin/xargs"  ; sha256sum= "/usr/bin/sha256sum"
    cmp  = "/usr/bin/cmp"    ; gzip  = "/bin/gzip"       ; diff     = "/usr/bin/diff"
    mountpoint = "/bin/mountpoint"    ; thin_delta = "/usr/sbin/thin_delta"
    blkdiscard = "/sbin/blkdiscard"   ; nice       = "/usr/bin/nice"


class Lvm_VolGroup:
    def __init__(self, name):
        self.name  = name
        self.lvs   = {}
        self.poolnames = set()
        self.gc_procs = []
        self.clean = options.clean

    def __del__(self):
        if self.clean:
            for p in self.gc_procs:   p.wait()


class Lvm_Volume:
    colnames  = ["vg_name","lv_name","lv_attr","lv_size","lv_time","lv_uuid",
                 "pool_lv","thin_id","transaction_id","tags"]
    attr_ints = ["lv_size"]

    def __init__(self, members):
        for attr in self.colnames:
            val = members[self.colnames.index(attr)]
            if attr == "tags":
                setattr(self, attr, val.split(","))   ; uuid = None
            else:
                setattr(self, attr, int(re.sub("[^0-9]", "", val)) if attr \
                    in self.attr_ints else val)

        if uuid:   setattr(self, "lv_uuid", uuid)


# Retrieves survey of all LVs as vgs[].lvs[] dicts

def get_lvm_vgs(vg_to_lvols=None):
    global volgroups, l_vols
    l_vols = None    ; volgroups = {}

    do_exec([[CP.lvm, "lvs", "--units=b", "--noheadings", "--separator=::",
                "--options=" + ",".join(Lvm_Volume.colnames)]],
            out=tmpdir+"/volumes.lst")

    with open(tmpdir+"/volumes.lst", "r") as vlistf:
        for ln in vlistf:
            members = ln.strip().split("::")
            vgname = members[0] # Fix: use colname index
            lvname = members[1]    ; lv = Lvm_Volume(members)
            if vgname not in volgroups.keys():
                volgroups[vgname] = Lvm_VolGroup(vgname)
            volgroups[vgname].lvs[lvname] = lv
            if lv.pool_lv:   volgroups[vgname].poolnames.add(lv.pool_lv)

    if vg_to_lvols:  l_vols = volgroups[vg_to_lvols].lvs


def lv_exists(vgname, lvname):
    return vgname in volgroups.keys() \
            and lvname in volgroups[vgname].lvs.keys()


def vg_exists(vgname):
    try:
        do_exec([[CP.lvm, "vgdisplay", vgname]])
    except subprocess.CalledProcessError:
        return False
    else:
        return True


# Converts a non-cannonical LV path to LV name plus pool and vg names.
def get_lv_path_pool(path):
    try:
        p = subprocess.run([CP.lvm, "lvs", "--separator=::", "--noheadings",
                            "--options=lv_name,pool_lv,vg_name", path], check=True,
                            stdout=subprocess.PIPE, stderr=subprocess.DEVNULL)
    except subprocess.CalledProcessError:
        return "", "", ""
    else:
        return p.stdout.decode("utf-8").strip().split("::")


def lv_remove(vgname, lvname, sync=True, check=False):
    # Enh: re-write with asyncio
    assert not (sync == False and check)
    if lv_exists(vgname, lvname):
        del(volgroups[vgname].lvs[lvname])
        procs = volgroups[vgname].gc_procs    ; maxprocs = 16
        if len(procs) == maxprocs:
            clean = options.clean
            for ii in reversed(range(len(procs))):
                retcode = procs[ii].returncode
                if retcode is not None:
                    if clean and retcode != 0:
                        raise CalledProcessError("lvremove failed "+str(retcode))
                    del(procs[ii])

        cmds = [CP.sh, "-c", CP.lvm + " lvchange -p rw " + vgname+"/"+lvname + " ; "  \
               + CP.blkdiscard + " /dev/" + vgname+"/"+lvname    + " ; "  \
               + CP.lvm + " lvremove -f " + vgname+"/"+lvname]
        if not (sync or options.clean):   cmds.insert(0, CP.nice)
        p = subprocess.Popen(cmds, shell=False,
                             stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        if not sync and len(procs) < maxprocs:
            procs.append(p)
        else:
            retcode = p.wait()
            if check and retcode != 0:   raise CalledProcessError("lvremove failed "+str(retcode))


def lv_rename(vgname, vol1, vol2):
    do_exec([[CP.lvm, "lvrename", vgname+"/"+vol1, vol2]])    ; lvs = volgroups[vgname].lvs
    lvs[vol2] = lvs[vol1]    ; lvs[vol2].lv_name = vol2    ; del(lvs[vol1])


# Reserve or release lvm thinpool metadata snapshot.
# arch must be a configured ArchiveSet; action must be "reserve" or "release".
# Specifying "release" without "pool=" will attempt to release all pools.
def lvm_meta_snapshot(arch, action, pool=None):
    vgname   = arch.vgname.replace("-","--")
    the_pool = pool if pool else arch.poolname
    checkerr = action == "reserve"

    for pool in volgroups[arch.vgname].poolnames if not pool and action == "release" \
                else [the_pool]:
        poolname = pool.replace("-","--")
        do_exec([[CP.dmsetup,"message", vgname+"-"+poolname+"-tpool",
                "0", action+"_metadata_snap"]], check=checkerr)


# Try to sync only selected filesystem
def fssync(path):
    if subprocess.call([CP.sync,"-f",path]) != 0:   subprocess.call([CP.sync])


# Initialize a new ArchiveSet:

def arch_init(aset):
    if not options.from_arch:
        if not options.local or not options.dest:
            x_it(1,"--local and --dest are required together.")
    elif options.dest:
        x_it(1,"--from and --dest cannot be used together.")
    if options.local:
        vgname, poolname = options.local.split("/")
        if not vg_exists(vgname):
            print("Warning: Volume group '%s' does not exist." % vgname)
        if not lv_exists(vgname, poolname):
            print("Warning: LV pool '%s' does not exist." % poolname)
        aset.vgname   = vgname
        aset.poolname = poolname

    dest    = options.from_arch if options.from_arch else options.dest
    destsys = delim = mountpoint = ""
    for i in url_types:
        if dest.startswith(i):
            destsys, delim, mountpoint = dest.replace(i,"",1).partition("/")
            break
    if (not mountpoint and not delim) or (i != "internal:" and not destsys) or \
       (i == "qubes-ssh://" and ("" in destsys.split(":") or len(destsys.split(":"))<2)):
        x_it(1,"Error: Malformed --dest specification.")

    aset.destsys        = i+destsys
    aset.destmountpoint = delim+mountpoint

    if options.subdir:
        if options.subdir.strip()[0] == "/":
            x_it(1,"Subdir cannot be absolute path.")
        aset.destdir    = options.subdir.strip()

    if options.from_arch:
        return

    if options.encrypt.lower() in DataCryptography.crypto_ciphers:
        aset.cipher = options.encrypt.lower()
        aset.key    = DataCryptography(aset.cipher).key.hex()
    else:
        aset.cipher = "off"

    print("Encryption =", aset.cipher)

    if options.hashtype:
        if options.hashtype not in hash_funcs:
            x_it(1, "Hash function '"+options.hashtype+"' is not available on this system.")
        aset.hashtype = options.hashtype

    print("Hashing =", aset.hashtype)

    if options.compression:
        if ":" in options.compression:
            compression, compr_level = options.compression.strip().split(":")
        else:
            compression = options.compression.strip()
            compr_level = str(compressors[compression][1])
        compression = compression.strip()   ; compr_level = compr_level.strip()
        if compression not in compressors.keys() or not is_num(compr_level):
            x_it(1, "Invalid compression spec.")
        aset.compression = compression      ; aset.compr_level = compr_level

    print("Compression = %s:%s" % (aset.compression, aset.compr_level))

    if options.chfactor:
        # accepts an exponent from 1 to 6
        if not ( 0 < options.chfactor < 7 ):
            x_it(1, "Requested chunk size not supported.")
        aset.chunksize = (bkchunksize//2) * (2** options.chfactor)
        if aset.chunksize > 256 * 1024:
            print("Large chunk size set:", aset.chunksize)

    aset.save_conf()


# Check/verify an entire archive

def arch_check(vol_list=None, startup=False):
    gethash  = hash_funcs[aset.hashtype]   ; chunksize = aset.chunksize
    attended = not options.unattended      ; stray_dirs= []
    compare_digest = hmac.compare_digest

    if aset.key:
        crypto_key = bytes.fromhex(aset.key)
        decrypt    = DataCryptography(aset.cipher, key=crypto_key).decrypt
    else:
        crypto_key = None

    if not vol_list:   vol_list = [x.name for x in os.scandir(aset.path) if x.is_dir()]

    # Remove orphan snapshots
    for lv in list(l_vols.values()):
        if "wyng" in lv.tags and lv.lv_name.endswith((".tick",".tock")) \
        and "arch-"+aset.uuid in lv.tags:
            basename = lv.lv_name[:-5]
            if basename not in aset.conf["volumes"] \
            or (options.clean and basename not in l_vols.keys()):
                print("Removing orphan snapshot:", lv.lv_name)
                lv_remove(lv.vg_name, lv.lv_name, check=True, sync=True)

    # Check volume contents at various levels (dirs, metadata, content)
    for volname in vol_list if dest_online else []:

        # Handle disabled volumes and stray dirs
        if startup and volname not in aset.vols:   continue
        if volname not in aset.conf["volumes"]:
            print("Stray dir '%s'" % volname)    ; stray_dirs.append(volname)
            if not startup:   continue
        elif aset.conf["volumes"][volname] == "disable":
            if startup:       continue
            aset.vols[volname] = ArchiveSet.Volume(aset, volname,aset.path+"/"+volname,aset.vgname)

        vol = aset.vols[volname]
        if not startup or options.debug:   print("\nChecking volume", volname, flush=True)

        # Remove session tmp dirs
        for sdir in os.scandir(vol.path):
            if sdir.name.startswith("S_") and sdir.name.endswith("-tmp"):
                if options.debug:   print("Removing partial session dir '%s'" % sdir.name)
                dest_run([destcd+bkdir + " && rm -rf " + volname+"/"+sdir.name])
                shutil.rmtree(vol.path+"/"+sdir.name, ignore_errors=True)

        if startup:   continue

        if compare_files(volumes=[vol], sessions=list(vol.sessions.values())):
            x_it(1, "Error: Local and archive metadata differ.")

        # Check index sequencing in first and combined manifests
        print("  Checking index sequence...")
        mset = []
        for ses in vol.sesnames:
            # print(ses, "size =", vol.sessions[ses].volsize)
            mset.append(ses)
            check_manifest_sequence(volname, mset)

        # Check hashes of each session individually
        print("  Checking data...")
        for sesname in reversed(vol.sesnames):
            ses = vol.sessions[sesname]    ; manifest = ses.path+"/manifest"    ; bcount = 0
            if options.session \
            and ((options.session.lower() == "newest" and sesname != vol.sesnames[-1]) \
            or options.session < ses.localtime):
                continue # Enh: use seq
            if attended:   print("  Session %s... " % sesname, end="", flush=True)

            # FixCheck for stray entries here
            cmd1 = [[CP.sed, "-E", "/^0\s/ d; "
                    +"s|^\S+\s+x(\S{" +str(address_split[0])+ "})(\S+)|"+sesname+"/\\1/x\\1\\2|",
                    manifest],
                    [CP.gzip, "-c", "-4"],
                    dest_run_args(desttype, ["cat >"+tmpdir+"-rpc/dest.lst.gz"])]
            do_exec(cmd1)

            cmd2 = dest_run_args(desttype,
                    [destcd + bkdir+"/"+volname
                    +" && exec 2>>"+tmpdir+"-rpc/receive.log"
                    +" && python3 "+tmpdir+"-rpc/dest_helper.py receive"
                    ])
            getses = subprocess.Popen(cmd2, stdout=subprocess.PIPE)

            # Open manifest then receive, check and save data
            with open(manifest, "r") as mf:
                for mfline in mf:
                    cksum, fname = mfline.split()
                    if cksum.strip() == "0":   continue

                    # Read chunk size
                    untrusted_size = int.from_bytes(getses.stdout.read(4),"big")
                    # allow for slight expansion from compression algo
                    if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
                        raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

                    # Size is OK.
                    size = untrusted_size    ; bcount += size
                    # Read chunk buffer
                    untrusted_buf = getses.stdout.read(size)
                    rc  = getses.poll()
                    if rc is not None and len(untrusted_buf) == 0:   break

                    if len(untrusted_buf) != size:
                        with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
                        print(mfline)
                        raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))

                    # Decrypt the data chunk
                    # Validation MUST be next step!
                    if crypto_key:
                        untrusted_buf = decrypt(untrusted_buf)

                    # Validate data
                    if not compare_digest(cksum, gethash(untrusted_buf).hexdigest()):
                        with open(tmpdir+"/bufdump", "wb") as dump:   dump.write(untrusted_buf)
                        print(mfline)
                        raise ValueError("Bad hash "+fname+" :: "+gethash(untrusted_buf).hexdigest())

            if attended:   print(bcount, "bytes OK")


def check_manifest_sequence(datavol, sesnames):
    vol = aset.vols[datavol]     ; volsize = vol.sessions[sesnames[-1]].volsize
    with open(merge_manifests(datavol, msessions=sesnames, addcol=False), "r") as mrgf:
        for addr in range(0, volsize, aset.chunksize):
            ln = mrgf.readline().strip()
            if not ln:   break
            ln1, ln2 = ln.split()
            assert (len(ln1) == 64 or ln1 == "0")    ; h1 = int(ln1, 16)
            assert len(ln2) == 17    ; a1 = int("0"+ln2, 16)
            if addr != a1:
                print(ln); raise("Manifest seq error. Expected %d got %d." % (addr, a1))

    if addr+aset.chunksize != volsize:
        raise ValueError("Manifest range stopped short at", addr)


# Get global configuration settings:

def get_configs():
    aname = "default"    ; bkdir = topdir + "/" + aname

    if options.from_arch:
        # Prepare a temporary metadata dir and init aset with it
        tmpmeta = tmpdir+"/var"    ; os.makedirs(tmpmeta+bkdir)
        aset    = ArchiveSet(aname, tmpmeta+topdir)
        arch_init(aset)
    else:
        aset    = ArchiveSet(aname, metadir+topdir)

    if aset.vgname:   lvm_meta_snapshot(aset, "release")

    if options.from_arch:
        return aset

    if options.action == "arch-init" and not aset.destmountpoint:
        arch_init(aset)
        x_it(0, "Done.")
    elif options.action == "arch-init":
        x_it(1, "Archive already initialized for "+aset.name)
    if not aset.destmountpoint:
        x_it(1,"Configuration not found.")

    return aset


def get_configs_remote():
    make_local = options.action == "arch-init"
    if not options.unattended:
        print("\nMetadata will be read from the archive;"
              "\nPlease check that the archive is in a trusted, secure condition!")
        ans = ask_input("Proceed? [y/N] ")
        if ans.lower() not in ("y","yes"):
            x_it(0,"Stopped.")

    dest_run([destcd + bkdir +"  && "+CP.python+" "+tmpdir+"-rpc/dest_helper.py get-configs "
              +(options.volumes[0] if len(options.volumes)>0 else "")
             ], out=tmpdir+"/metadata.tgz")

    do_exec([[CP.tar,"-xzf",tmpdir+"/metadata.tgz"]], cwd=aset.path)
    if not exists(aset.path+"/archive.ini"):
        x_it(1,"Failed to retrieve remote configuration: No info.")

    if make_local:
        if exists(metadir+bkdir):
            os.replace(metadir+bkdir,
                       metadir+topdir+"/wyng.old/"+aset.name+"--"+time.strftime("%m%d-%H%M%S"))
        shutil.copytree(aset.path, metadir+bkdir)
        new_path = metadir+topdir
    else:
        new_path = os.path.dirname(aset.path)

    # Create final ArchiveSet object with imported config & current dest+local
    new_aset = ArchiveSet(aset.name, new_path)   ; new_aset.destsys = aset.destsys
    new_aset.destdir = aset.destdir              ; new_aset.destmountpoint= aset.destmountpoint
    if options.local:
        vgname, poolname = options.local.split("/")
        if not vg_exists(vgname):
            print("Warning: Volume group '%s' does not exist." % vgname)
        if not lv_exists(vgname, poolname):
            print("Warning: LV pool '%s' does not exist." % poolname)
        new_aset.vgname   = vgname    ; new_aset.poolname = poolname
        lvm_meta_snapshot(new_aset, "release")

    new_aset.save_conf()
    return new_aset


# Detect features of internal and destination environments:

def detect_internal_state():

    destsys = aset.destsys    ; desttype = None
    for dt in url_types:
        if destsys.find(dt) == 0:
            desttype = dt.rstrip(":/")    ; destsys = destsys[len(dt):]
            break
    if not desttype:
        raise ValueError("'%s' not an accepted type." % destsys)

    #    CP.thin_delta, CP.lvm, CP.blkdiscard,
    for prg in (CP.sort, CP.sed, CP.ssh if desttype=="ssh" else CP.sh):
        if not shutil.which(prg):
            raise RuntimeError("Required command not found: "+prg)

    try:
        p = subprocess.check_output([CP.thin_delta, "-V"])
    except:
        p = b""
    ver = p[:5].decode("UTF-8").strip()    ; target_ver = "0.7.4"
    if p and ver < target_ver:
        print("Note: Thin provisioning tools version", target_ver,
              "or later is recommended for stability. Installed version =", ver+".")


    #####>  Begin helper program  <#####

    dest_program = \
    '''#  Copyright Christopher Laprise 2018-2021
#  Licensed under GNU General Public License v3. See github.com/tasket/wyng-backup
import os, sys, signal, shutil, subprocess, gzip, tarfile
cmd = sys.argv[1] ;    tmpdir = "''' + tmpdir + '''-rpc"
exists = os.path.exists    ; replace = os.replace    ; remove = os.remove

def fssync(path):
    if subprocess.call(["sync","-f",path]) != 0:   subprocess.call(["sync"])

def catch_signals():
    for sig in (signal.SIGINT, signal.SIGTERM, signal.SIGQUIT, signal.SIGABRT, signal.SIGALRM,
                signal.SIGTSTP, signal.SIGUSR1):
        signal.signal(sig, signal.SIG_IGN)    ; signal.siginterrupt(sig, False)

def helper_send():
    mkdirs = os.makedirs    ; hlink = os.link    ; dirname = os.path.dirname
    with tarfile.open(mode="r|", fileobj=sys.stdin.buffer) as tarf:
        extract = tarf.extract    ; substitutions = {}    ; dirlist = set()
        for member in tarf:   sdir = member.name    ; mkdirs(sdir)    ; print(sdir)    ; break
        for member in tarf:
            if not member.islnk():
                extract(member, set_attrs=False)
            else:
                source = src_orig = member.linkname   ; dest = member.name   ; ddir = dirname(dest)
                if source in substitutions:   source = substitutions[source]
                if ddir not in dirlist:   mkdirs(ddir, exist_ok=True)   ; dirlist.add(ddir)
                try:
                    hlink(source, dest)
                except OSError as err:
                    print(err)    ; print("Substitution:", source, dest)
                    shutil.copyfile(source, dest)    ; substitutions[src_orig] = dest

def helper_receive(lstf):
    stdout_write = sys.stdout.buffer.write   ; exists = os.path.exists   ; getsize= os.path.getsize
    stdout_flush = sys.stdout.flush
    for line in lstf:
        fname = line.strip()
        if not fname:   break
        if exists(fname):
            fsize = getsize(fname)
        else:
            fsize = 0   ; i = sys.stderr.write("File not found: %s\\x0a" % fname)
        stdout_write(fsize.to_bytes(4,"big"))
        if fsize:
            with open(fname,"rb") as dataf:
                stdout_write(dataf.read(fsize))
        stdout_flush()

def helper_merge():
    exists = os.path.exists    ; replace = os.replace    ; remove = os.remove
    try:
        if resume:
            if exists("merge-init") or not exists("merge"):
                raise RuntimeError("Merge: Init could not complete; Aborting merge.")
        else:
            print("Merge: Initialization.")
            for f in (target+"/info", target+"/manifest", "volinfo"):
                if not exists(f+".tmp"):  raise FileNotFoundError(f)
            for ex in ("","-init"):  shutil.rmtree("merge"+ex, ignore_errors=True)
            os.makedirs("merge-init")   ; replace(merge_target, "merge-init/"+merge_target)
            for src in src_list:   replace(src, "merge-init/"+src)
            replace("merge-init", "merge")    ; fssync(".")
    except Exception as err:
        if exists("merge-init"):
            for i in os.scandir("merge-init"):
                if i.is_dir() and i.name.startswith("S_"):   replace(i.path, i.name)
        fssync(".")    ; print(err)    ; sys.exit(50)
    try:
        os.chdir("merge")  #  CD
        if not resume or not exists("CHECK-mv-rm"):
            print("Merge: remove/replace files.")    ; subdirs = set()
            for src in src_list:  # Enh: replace os.scandir w manifest method
                for i in os.scandir(src):
                    if i.is_dir():   subdirs.add(i.name)
            for sdir in subdirs:   os.makedirs(merge_target+"/"+sdir, exist_ok=True)
            for line in lstf:
                ln = line.split() # default split() does strip()
                if ln[0] == "rename" and (not resume or exists(ln[1])):
                    replace(ln[1], ln[2])
                elif ln[0] == "-rm" and exists(ln[1]):
                    remove(ln[1])
            fssync(".")    ; open("CHECK-mv-rm","w").close()
    except Exception as err:
        print(err)    ; sys.exit(60)

def helper_merge_finalize():
    try:
        print("Merge: Finalize target")
        os.chdir("merge")  #  CD                 ; open("CHECK-start-finalize","w").close()
        for f in ("/info", "/manifest"):
            if not resume or exists(target+f+".tmp"):  replace(target+f+".tmp", merge_target+f)
        os.chdir("..")     #  CD
        if not resume or exists("volinfo.tmp"):    replace("volinfo.tmp", "volinfo")
        if not resume or not exists(target):       replace("merge/"+merge_target, target)
        if not exists(target):   raise FileNotFoundError(target)
        fssync(".")
    except Exception as err:
        print(err)    ; sys.exit(70)
    shutil.rmtree("merge", ignore_errors=True)
    print("wyng_check_free", shutil.disk_usage(".").free, flush=True)

## MAIN ##
if "--finalize" in sys.argv:   catch_signals()
if cmd not in ("merge","merge-finalize","send","rename") and exists(tmpdir+"/dest.lst.gz"):
    lstf = gzip.open(tmpdir+"/dest.lst.gz", "rt")
else:
    lstf = None
if cmd == "merge":
    if not exists("../archive.ini") or not exists("volinfo"):
        print("Error: Not in volume dir.")   ; sys.exit(40)
    src_list = []    ; resume = "--resume" in sys.argv
    lstf = gzip.open("merge.lst.gz", "rt")
    merge_target, target = lstf.readline().split()
    while True:
        ln = lstf.readline().strip()
        if ln == "###":  break
        src_list.append(ln)
    if "--finalize" in sys.argv:
        helper_merge_finalize()
    else:
        helper_merge()
elif cmd == "receive":
    helper_receive(lstf if lstf else sys.stdin)
elif cmd == "send":
    helper_send()
elif cmd == "dedup":
    ddcount = 0    ; substitutions = {}
    for line in lstf:
        source, dest = line.split()   ; src_orig = source
        if source in substitutions:   source = substitutions[source]
        if os.stat(source).st_ino != os.stat(dest).st_ino:
            try:
                os.link(source, dest+"-lnk")    ; replace(dest+"-lnk", dest)    ; ddcount += 1
            except OSError as err:
                if err.errno == 31:
                    # source has too many links; substitute
                    substitutions[src_orig] = dest    ; continue
                else:
                    if exists(dest+"-lnk"):   remove(dest+"-lnk")
                    print(err)   ; raise err
    print(ddcount, "reduced.")
elif cmd == "rename":
    catch_signals()    ; oldname = sys.argv[2]    ; newname = sys.argv[3]    ; replaced = False
    if exists(oldname) and exists(newname):   sys.exit(50)
    if exists(oldname):   replace(oldname, newname)   ; replaced = True
    replace("archive.ini.tmp", "archive.ini")    ; fssync(".")
    if not replaced:   sys.exit(40)
elif cmd == "get-configs":
    with open(tmpdir+"/tar.lst","w") as tarlstf:
        vols = [sys.argv[2]] if len(sys.argv)>2 else [x.name for x in os.scandir() if x.is_dir()]
        for volname in vols:
            if exists(volname+"/volinfo"):   print(volname+"/volinfo", file=tarlstf)
            for ses in os.scandir(volname):
                if ses.is_dir() and ses.name.startswith("S_") and not ses.name.endswith("-tmp"):
                    for f in ("info","manifest"):   print(volname+"/"+ses.name+"/"+f, file=tarlstf)
        if exists("in_process"):  print("in_process", file=tarlstf)
    p = subprocess.check_call(["tar","-czf","-","--no-recursion","--verbatim-files-from",
    "--files-from="+tmpdir+"/tar.lst","archive.ini"], stderr=subprocess.DEVNULL)
    '''
    with open(tmpdir +"/rpc/dest_helper.py", "wb") as progf:
        progf.write(bytes(dest_program, encoding="UTF=8"))

    #####>  End helper program  <#####

    return destsys, desttype


def detect_dest_state(destsys):
    global dest_run_map
    dest_run_map     = {"internal": [CP.sh],
                        "ssh":      [CP.ssh] + ssh_opts + [destsys],
                        "qubes":    [CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                     "-p", destsys],
                        "qubes-ssh":[CP.qvm_run, "--no-color-stderr", "--no-color-output",
                                     "-p", destsys.split(":")[0]]
                        }

    dest_in_proc = False    ; dest_free = None    ; online = False

    if (options.action not in local_actions and destsys is not None) \
        or options.remap or options.from_arch:

        if desttype == "qubes-ssh":
            # fix: possibly remove dargs and use dest_run()
            dargs = dest_run_map["qubes"][:-1] + [destsys.split(":")[0]]

            cmd = dargs + [shell_prefix \
                  +CP.rm+" -rf "+tmpdir+"-rpc  &&  mkdir -p "+tmpdir+"-rpc"
                  ]
            do_exec([cmd])

        # Fix: get OSTYPE env variable
        cmd = ["mountpoint -q '"+aset.destmountpoint
                +"' && mkdir -p '"+aset.destmountpoint+"/"+aset.destdir+topdir+"/"+aset.name
                +"' && cd '"+aset.destmountpoint+"/"+aset.destdir+topdir+"'"

                # send helper program to remote dest
                +"  && rm -rf "+tmpdir+"-rpc  &&  mkdir -p "+tmpdir+"-rpc"
                +"  && cat >"+tmpdir+"-rpc/dest_helper.py"

                # test write access and hardlinks
                +"  && touch archive.dat"
                +(" && ln -f archive.dat .hardlink" if options.dedup else "")

                # check free space and in_process status on remote
                +"  && echo -n 'wyng_check_free ' && stat -f -c '%a %S' ."
                +"  && { if [ -e "+aset.name+"/in_process ]; then"
                +"  echo 'wyng_in_process'; fi }"
                ]
        try:
            do_exec([dest_run_args(desttype, cmd),
                     # sanitize remote output:
                     ["cat","-v"],  ["tail","--bytes=8000"]
                    ],
                    out=tmpdir+"/dest-state.log", infile=tmpdir+"/rpc/dest_helper.py")
            online = True
        except subprocess.CalledProcessError:
            online = False

    if online:
        with open(tmpdir+"/dest-state.log","r") as logf:
            for ln in logf:
                if ln.startswith("wyng_in_process"):
                    dest_in_proc = True
                elif ln.startswith("wyng_check_free"):
                    parts = ln.split()    ; dest_free = int(parts[1]) * int(parts[2])

    return dest_free is not None, dest_free, dest_in_proc


# Set or clear state for the archive as 'in_process' in case of interruption during write.
# Format is list containing strings or list of strings. For latter, '/' is the delimiter.

def set_in_process(outer_list, tmp=False, dest=True):
    fssync(aset.path)

    if outer_list is None:
        if dest:  dest_run([destcd + bkdir + " && rm -f in_process"])
        for ext in ("", ".tmp", "_retry"):
            if exists(aset.path+"/in_process"+ext):  os.remove(aset.path+"/in_process"+ext)
        aset.in_process = None
        return

    with open(aset.path+"/in_process.tmp", "w") as ipf:
        for ln in outer_list:
            print(ln if type(ln) is str else "/".join(ln), file=ipf)

    if dest:
        dest_run([destcd + bkdir +" && cat >in_process.tmp"
                                +(" && mv in_process.tmp in_process") if not tmp else ""
                                 +" && { sync -f . || sync; }"],
                infile=aset.path+"/in_process.tmp")
    if not tmp:
        os.rename(aset.path+"/in_process.tmp", aset.path+"/in_process")

    aset.in_process = outer_list


def get_in_process(archpath=""):
    outer_list = []
    if not archpath: archpath = aset.path
    if exists(archpath+"/in_process"):
        with open(archpath+"/in_process", "r") as ipf:
            for ln in ipf:   outer_list.append(ln.strip())

            #self.in_process = [x.strip() for x in ipf]
    return outer_list if len(outer_list) else None


# Run system commands with pipes, without shell:
# 'commands' is a list of lists, each element a command line.
# If multiple command lines, then they are piped together.
# 'out' redirects the last command output to a file; append mode can be
# selected by beginning 'out' path with '>>'.
# List of commands may include 'None' instead of a child list; these will be ignored.

def do_exec(commands, cwd=None, check=True, out="", infile="", text=False):
    ftype   = "t" if text else "b"
    outmode = "a" if out.startswith(">>") else "w"
    out = out.lstrip(">>")
    if cwd and out and out[0] != "/":
        out = pjoin(cwd,out)
    if cwd and infile and infile[0] != "/":
        infile = pjoin(cwd,infile)
    outfunc = gzip.open if out.endswith(".gz") else open
    infunc  = gzip.open if infile.endswith(".gz") else open
    outf = outfunc(out, outmode+ftype) if out else subprocess.DEVNULL
    inf  = infunc(infile, "r"+ftype) if infile else subprocess.DEVNULL
    errf = open(tmpdir+"/err.log", "a")  ; print("--+--", file=errf)
    commands = [x for x in commands if x is not None]

    # Start each command, linking them via pipes
    procs = []
    for i, clist in enumerate(commands):
        p = subprocess.Popen(clist, cwd=cwd, stdin=inf if i==0 else procs[i-1].stdout,
                             stdout=outf if i==len(commands)-1 else subprocess.PIPE,
                             stderr=errf)
        if len(procs):  procs[-1].stdout.close()
        procs.append(p)

    # Monitor and control processes
    while True:
        err = None    ; finish = timeout = False
        for p1 in reversed(procs):
            retcode = p1.poll()
            if not finish and retcode is None:
                try:
                    p1.wait(timeout=2)
                except subprocess.TimeoutExpired:
                    timeout = True
                    continue
                retcode = p1.returncode   ; finish = True
                if check and (retcode != 0):
                    err = p1              ; finish = True
            elif finish and retcode is None:
                p1.terminate()
                continue

        if err or not timeout:
            break

    for f in [inf, outf, errf]:
        if type(f) is not int: f.close()
    if err and check:
        raise subprocess.CalledProcessError(err.returncode, err.args)

    return procs[-1].returncode


# Run system commands on destination

def dest_run(commands, dest_type=None, infile="", out="", check=True, trap=False):
    if dest_type is None:
        dest_type = desttype

    cmd = dest_run_args(dest_type, commands, trap=trap)
    return do_exec([cmd], infile=infile, out=out, check=check)


# Build command lists that can be shunted to remote systems.
# The input commands are stored in a temp file and a standard command that
# runs the temp file is returned.

def dest_run_args(dest_type, commands, trap=False):
    trapcmd = "trap '' INT TERM QUIT ABRT ALRM TSTP USR1\n" if trap else ""
    # shunt commands to tmp file
    with tempfile.NamedTemporaryFile(dir=tmpdir, delete=False) as tmpf:
        cmd = bytes(trapcmd + shell_prefix
                    + " ".join(commands) + "\n", encoding="UTF-8")
        tmpf.write(cmd)
        remotetmp = os.path.basename(tmpf.name)

    if dest_type in {"qubes","qubes-ssh"}:
        do_exec([[CP.qvm_run, "--no-color-stderr", "--no-color-output", "-p",
                  (destsys if dest_type == "qubes" else destsys.split(":")[0]),
                  CP.mkdir+" -p "+tmpdir+"-rpc"
                  +" && "+CP.cat+" >"+pjoin(tmpdir+"-rpc",remotetmp)
                ]], infile=pjoin(tmpdir,remotetmp))
        if dest_type == "qubes":
            add_cmd = [CP.sh+" "+pjoin(tmpdir+"-rpc",remotetmp)]
        elif dest_type == "qubes-ssh":
            add_cmd = [CP.ssh+" "+" ".join(ssh_opts)+" "+destsys.split(":")[1]
                       +' "$('+CP.cat+' '+pjoin(tmpdir+"-rpc",remotetmp)+')"']

    elif dest_type == "ssh":
        #add_cmd = [' "$(cat '+pjoin(tmpdir,remotetmp)+')"']
        add_cmd = [cmd]

    elif dest_type == "internal":
        add_cmd = [pjoin(tmpdir,remotetmp)]

    ret = dest_run_map[dest_type] + add_cmd
    #print("CMD",ret)
    return ret


# Prepare snapshots and check consistency with metadata.
# Must run get_lvm_vgs() again after this.

def prepare_snapshots(datavols):

    ''' Normal precondition will have a snap1vol already in existence in addition
    to the local datavol. Here we create a fresh snap2vol so we can compare
    it to the older snap1vol. Then, depending on monitor or backup mode, we'll
    accumulate delta info and possibly use snap2vol as source for a
    backup session.
    '''

    print("Preparing snapshots...")
    incr_vols, complete_vols = set(), set()    ; vgname = aset.vgname
    for datavol in datavols:
        # 'mapfile' is the deltamap file, 'snapXvol' are the .tick and .tock snapshots.
        # .tick holds vol state between send/monitor ops;
        # .tock is new snapshot which is compared w .tick and then replaces it (send/monitor).
        vol      = aset.vols[datavol]   ; mapfile  = vol.mapfile
        snap1vol = datavol + ".tick"    ; snap2vol = datavol + ".tock"

        if not lv_exists(vgname, datavol):
            print("Warning: Local '%s' does not exist!" % datavol)
            continue

        if lv_exists(vgname, snap1vol) and "arch-"+aset.uuid not in l_vols[snap1vol].tags \
        and "arch-" in " ".join(l_vols[snap1vol].tags):
            if options.remap:
                lv_remove(vgname, snap1vol)    ; print("  Removed mis-matched snapshot", snap1vol)
            else:
                print("  Skipping %s; LV snapshot is from a different archive." % datavol)
                continue

        # Reclaim deltamap from interrupted session.
        if exists(mapfile+"-tmp"):  os.rename(mapfile+"-tmp", mapfile)

        # Make deltamap or initial snapshot if necessary. Try to recover paired state
        # by comparing snapshot UUIDs; a match means remap is unnecessary.
        if len(vol.sessions):
            if lv_exists(vgname, snap1vol) and vol.last not in l_vols[snap1vol].tags \
            and l_vols[snap1vol].lv_uuid != vol.sessions[vol.last].uuid:
                lv_remove(vgname, snap1vol)    ; print("  Removed mis-matched snapshot", snap1vol)

            if not lv_exists(vgname, snap1vol) and lv_exists(vgname, snap2vol) \
            and vol.last in l_vols[snap2vol].tags and "arch-"+aset.uuid in l_vols[snap2vol].tags:
                # Recover interrupted snapshot rotation
                lv_rename(vgname, snap2vol, snap1vol)

            if lv_exists(vgname, snap1vol) and exists(mapfile) \
            and os.stat(mapfile).st_blocks == 0 and "delta" in l_vols[snap1vol].tags:
                # Handle inadvertant mapfile reset
                lv_remove(vgname, snap1vol)    ; print("  Removed mis-matched delta", snap1vol)

            if lv_exists(vgname, snap1vol) and not exists(mapfile) \
            and vol.last in l_vols[snap1vol].tags and "delta" not in l_vols[snap1vol].tags \
            and l_vols[snap1vol].lv_uuid == vol.sessions[vol.last].uuid:
                # Latest session matches current snapshot; OK to make blank map.
                init_deltamap(vol, mapfile, vol.mapsize())

        elif monitor_only:
            print("  Skipping %s; No data." % datavol)    ; continue

        # Handle circumstances where a new mapping is needed. New volume or vol has history
        # but .tick and/or deltamap are still missing after above checks.
        # In this case 'send' can determine any differences w prior backups.
        if not exists(mapfile) or not lv_exists(vgname, snap1vol):
            if not monitor_only:
                print("  Pairing snapshot for", datavol)
                complete_vols.add(datavol)
            else:
                if not monitor_only:   error_cache.append(datavol)
                print("  Skipping %s; No snapshot." % datavol)    ; continue

        # Make fresh snap2vol
        lv_remove(vgname, snap2vol)
        tags =["--addtag=delta"] if monitor_only else []
        do_exec([[CP.lvm, "lvcreate", "-pr", "-kn", "-ay", "--addtag=wyng"] + tags
                        + ["--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n",snap2vol]])

        # Volume is OK, add to list of vols.
        if datavol not in complete_vols and vol.sessions:
            incr_vols.add(datavol)
        else:
            complete_vols.add(datavol)

    return incr_vols, complete_vols


# Get raw lvm deltas between snapshots
# Runs the 'thin_delta' tool to output diffs between vol's old and new snapshots.
# Result can be read as an xml file by update_delta_digest().

def get_lvm_deltas(datavols):
    vgname   = aset.vgname.replace("-","--")
    poolset  = set(l_vols[x].pool_lv for x in datavols)
    print("Acquiring deltas.")

    # Reserve a metadata snapshot for the LVM thin pool; required for a live pool.
    catch_signals()
    for apool in sorted(poolset):
        poolname = apool.replace("-","--")
        lvm_meta_snapshot(aset, "reserve", pool=apool)

        err = False
        try:
            for datavol in datavols:
                if l_vols[datavol].pool_lv != apool:   continue
                snap1vol = datavol + ".tick"    ; snap2vol = datavol + ".tock"
                cmds = [[CP.thin_delta, "-m",   "--thin1=" + l_vols[snap1vol].thin_id,
                                                "--thin2=" + l_vols[snap2vol].thin_id,
                                        "/dev/mapper/"+vgname+"-"+poolname+"_tmeta"],
                        [CP.grep, "-v", "^\s*<same .*\/>$"]
                        ]
                do_exec(cmds,  out=tmpdir+"/delta."+datavol)
        except Exception as e:
            sys.stderr.write("ERROR running thin_delta process.\n")
            raise e
        finally:
            lvm_meta_snapshot(aset, "release", pool=apool)

    catch_signals(None)


# update_delta_digest: Translates raw lvm delta information
# into a bitmap (actually chunk map) that repeatedly accumulates change status
# for volume block ranges until a 'send' command is successfully completed and
# the mapfile is cleared.

def update_delta_digest(datavol):

    vol         = aset.vols[datavol]         ; chunksize  = aset.chunksize
    snap1vol    = vol.name + ".tick"         ; snap2vol   = vol.name + ".tock"

    if len(vol.sessions) == 0 or not lv_exists(aset.vgname, snap1vol):   return 0, 0
    snap1size   = l_vols[snap1vol].lv_size    ; snap2size = l_vols[snap2vol].lv_size

    # Put deltamap into a temporary state
    os.rename(vol.mapfile, vol.mapfile+"-tmp")

    # Get xml parser and initialize vars
    dtree       = xml.etree.ElementTree.parse(tmpdir+"/delta."+datavol).getroot()
    dblocksize  = int(dtree.get("data_block_size"))
    dnewchunks   = isnew  = dfreedblocks = 0

    # Check for volume size increase;
    # Chunks from 'markall_pos' onward will be marked for backup.
    next_chunk_addr  = vol.last_chunk_addr() + chunksize
    markall_pos = (next_chunk_addr//chunksize//8) if snap2size-1 >= next_chunk_addr else None

    # Setup access to deltamap as an mmap object.
    with open(vol.mapfile+"-tmp", "r+b") as bmapf:
        snap_ceiling = max(snap1size, snap2size) // bs    ; chunkblocks = chunksize // bs
        bmap_size    = vol.mapsize(max(snap1size, snap2size))
        bmapf.truncate(bmap_size)    ; bmapf.flush()
        bmap_mm      = mmap.mmap(bmapf.fileno(), 0)

        # Cycle through the 'thin_delta' metadata, marking bits in bmap_mm as needed.
        # Entries carry a block position 'blockbegin' and the length of changed blocks.
        # 'snap_ceiling' is used to discard ranges beyond current vol size.
        for delta in dtree.find("diff"):
            blockbegin = int(delta.get("begin")) * dblocksize
            if blockbegin >= snap_ceiling:  continue
            blocklen   = int(delta.get("length")) * dblocksize
            blockend   = min(blockbegin+blocklen, snap_ceiling)
            if delta.tag in ("different", "right_only"):
                isnew = 1
            elif delta.tag == "left_only":
                isnew = 0    ; dfreedblocks += blockend - blockbegin
            else: # superfluous tag
                continue

            # 'blockpos' iterates over disk blocks, with thin LVM constant of 512 bytes/block.
            # dblocksize (local) & chunksize (dest) may be somewhat independant of each other.
            for blockpos in range(blockbegin, blockend):
                volsegment = blockpos // chunkblocks
                bmap_pos = volsegment // 8    ; b = 1 << (volsegment%8)
                if not bmap_mm[bmap_pos] & b:
                    bmap_mm[bmap_pos] |= b    ; dnewchunks += isnew

        if markall_pos is not None:
            # If volsize increased, flag the corresponding bmap area as changed.
            if monitor_only:  print("  Volume size has increased.")
            for pos in range(markall_pos, bmap_size):  bmap_mm[pos] = 0xff
            dnewchunks += (bmap_size - markall_pos) * 8

        map_updated = dnewchunks+dfreedblocks > 0
        if map_updated:   bmapf.flush()    ; os.fsync(bmapf.fileno())

    catch_signals()
    if monitor_only:
        rotate_snapshots(vol, rotate=map_updated)
        os.rename(vol.mapfile+"-tmp", vol.mapfile)
        print(("\r  %d ch, %d dis" % (dnewchunks, dfreedblocks//chunksize))
                if map_updated else "\r  No changes   ")

    if dnewchunks:   vol.changed_bytes_add(dnewchunks*aset.chunksize, save=monitor_only)
    catch_signals(None)
    return dnewchunks, dfreedblocks #fix


# Reads addresses from manifest and marks corresponding chunks in a volume's deltamap.

def manifest_to_deltamap(datavol, manifest, mapsize):
    with open(manifest, "r") as mf, \
         open(aset.vols[datavol].mapfile, "r+b") as bmapf:

        bmapf.truncate(mapsize)    ; bmapf.flush()
        bmap_mm = mmap.mmap(bmapf.fileno(), 0)       ; chunksize  = aset.chunksize
        for ln in mf:
            addr = int(ln.split()[1][1:], 16)        ; volsegment = addr // chunksize
            bmap_pos = volsegment // 8               ; bmap_mm[bmap_pos] |= 1 << (volsegment % 8)


# Send volume to destination.
#
# send_volume() has two main modes which are full (send_all) and incremental. After send
# finishes a full session, the volume will have a blank deltamap and .tick snapshot to
# track changes. After an incremental send, snapshots are rotated and the deltamap is reset.
#
# The 'dedup' deduplication level also represents a range of modes, where '0' means that all
# marked chunks will be sent unconditionally, '1' means a chunk won't be sent if its marked
# but same content, and '>= 2' will check w the archive-wide index and link it if matched.

def send_volume(datavol, localtime, ses_tags, send_all):

    vol         = aset.vols[datavol]            ; dedup       = options.dedup
    snap2vol    = vol.name + ".tock"            ; snap2size   = l_vols[snap2vol].lv_size
    bmap_size   = vol.mapsize(snap2size)        ; chunksize   = aset.chunksize
    chdigits    = max_address.bit_length()//4   ; chformat    = "%0"+str(chdigits)+"x"
    bksession   = "S_"+localtime                ; sdir        = pjoin(datavol, bksession)
    prior_size  = vol.volsize()                 ; prior_ses   = vol.last

    if len(vol.sessions):
        # Our chunks are usually smaller than LVM's, so generate a full manifest to detect
        # significant amount of unchanged chunks that are flagged in the delta bmap.
        fullmanifest = open(merge_manifests(datavol), "r")
        fullmanifest_readline = fullmanifest.readline
    else:
        fullmanifest = None
    fman_hash     = fman_fname = ""

    ses = vol.new_session(bksession, addtags=ses_tags)
    ses.localtime = localtime
    ses.uuid      = l_vols[snap2vol].lv_uuid
    ses.trans_id  = l_vols[datavol].transaction_id
    ses.volsize   = snap2size
    ses.format    = "folders"
    ses.path      = vol.path+"/"+bksession+"-tmp"

    # Code from init_dedup_indexN() localized here for efficiency.
    dedup_idx     = dedup_db = None
    if dedup:
        hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w \
                  = aset.dedupindex
        chtree_max= 2**(chtree[0].itemsize*8)
        idxcount  = dataf.seek(0,2) // (ch_w+ses_w)    ; ddblank_ch = bytes(hash_w)
        ddsessions = aset.dedupsessions                ; ses_index = ddsessions.index(ses)

    # Set current dir and make new session folder
    os.chdir(metadir+bkdir)
    os.makedirs(sdir+"-tmp")

    zeros     = bytes(chunksize)
    bcount    = ddbytes = 0
    addrsplit = -address_split[1]
    lchunk_addr = vol.last_chunk_addr(snap2size)

    compresslevel = int(aset.compr_level)    ; compress = compressors[aset.compression][2]


    if aset.key:
        crypto_key = bytes.fromhex(aset.key)
        crypto     = DataCryptography(aset.cipher, key=crypto_key)
        encrypt    = crypto.encrypt    # ; crypto_blk_sz = crypto.blk_sz
    else:
        crypto_key = None


    # Use tar to stream files to destination
    stream_started = False
    untar_cmd = [destcd + " && mkdir -p ."+bkdir
                 + " && exec >>"+tmpdir+"-rpc/send.log 2>&1"
                 + " && "+destcd+bkdir + " && echo "+datavol
                 + " && python3 "+tmpdir+"-rpc/dest_helper.py send" ]

    # Open source volume and its delta bitmap as r, session manifest as w.
    with open(pjoin("/dev",aset.vgname,snap2vol),"rb", buffering=chunksize) as vf, \
         open(sdir+"-tmp/manifest", "w") as hashf,             \
         open("/dev/zero" if send_all else vol.mapfile+"-tmp","rb") as bmapf:

        vf_seek = vf.seek; vf_read = vf.read
        gethash = hash_funcs[aset.hashtype]   ; BytesIO = io.BytesIO

        # Show progress in increments determined by 1000/checkpt_pct
        # where '200' results in five updates i.e. in unattended mode.
        checkpt = checkpt_pct = 335 if options.unattended else 1
        addr    = percent = 0

        # Feed delta bmap to inner loop in pieces segmented by large zero delimeter.
        # This allows skipping most areas when changes are few.
        zdelim  = bytes(64)    ; minibmap = None    ; bmap_list = []

        while addr < snap2size:
            if len(bmap_list):
                # At boundary inside list, so use islice to jump ahead here.
                if fullmanifest:  list(islice(fullmanifest, len(zdelim)*8))
                addr += chunksize*len(zdelim)*8
                minibmap = bmap_list.pop(0)
            elif not send_all:
                # Get more: split(zdelim) shows where large unmodified zones exist.
                bmap_list.extend(bmapf.read(25000).split(zdelim))
                minibmap = bmap_list.pop(0)

            # Cycle over range of chunk addresses.
            for chunk, addr in enumerate(range( addr, snap2size if send_all
                        else min(snap2size, addr+len(minibmap)*8*chunksize), chunksize)):

                destfile = "x"+chformat % addr

                if fullmanifest:
                    try:
                        fman_hash, fman_fname = fullmanifest_readline().split()
                    except ValueError: # EOF
                        fullmanifest = None    ; fman_hash = ""
                    else:
                        if fman_fname != destfile:
                            raise ValueError("expected manifest addr %s, got %s"
                                            % (destfile, fman_fname))

                # Skip chunk if its deltamap bit is off.
                if not send_all and not (minibmap[chunk//8] & (1 << chunk%8)):  continue

                # Fetch chunk as buf
                vf_seek(addr)    ; buf = vf_read(chunksize)

                # Show progress.
                percent = int(addr/snap2size*1000)
                if percent >= checkpt:
                    print("\x0d  %4.1f%% %7dM" % (percent/10, bcount//1000000),
                            end="", flush=True)
                    checkpt += checkpt_pct

                # Compress & write only non-empty chunks
                if buf == zeros:
                    if fman_hash != "0":  print("0", destfile, file=hashf)
                    continue

                # Compress chunk and hash it
                buf    = compress(buf, compresslevel)
                bhash  = gethash(buf)   ; hexhash = bhash.hexdigest()

                # Skip when current and prior chunks are the same
                if fman_hash == hexhash:  continue

                # Start tar stream
                if not stream_started:
                    untar = subprocess.Popen(dest_run_args(desttype, untar_cmd),
                            stdin =subprocess.PIPE,    stdout=subprocess.DEVNULL,
                            stderr=subprocess.DEVNULL)
                    tarf = tarfile.open(mode="w|", fileobj=untar.stdin)
                    tarf_addfile = tarf.addfile        ; TarInfo = tarfile.TarInfo
                    stream_started = True              ; LNKTYPE = tarfile.LNKTYPE
                    tar_info = TarInfo(sdir+"-tmp")    ; tar_info.type = tarfile.DIRTYPE
                    tarf_addfile(tarinfo=tar_info)

                # Add buffer to stream
                tar_info = TarInfo("%s-tmp/%s/%s" % (sdir, destfile[1:addrsplit], destfile))
                print(hexhash, destfile, file=hashf)

                # If chunk already in archive, link to it
                if dedup:
                    bhashb = bhash.digest()
                    i      = int(hexhash[:ht_ksize], 16)
                    pos    = hashtree[i].find(bhashb)     ; ddses = None
                    if pos % hash_w == 0:
                        data_i = chtree[i][pos//hash_w]   ; dataf.seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[int.from_bytes(dataf.read(ses_w),"big")]
                        if ddses is None:
                            hashtree[i][pos:pos+hash_w] = ddblank_ch # zero-out obsolete entry
                        else:
                            ddchx = dataf.read(ch_w).hex().zfill(chdigits)
                            tar_info.type = LNKTYPE
                        dataf.seek(0,2)
                    if ddses is None and bhashb != ddblank_ch and idxcount < chtree_max:
                        hashtree[i].extend(bhashb)   ; chtree[i].append(idxcount)   ; idxcount += 1
                        dataf.write(ses_index.to_bytes(ses_w,"big") + addr.to_bytes(ch_w,"big"))

                if tar_info.type == LNKTYPE:
                    tar_info.linkname = "%s/%s/%s/x%s" % \
                        (ddses.volume.name,
                            ddses.name+"-tmp" if ddses is ses else ddses.name,
                            ddchx[:addrsplit],
                            ddchx)
                    ddbytes += len(buf)
                    tarf_addfile(tarinfo=tar_info)

                else:
                    # Encrypt the data chunk
                    if crypto_key:
                        buf = encrypt(buf)

                    # Send data chunk to the archive
                    tar_info.size = len(buf)
                    tarf_addfile(tarinfo=tar_info, fileobj=BytesIO(buf))
                    bcount += len(buf)

            # Advance addr, except when minibmap is zero len.
            if minibmap or send_all:  addr += chunksize

    print("\r  100% ", ("%8.1fM  |  %s" % (bcount/1000000, datavol)),
          ("\n  (reduced %0.1fM)" % (ddbytes/1000000)) if ddbytes and options.debug else "",
          end="")

    # Send session info, end stream and cleanup
    if fullmanifest:   fullmanifest.close()
    if stream_started:
        # Save session info
        ses.save_info()
        for session in vol.sessions.values() if vol.que_meta_update == "true" else [ses]:
            tarf.add(pjoin(vol.name, os.path.basename(session.path)))
        vol.que_meta_update = "false"    #; vol.changed_bytes = 0
        vol.save_volinfo("volinfo-tmp")
        tarf.add(datavol+"/volinfo-tmp")
        tarf.add(os.path.basename(aset.confpath))

        #print("Ending tar process ", end="")
        tarf.close()    ; untar.stdin.close()
        try:
            untar.wait(timeout=60)
        except subprocess.TimeoutExpired:
            print("Warning: tar process timeout.")
            retcode = 99
            untar.kill()
        else:
            retcode = untar.poll()

        if retcode != 0:
            raise RuntimeError("tar transport failure code %d" % retcode)

        if ses.volsize != prior_size:   check_manifest_sequence(datavol, vol.sesnames)

        # Finalize on VM/remote
        catch_signals()
        dest_run([ destcd + bkdir
                 +" && mv -fT "+sdir+"-tmp "+sdir
                 +" && mv -f "+datavol+"/volinfo-tmp "+datavol+"/volinfo"
                 +" && { sync -f . || sync; }"], trap=True)

        # Local finalize
        ses.path = ses.path.rsplit("-tmp",maxsplit=1)[0]   ; os.replace(ses.path+"-tmp", ses.path)
        os.replace(vol.path+"/volinfo-tmp", vol.path+"/volinfo")
        fssync(vol.path)

    else:
        catch_signals()
        vol.delete_session(bksession)    ; shutil.rmtree(aset.path+"/"+sdir+"-tmp")

    rotate_snapshots(vol, rotate=True)
    init_deltamap(vol, vol.mapfile, vol.mapsize())
    catch_signals(None)
    if dedup and options.debug:   show_mem_stats()

    return stream_started + (bcount//bs)


# Build deduplication hash index and list

def init_dedup_index5(listfile=""):

    ctime = time.perf_counter()    ; makelist = bool(listfile)
    # Define arrays and element widths
    hash_w     = hash_bits // 8
    ht_ksize   = 4 # hex digits for tree key
    hashtree   = [bytearray() for x in range(2**(ht_ksize*4))]
    chtree     = [array("I") for x in range(2**(ht_ksize*4))]
    chtree_max = 2**(chtree[0].itemsize*8) # "I" has 32bit range
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    ses_w = 2; ch_w = chdigits //2
    # limit number of sessions to ses_w range:
    ddsessions = aset.dedupsessions[:2**(ses_w*8)-(len(aset.vols))-1]
    addrsplit  = -address_split[1]

    dataf  = open(tmpdir+"/hashindex.dat","w+b")
    dataf_read  = dataf.read    ; dataf_seek = dataf.seek    ; int_frbytes = int.from_bytes
    dataf_write = dataf.write   ; bfromhex = bytes().fromhex
    if makelist:   dedupf = gzip.open(tmpdir+"/"+listfile, "wt")

    count = match = 0
    ##count = 1; dataf.write(bytes(ses_w + ch_w)) # first is null
    for sesnum, ses in enumerate(ddsessions):
        volname = ses.volume.name; sesname = ses.name
        with open(pjoin(ses.path,"manifest"),"r") as manf:
            for ln in manf:
                ln1, ln2 = ln.split()
                if ln1 == "0":   continue
                bhashb = bfromhex(ln1)
                i      = int(ln1[:ht_ksize], 16)
                pos    = hashtree[i].find(bhashb)
                if pos % hash_w == 0:
                    match += 1
                    if makelist:
                        data_i = chtree[i][pos//hash_w]
                        dataf_seek(data_i*(ses_w+ch_w))
                        ddses  = ddsessions[int_frbytes(
                                 dataf_read(ses_w),"big")]
                        ddchx  = dataf_read(ch_w).hex().zfill(chdigits)
                        print("%s/%s/%s/x%s %s/%s/%s/%s" % \
                            (ddses.volume.name, ddses.name, ddchx[:addrsplit], ddchx,
                            volname, sesname, ln2[1:addrsplit], ln2),
                            file=dedupf)
                        dataf_seek(0,2)
                elif count < chtree_max:
                    hashtree[i].extend(bhashb)
                    chtree[i].append(count)
                    dataf_write(sesnum.to_bytes(ses_w,"big"))
                    dataf_write(bfromhex(ln2[1:]))
                    count += 1

    if listfile:
        dedupf.close()
        #dataf.close()

    aset.dedupindex    = (hashtree, ht_ksize, hash_w, dataf, chtree, chdigits, ch_w, ses_w)
    aset.dedupsessions = ddsessions

    if not options.debug:  return
    print("\nIndexed in %.1f seconds." % (time.perf_counter()-ctime))
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\nMemory use: Max %dMB, index count: %d, matches: %d" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024,
         count, match)
        )
    print("Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))
    #print("idx size: %d" % sys.getsizeof(idx))


# Deduplicate data already in archive

def dedup_existing():

    print("Building deduplication index...")
    init_dedup_index("dedup.lst.gz")

    print("Linking...")
    do_exec([dest_run_args(desttype, [destcd + bkdir
               +" && /bin/cat >"+tmpdir+"-rpc/dest.lst.gz"
               +" && /usr/bin/python3 "+tmpdir+"-rpc/dest_helper.py dedup"
               ])
            ], infile=tmpdir+"/dedup.lst.gz", out=tmpdir+"/arch-dedup.log")


# Controls flow of monitor and send_volume procedures:

def monitor_send(datavols, selected=[], monitor_only=True):
    global dest_free

    for prg in (CP.lvm, CP.dmsetup, CP.thin_delta ):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    localtime = time.strftime("%Y%m%d-%H%M%S")

    incrementals, send_alls \
        = prepare_snapshots(selected if len(selected) >0 else datavols)

    get_lvm_vgs(aset.vgname)
    if aset.vgname not in volgroups.keys():
        raise ValueError("Volume group "+aset.vgname+" not present.")

    if monitor_only:   send_alls.clear()

    if len(incrementals)+len(send_alls) == 0:
        x_it(0, "No new data.")

    # Process session tags
    ses_tags = []
    if options.tag and options.tag == [""] and not options.unattended and not monitor_only:
        print("Enter tag info as 'tagID[, tag description]'. Blank to end input.")
        while True:
            ans = ask_input("[%d]: " % (len(ses_tags)+1)).strip()
            if not ans:   break
            tag = aset.Volume.Ses.tag_parse(None, ans, delim=",")
            if not tag:   continue
            ses_tags.append(tag)
            if len(ses_tags) > 4:   break
        print(len(ses_tags), "tags total.")
    elif options.tag and not monitor_only:
        for tag_opt in options.tag:
            tag = aset.Volume.Ses.tag_parse(None, tag_opt, delim=",")
            if not tag:   raise ValueError("Invalid tag "+tag_opt)
            ses_tags.append(tag)

    if len(incrementals) > 0:
        get_lvm_deltas(incrementals)

    if options.dedup:
        init_dedup_index()

    if not monitor_only:
        cmpvols = [x for x in aset.vols.values()
                         if x.name in incrementals | send_alls and x.sessions]
        cmpses  = [v.sessions[v.sesnames[-1]] for v in cmpvols]
        if compare_files(volumes=cmpvols, sessions=cmpses):
            x_it(1, "Error: Local and archive metadata differ.")

        print("\nSending backup session", localtime,
              "to", (desttype+"://"+destsys) if desttype != "internal" else destpath)

    for datavol in sorted(incrementals | send_alls):
        vol = aset.vols[datavol]    ; dnew = dfreed = 0
        print(" ", "Scan" if monitor_only else "Send", "volume      | ", datavol, flush=True,end="")

        dnew, dfreed = update_delta_digest(datavol)
        if monitor_only:   continue

        if datavol in send_alls or os.stat(vol.mapfile+"-tmp").st_blocks > 0:
            if vol.changed_bytes > dest_free:
                # Enh: add loop here for all volumes, implement a 'forced' mode
                autoprune(datavol, needed_space=vol.changed_bytes, apmode=options.autoprune)
                if vol.changed_bytes > dest_free:
                    print(" %d additional bytes needed." % (vol.changed_bytes-dest_free))
                    print("Insufficient space on destination %d; Skipping." % dest_free)
                    error_cache.append(datavol)
                    continue

            dnew       = send_volume(datavol, localtime, ses_tags, send_all=datavol in send_alls)
            dest_free -= int(dnew + (dnew * 0.05))

        else:
            rotate_snapshots(vol, rotate=False)
            os.rename(vol.mapfile+"-tmp", vol.mapfile)

        print("\r" if dnew+dfreed else "\r  No changes   ", flush=True)


def init_deltamap(vol, bmfile, bmsize):
    vol.changed_bytes_add(0, reset=True)
    if exists(bmfile):
        os.remove(bmfile)
    if exists(bmfile+"-tmp"):
        os.remove(bmfile+"-tmp")
    with open(bmfile, "wb") as bmapf:
        bmapf.truncate(bmsize)    ; bmapf.flush()


def rotate_snapshots(vol, rotate=True):
    snap1vol = vol.name+".tick"   ; snap2vol = vol.name+".tock"
    if rotate:
        do_exec([[CP.lvm,"lvchange","--addtag="+vol.last, aset.vgname+"/"+snap2vol]])
        lv_remove(aset.vgname, snap1vol)    ; lv_rename(aset.vgname, snap2vol, snap1vol)
    else:
        lv_remove(aset.vgname, snap2vol, sync=False)



# Prune backup sessions from an archive. Basis is a non-overwriting dir tree
# merge starting with newest dirs and working backwards. Target of merge is
# timewise the next session dir after the pruned dirs.
# Specify data volume and one or two member list with start [end] date-time
# in YYYYMMDD-HHMMSS or ^tagname format.

def prune_sessions(datavol, times):

    volume = aset.vols[datavol]    ; sessions = volume.sesnames
    t1, t2 = "", ""                ; to_prune  = []

    if len(sessions) < 2:    print("  No extra sessions to prune.")    ; return

    # Validate date-time params
    for pos, dt in enumerate(times[:]):
        if not dt[0].startswith("^"):
            if not dt.startswith("S_"):   times[pos] = "S_"+dt.strip()
            datetime.datetime.strptime(times[pos][2:], "%Y%m%d-%H%M%S")
        elif dt[1:] not in volume.tags:
            print(" No match for", dt)    ; return
        elif pos == 0:
            for sesname in sessions:
                if dt[1:] in volume.sessions[sesname].tags:
                    if len(times) == 1 and not options.allbefore:
                        to_prune.append(sesname)
                    else:
                        t1 = sesname   ; break
        elif pos == 1:
            for sesname in reversed(sessions):
                if dt[1:] in volume.sessions[sesname].tags:   t2 = sesname   ; break

    # t1 alone should be a specific session date-time,
    # t1 and t2 together are a date-time range.
    if options.allbefore:   t1 = sessions[0]    ; t2 = times[0]
    if not t1:   t1 = times[0]
    if not t2 and len(times) > 1:
        t2 = times[1]
        if t2 <= t1:  x_it(1, "Error: Second date-time must be later than first.")

    # Find specific sessions to prune in contiguous range
    if to_prune:
        pass

    elif t2 == "":
        # find single session
        if t1 in sessions:   to_prune.append(t1)

    else:
        # find sessions in a date-time range
        start = len(sessions)   ; end = 0
        if t1 in sessions:
            start = sessions.index(t1)
        else:
            for ses in sessions:
                if ses > t1:   start = sessions.index(ses)    ; break
        if t2 in sessions:
            end = sessions.index(t2)+1
        else:
            for ses in reversed(sessions):
                if ses < t2:   end = sessions.index(ses)+1    ; break
        to_prune = sessions[start:end]

    if len(to_prune) and to_prune[-1] == sessions[-1]:
        print("  Preserving latest session.")
        del(to_prune[-1])
    if len(to_prune) == 0:
        print("  No selections in this date-time range.")
        return

    autoprune(datavol, apmode=options.autoprune, include=set(to_prune))


# Parameters / vars for autoprune:
# oldest (date): date before which all sessions are pruning candidates
# thin_days (int): number of days ago before which the thinning params are applied
# ndays & nsessions: a days/sessions ratio for amount of sessions left after thinning
# nthresh: min number of sessions to prune this time (0 = prune all candidates)
# target_size (0 or MB int): User-selected size cap for archive (future)

def autoprune(datavol, needed_space=0, apmode="off", include=set()):
    global dest_free

    def get_dest_free(): ## Enh: add sanitize
        for ln in open(tmpdir+"/merge.log","r").readlines():
            if ln.startswith("wyng_check_free"):  dest_free = int(ln.split()[1])
        return dest_free

    dtdate = datetime.date
    def to_date(sesdate):
        return dtdate(int(sesdate[2:6]),int(sesdate[6:8]),int(sesdate[8:10]))

    vol       = aset.vols[datavol]
    if len(vol.sesnames) < 2:   return False
    apmode    = apmode.lower()                ; exclude = set()
    sessions  = vol.sesnames[:-1]             ; marked  = 0
    today     = dtdate.today()                ; oldest  = today-datetime.timedelta(days=366)
    thin_days = datetime.timedelta(days=32)   ; ndays   = 7      ; nsessions = 2
    startdate = to_date(sessions[0])          ; nthresh = 3 if apmode == "on" else 0
    enddate   = min(to_date(sessions[-2]), today-thin_days) if len(sessions) > 2 else None

    # Make a 2d array of ordinal dates and populate with session id + flag
    apcal = { day: [] for day in range(dtdate(startdate.year,1,1).toordinal(),
                                       to_date(sessions[-1]).toordinal()+ndays) }
    for ses in sessions:  apcal[to_date(ses).toordinal()].append([ses, True])

    # Build set of excluded sessions
    for sx in options.keep:
        if not sx.startswith("^"):
            datetime.datetime.strptime(sx, "%Y%m%d-%H%M%S")    ; exclude.add("S_"+sx)
        else:
            if sx[1:] == "all":
                exclude += {x.name for x in vol.sessions if x.tags}
            else:
                exclude += {x.name for x in vol.sessions if sx[1:] in x.tags}
    include -= exclude

    # Mark all sessions prior to oldest date setting, plus include list
    for ses in sessions:
        sdate = to_date(ses)
        if (apmode != "off" and sdate <= oldest) or ses in include:
            for dses in apcal[sdate.toordinal()]:
                if dses[0] == ses and ses not in exclude:
                    dses[1] = False    ; vol.sessions[ses].toggle = False

    # Mark sessions for thinning-out according to ndays + nsessions
    if apmode != "off" and (needed_space or apmode != "min") and enddate:
        for year in range(startdate.year, enddate.year+1):
            for span in range(dtdate(year,1,1).toordinal(),
                            min(dtdate(year,12,31), enddate).toordinal(), ndays):
                dlist = []    ; offset = 0
                for day in range(span, min(span+ndays, enddate.toordinal())):
                    dlist.append(sum( x[1] for x in apcal[day] ))
                while sum(dlist) > nsessions: ## Enh: Make even distribution
                    bigday = dlist.index(max(dlist[offset:]), offset)
                    offset += (ndays//nsessions)+1    ; offset %= min(ndays, len(dlist))
                    for dses in apcal[span+bigday]:
                        if dses[1]:
                            # always decr bigday, but don't toggle if session is excluded
                            dlist[bigday] -= 1     #### ; print("_",end="") ####
                            dses[1] = vol.sessions[dses[0]].toggle = dses[0] in exclude
                            break

    # Find contiguous marked ranges and merge/prune them. Repeat until free >= needed space.
    factor = 1    ; sessions.append("End")
    while True:
        to_prune = []    ; removed_ct = 0    ; skipped = False
        for ses in sessions:
            if ses is None:   continue
            if ses == "End" or vol.sessions[ses].toggle :
                if to_prune:
                    # prioritize ranges that overlap with requested includes
                    if include and not (set(to_prune) & include):
                        #print("#", end="") ####
                        to_prune.clear()    ; skipped = True    ; continue

                    target_s = vol.sesnames[vol.sesnames.index(to_prune[-1]) + 1]
                    merge_sessions(datavol, to_prune, target_s, clear_sources=True)

                    for i in to_prune:   sessions[sessions.index(i)] = None
                    include -= set(to_prune)    ; removed_ct += len(to_prune);   to_prune.clear()
                    if not include and nthresh and removed_ct >= nthresh*factor:  break # for ses
            else:
                #print(".", end="") ####
                to_prune.append(ses)

        if removed_ct:
            print(" Removed", removed_ct)
            dest_free = get_dest_free()    ; os.remove(tmpdir+"/merge.log")

        if skipped and apmode != "off":   continue # while
        if removed_ct == 0 or nthresh == 0 or needed_space <= dest_free:
            break # while
        elif factor > 4:
            nthresh = 0
        else:
            factor += 2

    return True


# Accepts a list of session names in ascending order (or else uses all sessions in the volume)
# and merges the manifests. Setting 'addcol' will add a colunm showing the session dir name.

def merge_manifests(datavol, msessions=None, mtarget=None, addcol=False):
    # Enh: implement mtarget to support merge_sessions()
    volume    = aset.vols[datavol]
    msessions = volume.sesnames if not msessions else msessions
    sespaths  = [ os.path.basename(volume.sessions[x].path) for x in msessions ]
    tmp = big_tmpdir if volume.volsize() > 128000000000 else tmpdir
    outfile   = tmp+"/manifest.mrg"     ; slist  = []
    for suffix in ("/manifest\x00", "\x00"):
        with tempfile.NamedTemporaryFile(dir=tmp, delete=False) as tmpf:
            tmpf.write(bytes(suffix.join(reversed(sespaths)), encoding="UTF-8"))
            tmpf.write(bytes(suffix, encoding="UTF-8"))
            slist.append(tmpf.name)

    if addcol:
        # add a column containing the source session
        cdir  = tmp+"/m"     ; slsort  = slist[1]
        shutil.rmtree(cdir, ignore_errors=True);   os.makedirs(cdir)

        # fix: extrapolate path with filename
        do_exec([[CP.xargs, "-0", "-a", slist[0],
                  CP.awk, '{sub("/manifest","",FILENAME); print $0, FILENAME > "'
                            +tmp+'/m/"FILENAME}']], cwd=volume.path)
    else:
        cdir  = volume.path     ; slsort  = slist[0]

    do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=64", "--files0-from="+slsort]],
            out=outfile, cwd=cdir)
    if addcol:  shutil.rmtree(tmp+"/m")

    return outfile


# Merge sessions together. Starting from first session results in a target
# that contains an updated, complete volume. Other starting points can
# form the basis for a pruning operation.
# Specify the data volume (datavol), source sessions (sources), and
# target. Caution: clear_sources is destructive.

def merge_sessions(datavol, sources, target, clear_sources=False):

    volume     = aset.vols[datavol]    ; resume = aset.in_process is not None
    chdigits   = max_address.bit_length() // 4 # 4bits per digit
    chformat   = "x%0"+str(chdigits)+"x"
    m_tmp      = tmpdir if volume.volsize() < 128000000000 else big_tmpdir

    # Prepare manifests for efficient merge using fs mv/replace. The target is
    # included as a source, and oldest source is our target for mv. At the end
    # the merge_target will be renamed to the specified target. This avoids
    # processing the full range of volume chunks in the likely case that
    # the oldest (full) session is being pruned.
    merge_target  = sources[0]    ; merge_sources = ([target] + list(reversed(sources)))[:-1]
    os.chdir(volume.path)

    if not resume:
        for ses in sources + [target]:
            if volume.sessions[ses].format == "tar":
                x_it(1, "Cannot merge range containing tarfile session.")
        volsize    = volume.sessions[target].volsize
        vol_shrank = volsize < max(x.volsize for x in volume.sessions.values()
                                    if x.name in sources)
        last_chunk = chformat % volume.last_chunk_addr(volsize)
        lc_filter  = '"'+last_chunk+'"'

        with open("merge.lst", "wt") as lstf:
            print(merge_target, target, file=lstf)

            # Get manifests, append session name to eol, print session names to list.
            #print("  Reading manifests")
            manifests = []
            for ses in merge_sources:
                if clear_sources:   print(ses, file=lstf)    ; manifests.append("man."+ses)
                do_exec([[CP.sed, "-E", "s|$| "+ses+"|", ses+"/manifest"
                        ]], out=m_tmp+"/man."+ses)
            print("###", file=lstf)

        # Unique-merge filenames: one for rename, one for new full manifest.
        do_exec([[CP.sort, "-umd", "-k2,2", "--batch-size=64"] + manifests],
                out="manifest.one", cwd=m_tmp)
        do_exec([[CP.sort, "-umd", "-k2,2", "manifest.one",
                pjoin(volume.path, merge_target, "manifest")]], out="manifest.two", cwd=m_tmp)
        # Make final manifest without extra column.
        do_exec([[CP.awk, "$2<="+lc_filter+" {print $1, $2}", m_tmp+"/manifest.two"]],
                out=target+"/manifest.tmp")

        # Output manifest filenames in the sftp-friendly form:
        # 'rename src_session/subdir/xaddress target/subdir/xaddress'
        # then pipe to destination and run dest_helper.py.
        do_exec([
                [CP.awk, "$2<="+lc_filter] if vol_shrank else None,
                [CP.sed, "-E",

                "s|^0 x(\S{" + str(address_split[0]) + "})(\S+)\s+(S_\S+)|"
                "-rm "+merge_target+"/\\1/x\\1\\2|; t; "

                "s|^\S+\s+x(\S{" + str(address_split[0]) + "})(\S+)\s+(S_\S+)|"
                "rename \\3/\\1/x\\1\\2 "+merge_target+"/\\1/x\\1\\2|"
                ]
                ], infile=m_tmp+"/manifest.one", out=">>merge.lst")

        if vol_shrank:
            # If volume size shrank in this period then make trim list.
            do_exec([[CP.awk, "$2>"+lc_filter, m_tmp+"/manifest.two"],
                     [CP.sed, "-E", "s|^\S+\s+x(\S{" + str(address_split[0]) + "})(\S+)|"
                                    "-rm "+merge_target+"/\\1/x\\1\\2|"]
                    ], out=">>merge.lst")

        do_exec([[CP.gzip, "-f", "merge.lst"]])

    # Update info records
    if clear_sources:
        for ses in sources:
            if ses in volume.sessions:   volume.delete_session(ses, remove=False)
        #print("Removing %d" % len(sources), end="...", flush=True)

    if not resume:
        volume.sessions[target].save_info("info.tmp")
        volume.save_volinfo("volinfo.tmp", prune=True)

        # Set archive in_process state to "merge"
        set_in_process(["merge", datavol, str(clear_sources), target, sources],
                       tmp=True, dest=False)

        # Send new metadata and process lists to dest
        do_exec([[CP.tar, "-cf", "-", "../in_process.tmp", "volinfo.tmp", "merge.lst.gz", target],
                 dest_run_args(desttype, [destcd + bkdir+"/"+datavol + " && tar -xmf -"
                                          +" && { sync -f . || sync; }"
                                          +" && mv in_process.tmp ../in_process"
                                          ])
                ])
        os.replace("../in_process.tmp", "../in_process")

    # Start merge operation on dest
    retcode = dest_run([destcd + bkdir+"/"+datavol + " && python3 "
                        +tmpdir+"-rpc/dest_helper.py merge"+(" --resume" if resume else "")
                        ],
                        check=False, out=">>"+tmpdir+"/merge.log"
                      )

    if retcode == 50:
        # Initialization didn't complete, so abort
        set_in_process(None)
        for f in ("merge.lst.gz","volinfo.tmp",target+"/info.tmp"):
            if exists(f):  os.remove(f)
        dest_run([destcd + bkdir+"/"+datavol
                  +" && rm -rf merge merge-init merge.lst.gz volinfo.tmp"], check=False)
        x_it(retcode, "Error: Merge could not initialize!")
    elif retcode != 0:
        x_it(retcode, "Error: Remote exited!")

    catch_signals()
    # Finalize merge operation on dest
    dest_run([destcd + bkdir+"/"+datavol + " && python3 "
                +tmpdir+"-rpc/dest_helper.py merge --finalize"+(" --resume" if resume else "")
                +" && rm -rf ../in_process merge merge.lst.gz"
                ],
                trap=True, out=">>"+tmpdir+"/merge.log"
            )

    # Local finalize
    if exists("volinfo.tmp"):  os.replace("volinfo.tmp", "volinfo")
    for f in ("/info","/manifest"):
        if exists(target+f+".tmp"):  os.replace(target+f+".tmp", target+f)

    set_in_process(None, dest=False)    ; os.remove("merge.lst.gz")
    catch_signals(None)

    # Check consistency after resuming merge
    if resume and compare_files(volumes=[volume], sessions=[volume.sessions[target]]):
        x_it(1, "Error: Local and dest metadata differ.")

    for ses in sources:  shutil.rmtree(volume.path+"/"+ses, ignore_errors=True)


# Compare files between local and dest archive, using hashes.
# The file tmpdir/compare-files.lst can be pre-populated with file paths if clear=False;
# otherwise will build metadata file list from Volume & Ses objects.
# Returns False if local and dest hashes match.

def compare_files(pathlist=[], volumes=[], sessions=[], clear=True):
    cmp_list = tmpdir+"/compare-files.lst"
    if clear and exists(cmp_list):  os.remove(cmp_list)
    realvols  = [x for x in volumes if len(x.sessions) and not x.meta_checked]
    realses   = [x for x in sessions if not x.meta_checked]
    if len(volumes)+len(sessions)+len(pathlist) == 0:   return False

    with open(cmp_list, "a") as flist:
        for pth in pathlist:
            if not exists(pth):   raise FileNotFoundError(pth)
            print(pth, file=flist)
        for v in realvols:
            v.meta_checked = True    ; print(v.name+"/volinfo", file=flist)
        for s in realses:
            s.meta_checked = True
            for sf in ("info","manifest"):   print(pjoin(s.volume.name,s.name,sf), file=flist)

    do_exec([[CP.xargs, CP.sha256sum]], cwd=aset.path,
            infile=cmp_list, out=tmpdir+"/compare-hashes.local")
    dest_run([destcd + bkdir +" && xargs sha256sum"],
             infile=cmp_list, out=tmpdir+"/compare-hashes.dest")
    # maybe switch cmp to diff/sha256sum if they are safe enough to read untrusted input
    files  = [tmpdir+"/compare-hashes.local", tmpdir+"/compare-hashes.dest"]
    result = do_exec([[CP.cmp] + files], check=False) > 0
    if result and options.debug:
        print("'diff %s %s' output:" % tuple(files))
        subprocess.call([CP.diff] + files)

    return result


# Receive volume from archive. If no save_path specified, then verify only.
# If diff specified, compare with current local volume; with --remap option
# can be used to resync volume with archive if the deltamap or snapshots
# are lost or if the local volume reverted to an earlier state.

def receive_volume(datavol, select_ses="", save_path="", diff=False):

    def diff_compare(dbuf,z):
        if dbuf != volf.read(chunksize):
            if remap:
                volsegment = addr // chunksize 
                bmap_pos = volsegment // 8
                bmap_mm[bmap_pos] |= 1 << (volsegment % 8)
            return len(dbuf)
        else:
            return 0

    verify_only = options.action == "verify"      ; remap     = options.remap
    attended    = not options.unattended          ; debug     = options.debug
    sparse      = options.sparse                  ; chunksize = aset.chunksize
    sparse_write= options.sparse_write or sparse
    vgname      = aset.vgname                     ; zeros     = bytes(chunksize)
    vol         = aset.vols[datavol]              ; snap1vol  = datavol+".tick"
    gethash     = hash_funcs[aset.hashtype]       ; sessions  = vol.sesnames
    compress    = compressors[aset.compression][2]  ; compresslevel = int(aset.compr_level)
    decompress  = compressors[aset.compression][0].decompress
    compare_digest = hmac.compare_digest


    if aset.key:
        crypto_key = bytes.fromhex(aset.key)
        decrypt    = DataCryptography(aset.cipher, key=crypto_key).decrypt
    else:
        crypto_key  = None


    if diff or verify_only:
        save_path = ""
    elif not save_path:
        save_path = pjoin("/dev",vgname,datavol)

    if save_path or diff:
        for prg in (CP.lvm, CP.blkdiscard):
            if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    # Set the session to retrieve
    if select_ses:
        if select_ses[0] == "^":
            # match tag to session id
            tag = select_ses[1:]
            if tag in vol.tags:
                select_ses = sorted(vol.tags[tag])[-1]
                print("Matched tag to", select_ses)
        else:
            # validate date-time input
            datetime.datetime.strptime(select_ses, "%Y%m%d-%H%M%S")
            select_ses = "S_"+select_ses

        if select_ses not in sessions:
            x_it(1, "No match for specified session criteria.")

    elif len(sessions) > 0:
        select_ses = sessions[-1]
    else:
        x_it(1, "No sessions available.")

    if diff and remap and select_ses != sessions[-1]:
        x_it(1, "Cannot use prior session for remap.")

    volsize   = vol.sessions[select_ses].volsize    ; volf = None

    if save_path and exists(save_path) and attended:
        print("\n!! This will", "overwrite" if sparse_write else "erase all",
              "existing data in",save_path,"!!")
        ans = ask_input("   Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    chdigits    = max_address.bit_length() // 4      ; chformat    = "x%0"+str(chdigits)+"x"
    lchunk_addr = vol.last_chunk_addr(volsize)       ; last_chunkx = chformat % lchunk_addr
    addrsplit   = -address_split[1]                  ; rc = None

    # Collect session manifests
    include = False   ; incl_ses = []
    for ses in sessions:
        incl_ses.append(vol.sessions[ses])

        if vol.sessions[ses].format == "tar":
            raise NotImplementedError("Receive from tarfile not yet implemented: "+ses)

        if ses == select_ses:  break

    # compare metadata hashes
    if not options.from_arch:
        print("Checking metadata... ", end="", flush=True)
        if compare_files(volumes=[vol], sessions=incl_ses):
            x_it(1, "Error: Local and archive metadata differ.")
        else:
            print("OK")

    # Merge manifests and send to archive system:
    # sed is used to expand chunk info into a path and filter out any entries
    # beyond the current last chunk, then piped to cat on destination.
    # Note address_split is used to bisect filename to construct the subdir.
    manifest = merge_manifests(datavol, msessions=[x.name for x in incl_ses], addcol=True)

    if not sparse:
        cmds = [[CP.sed, "-E", "/"+last_chunkx+"/q", manifest],  ## Enh: detect vol_shrank
                [CP.sed, "-E", "/^0\s/ d; "
                "s|^\S+\s+x(\S{" +str(address_split[0])+ "})(\S+)\s+(S_\S+)|\\3/\\1/x\\1\\2|;"],
                [CP.gzip, "-c", "-4"
                ],
                dest_run_args(desttype, ["cat >"+tmpdir+"-rpc/dest.lst.gz"]),
            ]
        do_exec(cmds)

    # Prepare save volume
    if save_path:
        # Decode dev path semantics and match to vg/lv if possible. Otherwise, open
        # simple block dev or file.
        save_type = "block device"
        returned_home = False    ; lv, pool, vg = get_lv_path_pool(save_path)
        if not lv and vg_exists(os.path.dirname(save_path)):
            # Got vg path, lv does not exist
            lv = os.path.basename(save_path)
            vg = os.path.basename(os.path.dirname(save_path))
        if vg:
            # Does save path == original path?
            returned_home = (lv== datavol) and (vg== aset.vgname) and not options.from_arch
            save_type = "logical volume"
            if not lv_exists(vg,lv):
                if vg != vgname:
                    x_it(1, "Cannot auto-create volume: Volume group does not match config.")
                print("Creating '%s' in thin pool [%s/%s]." % (lv, vg, aset.poolname))
                do_exec([[CP.lvm, "lvcreate", "-kn", "-ay", "-V", str(volsize)+"b",
                          "--thin", "-n", lv, vg+"/"+aset.poolname]])
            elif l_vols[lv].lv_size != volsize:
                print("Re-sizing LV to %d bytes." % volsize)
                do_exec([[CP.lvm, "lvresize", "-L", str(volsize)+"b", "-f", save_path]])

        if exists(save_path) and stat.S_ISBLK(os.stat(save_path).st_mode):
            if not sparse_write:   do_exec([[CP.blkdiscard, save_path]])
            volf = open(save_path, "w+b")
        elif save_path.startswith("/dev/"):
            x_it(1,"Cannot create new volume from ambiguous /dev path."
                 " Please create the volume before using 'receive', or specify"
                 " --save-to=/dev/volgroup/lv in case of a thin LV.")
        else: # file
            save_type = "file"
            volf = open(save_path, "w+b")
            if not sparse_write:   volf.truncate(0)   ; volf.flush()
            volf.truncate(volsize)    ; volf.flush()

    elif diff:
        if not lv_exists(vgname, datavol):
            x_it(1, "Local volume must exist for diff.")
        if remap:
            lv_remove(vgname, snap1vol)
            do_exec([[CP.lvm,"lvcreate", "-pr", "-kn", "-ay", "--addtag=wyng",
                    "--addtag="+vol.last, "--addtag=delta",
                    "--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n", snap1vol]])
            print("  Initial snapshot created for", datavol)
            get_lvm_vgs(aset.vgname)
            if not exists(vol.mapfile):
                init_deltamap(vol, vol.mapfile, vol.mapsize())
            bmapf = open(vol.mapfile, "r+b")
            bmapf.truncate(vol.mapsize())    ; bmapf.flush()
            bmap_mm = mmap.mmap(bmapf.fileno(), 0)
        else:
            if not lv_exists(vgname, snap1vol):
                print("Snapshot '.tick' not available; Comparing with source volume instead.")
                snap1vol = datavol

            if volsize != l_vols[snap1vol].lv_size:
                x_it(1, "Volume sizes differ:"
                    "\n  Archive = %d \n  Local   = %d" % (volsize, l_vols[snap1vol].lv_size))

        volf  = open(pjoin("/dev",vgname,snap1vol), "rb")


    if volf:   volf_read = volf.read    ; volf_write = volf.write    ; volf_seek = volf.seek

    print("Receiving" if save_path else "Scanning", "volume :", datavol, select_ses[2:])
    if save_path:    print("Saving to %s '%s'" % (save_type, save_path))

    # Create retriever process using py program
    cmd = dest_run_args(desttype,
            [destcd + bkdir+"/"+datavol
             +" && exec 2>>"+tmpdir+"-rpc/receive.log"
             +" && python3 "+tmpdir+"-rpc/dest_helper.py receive"
            ])
    getvol   = subprocess.Popen(cmd, stdout=subprocess.PIPE,
                                     stdin =subprocess.PIPE if sparse else subprocess.DEVNULL)
    gv_stdin = io.TextIOWrapper(getvol.stdin, encoding="utf-8") if sparse else None

    # Open manifest then receive, check and save data
    addr = bcount = diff_count = 0
    for mfline in open(manifest, "r"):
        if addr >= lchunk_addr:   break
        cksum, faddr, ses = mfline.split()    ; addr = int(faddr[1:], 16)

        if attended:   print("%.2f%%" % (addr/volsize*100), end="\x0d")

        # Process zeros quickly
        if cksum.strip() == "0":
            if save_path:
                volf_seek(addr)
                if sparse_write and volf_read(chunksize) != zeros:
                    volf_seek(addr)    ; volf_write(zeros)    ; diff_count += chunksize
            elif diff:
                volf_seek(addr)    ; diff_count += diff_compare(zeros,True)

            continue

        # Request chunks on-demand if local chunk doesn't match cksum
        if sparse and save_path:
            volf_seek(addr)
            if gethash(compress(volf_read(chunksize), compresslevel)).hexdigest() == cksum:
                continue
            else:
                print("%s/%s/%s" % (ses, faddr[1:addrsplit], faddr), flush=True, file=gv_stdin)

        # Read chunk size
        untrusted_size = int.from_bytes(getvol.stdout.read(4),"big")

        # allow for slight expansion from compression algo
        if untrusted_size > chunksize + (chunksize // 64) or untrusted_size < 1:
            raise BufferError("Bad chunk size %d for %s" % (untrusted_size, mfline))

        ##  Size is OK  ##
        size = untrusted_size

        # Read chunk buffer
        untrusted_buf = getvol.stdout.read(size)
        rc  = getvol.poll()
        if rc is not None and len(untrusted_buf) == 0:
            break

        if len(untrusted_buf) != size:
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)
            raise BufferError("Got %d bytes, expected %d" % (len(untrusted_buf), size))


        # Decrypt the data chunk
        # Validation MUST be next step!
        if crypto_key:
            untrusted_buf = decrypt(untrusted_buf)


        # Validate data chunk
        if not compare_digest(cksum, gethash(untrusted_buf).hexdigest()):
            with open(tmpdir+"/bufdump", "wb") as dump:
                dump.write(untrusted_buf)
            print(mfline)
            raise ValueError("Bad hash "+faddr+" :: "+gethash(untrusted_buf).hexdigest())

        ##  Buffer is OK  ##

        if verify_only:   continue

        buf = untrusted_buf

        # Proceed with decompress.
        decomp = decompress(buf)
        if len(decomp) != chunksize and addr < lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))
        if addr == lchunk_addr and len(decomp) != volsize - lchunk_addr:
            print(mfline)
            raise BufferError("Decompressed to %d bytes." % len(decomp))

        buf = decomp    ; bcount += len(buf)

        if save_path:
            volf_seek(addr)
            # Don't re-check buffer for sparse mode, check for sparse_write:
            if sparse:
                volf_write(buf)    ; diff_count += len(buf)
            elif sparse_write:
                if volf_read(chunksize) != buf:
                    volf_seek(addr)    ; volf_write(buf)    ; diff_count += len(buf)
            else:
                volf_write(buf)
        elif diff:
            volf_seek(addr)    ; diff_count += diff_compare(buf,False)


    if rc is not None and rc != 0:
        raise RuntimeError("Error code from getvol process: "+str(rc))

    if not sparse and addr+len(decompress(untrusted_buf)) != volsize:
        raise ValueError("Received range %d does not match volume size %d."
                            % (addr+len(decompress(untrusted_buf)), volsize))
    print("100.00%")
    if debug:
        print("Received range:", addr+len(buf),
            "\n    Data bytes:", bcount)
    if diff_count:  print("Diff bytes:", diff_count)

    if volf:  volf.close()
    if save_path:
        os.sync()
        if returned_home and select_ses == sessions[-1]:
            lv_remove(vgname, snap1vol)
            do_exec([[CP.lvm, "lvcreate", "-pr", "-kn", "-ay",
                "--addtag=wyng", "--addtag="+vol.last,
                "--addtag=arch-"+aset.uuid, "-s", vgname+"/"+datavol, "-n", snap1vol]])
            print("  Initial snapshot created for", datavol)
            init_deltamap(vol, vol.mapfile, vol.mapsize())
        elif returned_home:
            print("Restored from older session: Volume may be out of"
                " sync with archive until '%s --remap diff %s' is run!"
                % (prog_name, datavol))
    elif diff:
        if remap:
            bmapf.close()
            print("Delta bytes re-mapped:", diff_count)
            if diff_count > 0 and options.action != "send":
                print("\nNext 'send' will bring this volume into sync.")
        elif diff_count:
            x_it(1, "%d bytes differ." % diff_count)


# Rename a volume in the archive

def rename_volume(oldname, newname):
    for prg in (CP.lvm, CP.blkdiscard):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    os.chdir(aset.path)    ; volume   = aset.vols[oldname]
    confname = os.path.basename(aset.confpath)    ; was_in_process = aset.in_process is not None
    if not aset.rename_volume(oldname, newname, confname=confname+".tmp"):
        x_it(1, "Error: Cannot rename '%s' to '%s'." % (oldname,newname))

    do_exec[[CP.tar, "-czf", tmpdir+"/rename.tgz", confname+".tmp", oldname+"/volinfo"]]
    catch_signals()
    if not aset.in_process:   set_in_process(["rename", oldname, newname])
    rc = dest_run([ destcd + bkdir + " && tar -xzf - && python3 "
                   +tmpdir+("-rpc/dest_helper.py rename '%s' '%s'" % (oldname, newname)) ],
                    infile=tmpdir+"/rename.tgz", check=False, trap=True)

    if rc == 50:
        set_in_process(None)
        raise RuntimeError("'%s' already exists in destination!")

    if rc not in (0, 40):
        raise RuntimeError("Destination exit code "+str(rc))

    if oldname in os.listdir(aset.path):
        os.replace(aset.path+"/"+oldname, aset.path+"/"+newname)
    os.replace(aset.confpath+".tmp", aset.confpath)
    set_in_process(None)    ; catch_signals(None)

    for ext in (".tick",".tock"):
        vg = aset.vgname    ; lv_remove(vg, newname+ext)
        if lv_exists(vg, oldname+ext):   lv_rename(vg, oldname+ext, newname+ext)

    full_aset = ArchiveSet(aset.name, metadir+topdir, allvols=True)
    if rc != 40 and len(full_aset.vols[newname].sessions) \
    and compare_files(pathlist=[confname], volumes=[full_aset.vols[newname]]):
        x_it(1, "Error: Local and archive metadata differ.")

    return ArchiveSet(aset.name, metadir+topdir)


# Remove a volume from the archive

def delete_volume(dv):
    for prg in (CP.lvm, CP.blkdiscard):
        if not shutil.which(prg):  raise RuntimeError("Required command not found: "+prg)

    if not options.unattended and not aset.in_process:
        print("Warning! Delete will remove ALL metadata AND archived data",
              "for volume", dv)

        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    print("\nDeleting volume", dv, "from archive.")
    if not aset.in_process:       set_in_process(["delete", dv])
    aset.delete_volume(dv)

    confname = os.path.basename(aset.confpath)
    cmd = [destcd + bkdir
          +"  && cat >"+confname
          +(" && rm -rf '%s'" % dv)
          +"  && { sync -f . || sync; }"
          ]
    dest_run(cmd, infile=aset.confpath)
    set_in_process(None)
    return


# Remove Wyng metadata from local system

def remove_local_metadata(archive):
    for vg in volgroups.values():
        for lv in list(vg.lvs.values()):
            if lv.lv_name.endswith((".tick",".tock")) and "wyng" in lv.tags \
            and (not archive or "arch-"+archive.uuid in lv.tags):
                lv_remove(vg.name, lv.lv_name, check=True)

    # Remove metadata dir(s)
    shutil.rmtree(archive.path if archive else metadir+topdir,
                  ignore_errors=True)


def show_list(selected_vols):

    # Print list of sessions grouped by tag. First, organize by tag:
    if options.tag:
        print("\nTag Assignments")    ; ltags = {}
        for dv in selected_vols if selected_vols else datavols:
            vol = aset.vols[dv]
            for tag, tses in vol.tags.items():
                #tset = ( dv+" / "+x[2:] for x in tses )
                tset = ( (dv, x[2:], vol.sessions[x].tags[tag] ) for x in tses )
                if tag not in ltags:
                    ltags[tag] = list(tset)
                else:
                    ltags[tag] += tset
        # Print result:
        for tag in sorted(ltags):
            print("\n"+tag+":")
            for ses in sorted(ltags[tag]):   print(" ", ses)
        return

    # Print list of volumes if no volume is selected.
    if not aset.vols:
        print("No volumes.")    ; return
    elif not selected_vols and not len(options.volumes):
        print("\nConfigured Volumes [%s/%s]\n" % (aset.vgname, aset.poolname))
        maxwidth = max(len(x.name) for x in aset.vols.values())
        fmt    = "%7.1f GB  %-" + str(maxwidth+5) + "s  %s"
        for vname in sorted(x.name for x in aset.vols.values()):
            vol = aset.vols[vname]   ; sname = vol.sesnames[-1][2:] if len(vol.sesnames) else " "
            if options.verbose:
                print(fmt % ((aset.vols[vname].volsize() / 1024**3), vname, sname))
                if vol.desc:   print("  desc:", vol.desc)
            else:
                print(vname)
        return

    # Print list of sessions grouped by volume.
    # Get the terminal column width and set cols to number of list columns.
    ttycols = os.popen('stty size', 'r').read().split()[1]
    cols = max(4, min(10, int(ttycols)//17))
    for dv in selected_vols:
        print("\nSessions for volume '%s':\n" % dv)
        if not aset.vols[dv].sessions:   print("None.")    ; continue
        vol = aset.vols[dv]    ; lmonth = vol.sesnames[0][2:8]    ; slist = []

        # Blank at end of 'sesnames' is a terminator that doesn't match any month value.
        for sname in vol.sesnames + [""]:
            month = sname[2:8]    ; ses = vol.sessions[sname] if sname else None
            if options.unattended or options.verbose:
                # plain listing
                print(sname[2:])
                if ses and options.verbose:
                    for tag in sorted(ses.tags.items()):
                        print(" tag:", tag[0]+(", "+tag[1] if tag[1] else ""))
                continue

            if month == lmonth:
                # group sessions by month
                slist.append(sname)
            else:
                # print the month listing: 'rows' is adjusted to carry the remainder on
                # additional lines. 'extra' and 'steps' are used to eliminate ragged
                # column on the right (a ragged row is more pleasing to the eye).
                size  = len(slist)    ; rows = size//cols
                rows += (size%rows)//cols if rows else 0     ; extra = size - cols*rows
                heights = [rows+(x<extra) for x in range(cols) ]

                # output one row at a time
                for ii in range(max(heights)):
                    print("  ".join(  slist[ii+sum(heights[:c])][2:] for c in range(cols)
                                       if ii < heights[c]  ))
                print()

                # start new month list
                lmonth = month    ; slist = [sname]


def show_mem_stats():
    vsz, rss = map(int, os.popen("ps -up"+str(os.getpid())).readlines()[-1].split()[4:6])
    print("\n  Memory use: Max %dMB" %
        (resource.getrusage(resource.RUSAGE_SELF).ru_maxrss * resource.getpagesize() // 1024//1024)
        )
    print("  Current: vsize %d, rsize %d" % (vsz/1000,rss/1000))


def is_num(val):
    try:
        float(val)
    except:
        return False
    else:
        return True


def catch_signals(action="do"):
    if action is not None and len(signal_handlers):   print("Already catching")   ; return
    for sig in (signal.SIGINT, signal.SIGTERM, signal.SIGQUIT, signal.SIGABRT, signal.SIGALRM,
                signal.SIGHUP, signal.SIGTSTP, signal.SIGUSR1):
        if action is None:
            signal.signal(sig, signal_handlers[sig])       ; del(signal_handlers[sig])
        else:
            signal_handlers[sig] = signal.getsignal(sig)   ; signal.signal(sig, handle_signal)
            signal.siginterrupt(sig, False)

    if action is None:
        while len(signals_caught):   os.kill(os.getpid(), signals_caught.pop(0))


def handle_signal(signum, frame):
    sys.stderr.write(" *** Caught signal "+signum)
    if signum not in signals_caught:   signals_caught.append(signum)


def ask_input(text):
    sys.stderr.write(text)
    return input()


# Exit with simple message
def x_it(code, text):
    sys.stderr.write(text+"\n")
    if tmpdir:   cleanup()
    sys.exit(code)


def cleanup():
    if not debug:
        shutil.rmtree(big_tmpdir, ignore_errors=True)
        for f in (big_tmpdir, tmpdir, tmpdir+"-rpc", tmpdir+"-old"):
            shutil.rmtree(f, ignore_errors=True)
    if error_cache:
        sys.stderr.write("Error on volume(s): " + ", ".join(error_cache) + "\n")
        sys.exit(2)




##  MAIN  #########################################################################################

# Constants / Globals
prog_name             = "wyng"
prog_version          = "0.4.0wip"      ; prog_date = "2021xxxx"
format_version        = 2               ; debug     = False       ; tmpdir = None

# Disk block size:
bs                    = 512
# LVM min blocks = 128 = 64kBytes:
lvm_block_factor      = 128
# Default archive chunk size = 64kBytes:
bkchunksize           = 1 * lvm_block_factor * bs
assert bkchunksize % (lvm_block_factor * bs) == 0
max_address           = 0xffffffffffffffff # 64bits
# for 64bits, a subdir split of 9+7 allows =< 4096 files per dir:
address_split         = [len(hex(max_address))-2-7, 7]
hash_bits             = 256

os.environ["LC_ALL"]  = "C"
shell_prefix          = "set -e && export LC_ALL=C\n"

url_types             = ("ssh://", "qubes-ssh://", "qubes://", "internal:")

ssh_opts              = ["-x", "-o", "ControlPath=~/.ssh/controlsocket-%r@%h-%p",
                         "-o", "ControlMaster=auto", "-o", "ControlPersist=60"]

local_actions         = ("index-test","monitor","list","version","add")

comment_len = 100             ; tag_len = 25
volname_len = 112             ; alphanumsym = "^[a-zA-Z0-9\+\._-]+$"
pjoin       = os.path.join    ; exists = os.path.exists


if sys.hexversion < 0x3080000:
    x_it(1, "Python ver. 3.8 or greater required.")

# Root user required
if os.getuid() > 0:
    x_it(1, "Must be root user.")

# Allow only one instance at a time
lockpath = "/var/lock/"+prog_name
try:
    lockf = open(lockpath, "w")
    fcntl.lockf(lockf, fcntl.LOCK_EX|fcntl.LOCK_NB)
except IOError:
    x_it(1, "ERROR: "+prog_name+" is already running.")


# Parse Arguments:
parser = argparse.ArgumentParser(description="")
parser.add_argument("action", choices=["send","monitor","add","delete","prune","receive",
                    "verify","diff","list","version","rename","arch-init","arch-delete",
                    "arch-deduplicate","arch-check"],
                    default="monitor", help="Action to take")
parser.add_argument("-u", "--unattended", action="store_true", default=False,
                    help="Non-interactive, supress prompts")
parser.add_argument("--all-before", dest="allbefore", action="store_true", default=False,
                    help="Select all sessions before --session date-time.")
parser.add_argument("--all", action="store_true", default=False)
parser.add_argument("--session", help="YYYYMMDD-HHMMSS[,YYYYMMDD-HHMMSS] or ^tag_id"
                                 " select session date|tag, singular or range.")
parser.add_argument("--tag", action="append",
                    help="tag_id[,description] Add tags when sending")
parser.add_argument("--keep", action="append", default=[],
                    help="YYYYMMDD-HHMMSS or ^tag_id exclude session by date|tag (prune)") 
parser.add_argument("--volex", default="", help="Exclude volume(s)")
parser.add_argument("--autoprune", default="off", help="Automatic pruning")
parser.add_argument("--from", dest="from_arch", default="",
                    help="Address+Path of other non-configured archive (receive, verify)")
parser.add_argument("--save-to", dest="saveto", default="",help="Path to store volume for receive")
parser.add_argument("--sparse", action="store_true", default=False, help="Retrieve differences only")
parser.add_argument("--sparse-write", action="store_true", default=False, help="Save differences only")
parser.add_argument("--remap", action="store_true", default=False, help="Remap snapshots")
parser.add_argument("--local", default="",    help="Init: LVM vg/pool containing source volumes")
parser.add_argument("--dest", default="",     help="Init: type:location of archive")
parser.add_argument("--subdir", default="",   help="Init: optional subdir for --dest")
parser.add_argument("--encrypt", default="aes-256-siv", help="Init: compression type:level")
parser.add_argument("--compression", default="", help="Init: compression type:level")
parser.add_argument("--hashtype", default="", help="Init: hash function type")
parser.add_argument("--chunk-factor", dest="chfactor", type=int,
                    help="Init: set chunk size to N*64kB")
parser.add_argument("--meta-dir", dest="metadir", default="", help="Use alternate metadata path")
parser.add_argument("--force", action="store_true", default=False, help="For arch-delete")
parser.add_argument("--clean", action="store_true", default=False, help="Clean snapshots, metadata")
parser.add_argument("--debug", action="store_true", default=False, help="Debug mode")
parser.add_argument("--quiet", action="store_true", default=False)
parser.add_argument("--verbose", action="store_true", default=False)
parser.add_argument("--dedup", "-d", action="store_true", default=False,
                    help="Data deduplication (send)")
parser.add_argument("--volume-desc", dest="voldesc", default="",
                    help="Set volume description (add, rename, send)")
parser.add_argument("volumes", nargs="*")
options = parser.parse_args()
#subparser = parser.add_subparsers(help="sub-command help")
#prs_prune = subparser.add_parser("prune",help="prune help")

# Set stdout to devnull if --quiet is specified
debug = options.debug    ; assert not (options.quiet and (options.verbose or debug))
if options.quiet and options.action != "list":
    sys.stdout = open(os.devnull, "w")


if not options.quiet:   print("%s %s %s" % (prog_name.capitalize(), prog_version, prog_date))


# Setup tmp and metadata dirs
metadir     = options.metadir if options.metadir else "/var/lib"
topdir      = "/"+prog_name+".backup040" ####
tmpdir      = "/tmp/"+prog_name
big_tmpdir  = metadir+topdir+"/.tmp"

shutil.rmtree(tmpdir+"-old", ignore_errors=True)
shutil.rmtree(big_tmpdir, ignore_errors=True)
if exists(tmpdir):
    os.rename(tmpdir, tmpdir+"-old")
os.makedirs(tmpdir+"/rpc")
os.makedirs(metadir+topdir+"/wyng.old", exist_ok=True)
os.makedirs(big_tmpdir, exist_ok=True)


## General Configuration ##

signal_handlers  = {}    ; signals_caught = []    ; error_cache = []

# Dict of compressors in the form: (library, default_compress_level, compress_func)
compressors      =    {"zlib":   (zlib, 4, zlib.compress),
                       "bz2" :   (bz2,  9, bz2.compress)}
if zstd:   compressors["zstd"] = (zstd, 7, lambda data, lvl: zstd.compress(data, lvl, 2))

hash_funcs       =    {"sha256" : hashlib.sha256,
                       "blake2b": functools.partial(hashlib.blake2b, digest_size=hash_bits//8)}

# Check --from usage (access other/unconfigured archive).
if options.from_arch and options.action not in ("receive","verify","list","arch-init"):
    x_it(1,"--from option can be used only with: receive, verify, list, arch-init")
# Select dedup test algorithm.
init_dedup_index = [None, init_dedup_index5][options.dedup]
monitor_only     = options.action == "monitor" # gather metadata without backing up

# Get LVM listings
volgroups, l_vols= {}, {}    ; get_lvm_vgs()

# Create an ArchiveSet object with get_configs() and set global vars pertaining
# to the destination.
# Enh: Use separate dest class so aset can be inst after detect_dest_state()
#      and/or move detect call to aset init.
destsys          = desttype  = dest_run_map  = dest_online  = dest_in_process  = dest_free  = None
aset             = get_configs()
destsys, desttype= detect_internal_state()
dest_online, dest_free, dest_in_process  = detect_dest_state(destsys)
bkdir            = topdir+"/"+aset.name
destpath         = os.path.normpath(pjoin(aset.destmountpoint,aset.destdir))
destcd           = " cd '"+destpath+"'"

# Check online status for certain commands.
if not dest_online and (options.remap or options.from_arch \
                    or options.action not in local_actions) \
                    and not (options.action == "delete" and options.clean):
    x_it(1, "Destination not ready to receive commands.")

if options.from_arch:
    aset         = get_configs_remote()
os.makedirs(metadir+bkdir, exist_ok=True)

# Set 'l_vols' to reference selected volgroup and check for orphan snapshots
if aset.vgname in volgroups:   l_vols = volgroups[aset.vgname].lvs


# Handle unfinished in_process:
# Functions supported here must not internally use global variable inputs that are unique to
# a runtime invocation (i.e. the 'options' objects), or they must test such variables
# in conjunction with aset.in_process.
if aset.in_process and dest_online and not options.from_arch \
    and options.action != "arch-init":
    if exists(aset.path+"/in_process_retry"):
        x_it(1, "Interrupted process already retried; Exiting.")
    open(aset.path+"/in_process_retry","w").close()

    if options.action == "delete" and aset.in_process[1] == options.volumes[0]:
        # user is currently deleting the in_process volume
        set_in_process(None)
    elif aset.in_process[0] in ("delete","merge","rename"):
        print("Completing prior operation in progress:", " ".join(aset.in_process[0:2]))

        if aset.in_process[0] == "delete":
            delete_volume(aset.in_process[1])
        elif aset.in_process[0] == "rename":
            aset = rename_volume(aset.in_process[1], aset.in_process[2])
        elif aset.in_process[0] == "merge":
            merge_sessions(aset.in_process[1], aset.in_process[4].split("/"),
                        aset.in_process[3], clear_sources=bool(aset.in_process[2]))

    else:
        print("Unknown prior operation in progress:", aset.in_process[0:2])
        x_it(1,"Exiting.")
elif dest_in_process:
    x_it(1, "Archive locked in_process without local indicator. Exiting.")


if aset.vgname in volgroups and options.action != "arch-check":
    arch_check([], startup=True)


# Check volume args against config
exclude_vols  = set(options.volex.split(","))
datavols      = sorted(set(aset.vols.keys())  - exclude_vols)
selected_vols = sorted(set(options.volumes[:]) - exclude_vols)
for vol in selected_vols[:]:
    if vol not in aset.vols and options.action not in {"add","delete","rename"}:
        print("Volume "+vol+" not configured; Skipping.")
        del(selected_vols[selected_vols.index(vol)])


# Process Commands:

if options.action   == "monitor":
    monitor_send(datavols, monitor_only=True)


elif options.action == "send":
    monitor_send(datavols, selected_vols, monitor_only=False)


elif options.action == "version":
    print(prog_name, "version", prog_version)


elif options.action == "prune":
    if options.autoprune.lower() == "off" and not options.session:
        x_it(1, "Must specify --autoprune or --session for prune.")
    dvs = datavols if len(selected_vols) == 0 else selected_vols

    if not options.unattended and len(dvs):
        print("This operation will delete session(s) from the archive;")
        ans = ask_input("Are you sure? [y/N]: ").strip()
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")

    for dv in dvs:
        if dv in datavols:
            print("Pruning", dv)
            if options.session:
                prune_sessions(dv, options.session.split(","))
            elif options.autoprune.lower() in ("on","min","full"):
                autoprune(dv, apmode=options.autoprune)


elif options.action == "receive":
    if len(selected_vols) != 1:
        x_it(1, "Specify one volume for receive.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify only one session for receive.")
    receive_volume(selected_vols[0], select_ses="" if not options.session else options.session,
                   save_path=options.saveto if options.saveto else "")


elif options.action == "verify":
    if len(selected_vols) != 1:
        x_it(1, "Specify one volume for verify.")
    if options.session and len(options.session.split(",")) > 1:
        x_it(1, "Specify one session for verify.")
    receive_volume(selected_vols[0],
                   select_ses="" if not options.session \
                   else options.session.split(",")[0],
                   save_path="")


elif options.action == "diff":
    if selected_vols:
        receive_volume(selected_vols[0], save_path="", diff=True)


elif options.action == "list":
    show_list(selected_vols)


elif options.action == "add":
    if len(options.volumes) < 1:
        x_it(1, "Volume name(s) required for 'add'.")

    for vol in options.volumes:
        if not lv_exists(aset.vgname, vol):   print("Warning:", vol, "does not exist.")
        aset.add_volume(vol)


elif options.action == "rename":
    if len(options.volumes) != 2:  x_it(1,"Rename requires two volume names.")
    rename_volume(options.volumes[0], options.volumes[1])


elif options.action == "delete":
    if options.clean:
        print("Remove local Wyng metadata from system for path",
              metadir+topdir if options.all else aset.path)
        if not options.unattended:
            ans = ask_input("Are you sure? [y/N]: ")
            if ans.lower() not in {"y","yes"}:
                x_it(0,"")
        elif not options.force:
            x_it(1, "Ignoring --clean without --force.")
        remove_local_metadata(None if options.all else aset)

    else:
        delete_volume(selected_vols[0])


elif options.action == "arch-init":
    # handled by get_configs, get_configs_remote
    pass


elif options.action == "arch-check":
    arch_check(options.volumes)


elif options.action == "arch-delete":
    print("Deleting ALL metadata AND archived data!")

    if not options.unattended and not options.force:
        ans = ask_input("Are you sure? [y/N]: ")
        if ans.lower() not in {"y","yes"}:
            x_it(0,"")
    elif options.unattended and not options.force:
        x_it(1,"Error: --force required to delete entire archive.")

    # Remove metadata and snapshots
    for dv in list(aset.vols):   aset.delete_volume(dv)

    # Remove local metadata and stray snapshots associated with archive
    remove_local_metadata(aset)

    # Remove from destination
    print("\nDeleting entire archive...")
    cmd = [destcd
          +" && rm -rf ."+bkdir
          +" && { sync -f . || sync; }"
          ]
    dest_run(cmd)


elif options.action == "arch-deduplicate":
    dedup_existing()


cleanup()
# END
